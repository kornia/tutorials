[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nImage matching example with LightGlue and DISK\n\n\n\n\n\n\nIntermediate\n\n\nLightGlue\n\n\nDisk\n\n\nLAF\n\n\nImage matching\n\n\nkornia.feature\n\n\n\nIn this tutorial we are going to show how to perform image matching using a LightGlue algorithm with DISK\n\n\n\n\n\nNov 17, 2023\n\n\nDmytro Mishkin\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Prompter: Segment Anything\n\n\n\n\n\n\nIntermediate\n\n\nSegmentation\n\n\nkornia.contrib\n\n\n\nThis tutorials shows how to use our high-level API Visual Prompter. This API allow to set an image, and run multiple queries multiple times on this image. These query can be done with three types of prompts.\n\n\n\n\n\nApr 18, 2023\n\n\nJoão G. Atkinson\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage matching example with DISK local features\n\n\n\n\n\n\nIntermediate\n\n\nDISK\n\n\nLAF\n\n\nImage matching\n\n\nkornia.feature\n\n\n\nIn this tutorial we are going to show how to perform image matching using a DISK algorithm\n\n\n\n\n\nApr 1, 2023\n\n\nDmytro Mishkin\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nData Augmentation 2D\n\n\n\n\n\n\nBasic\n\n\n2D\n\n\nData augmentation\n\n\nkornia.augmentation\n\n\n\nA show case of the Data Augmentation operation available on Kornia for images.\n\n\n\n\n\nFeb 4, 2023\n\n\nJoão Gustavo A. Amorim\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nFit plane tutorial\n\n\n\n\n\n\nBasic\n\n\nPlane\n\n\nkornia.geometry\n\n\n\nThis tutorial use shows how to generate a plane based on a mesh. Using the Hyperplane and Hyperplane from kornia.gemetry.plane. As data structure we use kornia.geometry.liegroup.So3 e kornia.geometry.vector.Vector3\n\n\n\n\n\nDec 16, 2022\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage and Keypoints augmentations\n\n\n\n\n\n\nIntermediate\n\n\nKeypoints\n\n\nData augmentation\n\n\n2D\n\n\nAugmentation container\n\n\nAugmentation Sequential\n\n\nkornia.augmentation\n\n\n\nIn this tutorial we leverage kornia.augmentation.AugmentationSequential to apply augmentations to image and transform reusing the applied geometric transformation to a set of associated keypoints. This is useful for detection networks or geometric problems.\n\n\n\n\n\nOct 14, 2022\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nLine detection and matching example with SOLD2: Self-supervised Occlusion-aware Line Description and Detection\n\n\n\n\n\n\nIntermediate\n\n\nLine detection\n\n\nLine matching\n\n\nSOLD2\n\n\nSelf-supervised\n\n\nkornia.feature\n\n\n\nIn this tutorial we will show how we can quickly perform line detection, and matching using kornia.feature.sold2 API.\n\n\n\n\n\nAug 31, 2022\n\n\nJoão G. Atkinson\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Mosaic Augmentation\n\n\n\n\n\n\nBasic\n\n\n2D\n\n\nData augmentation\n\n\nkornia.augmentation\n\n\n\nIn this tutorial we will show how we can quickly perform mosaicing using the features provided by the kornia.augmentation.RandomMosaic API. Mosaicing means taking several input images and combine their random crops into mosaic.\n\n\n\n\n\nAug 29, 2022\n\n\nJian Shi\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage matching example with KeyNet-AdaLAM\n\n\n\n\n\n\nIntermediate\n\n\nKeyNet\n\n\nLAF\n\n\nAdalam\n\n\nImage matching\n\n\nkornia.feature\n\n\n\nIn this tutorial we are going to show how to perform image matching using a KeyNet-Adalam Algorithm\n\n\n\n\n\nAug 22, 2022\n\n\nDmytro Mishkin\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nFit line tutorial\n\n\n\n\n\n\nBasic\n\n\nLine\n\n\nkornia.geometry\n\n\n\nThis tutorial use shows how to generate a line based on points. Using the ParametrizedLine and fit_line from kornia.gemetry.line\n\n\n\n\n\nJul 15, 2022\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting and Combining Tensor Patches\n\n\n\n\n\n\nBasic\n\n\nPatches\n\n\nkornia.contrib\n\n\n\nIn this tutorial we will show how you can extract and combine tensor patches using kornia\n\n\n\n\n\nMar 7, 2022\n\n\nAshwin Nair\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nFace Detection and blurring\n\n\n\n\n\n\nIntermediate\n\n\nFace detection\n\n\nBlur\n\n\nkornia.contrib\n\n\n\nIn this tutorial we will show how to use the Kornia Face Detection and how we can blurring these detected faces.\n\n\n\n\n\nNov 30, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage stitching example with LoFTR\n\n\n\n\n\n\nIntermediate\n\n\nLoFTR\n\n\nkornia.feature\n\n\n\nA show case of how to do image stitching using LoFTR from Kornia.\n\n\n\n\n\nNov 19, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nConvert RGB to RAW\n\n\n\n\n\n\nBasic\n\n\nColor spaces\n\n\nkornia.color\n\n\n\nIn this tutorial we are going to learn how to convert image from raw color using kornia.color.\n\n\n\n\n\nOct 13, 2021\n\n\nOskar Flordal\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nConvert RGB to YUV420\n\n\n\n\n\n\nBasic\n\n\nColor spaces\n\n\nkornia.color\n\n\n\nIn this tutorial we are going to learn how to convert image from RGB color to YUV420 using kornia.color.\n\n\n\n\n\nOct 10, 2021\n\n\nOskar Flordal\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nConnected Components Algorithm\n\n\n\n\n\n\nBasic\n\n\nSegmentation\n\n\nLabeling\n\n\nUnsupervised\n\n\nkornia.contrib\n\n\n\nIn this tutorial we are going to learn how to segment small objects in the image using the kornia implementation of the classic Computer Vision technique called Connected-component labelling (CCL).\n\n\n\n\n\nSep 16, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage matching example with LoFTR\n\n\n\n\n\n\nIntermediate\n\n\nLoFTR\n\n\nLAF\n\n\nImage matching\n\n\nkornia.feature\n\n\n\nIn this tutorial we are going to show how to perform image matching using a LoFTR algorithm\n\n\n\n\n\nSep 11, 2021\n\n\nDmytro Mishkin\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage Registration by Direct Optimization\n\n\n\n\n\n\nIntermediate\n\n\nImage Registration\n\n\nkornia.geometry\n\n\n\nIn this tutorial we are going to learn how to perform the task of image alignment by optimizing the similarity transformation between two images in order to create a photo with wide in-focus area from set of narrow-focused images.\n\n\n\n\n\nSep 6, 2021\n\n\nDmytro Mishkin\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage histogram and equalizations techniques\n\n\n\n\n\n\nBasic\n\n\nColor spaces\n\n\nkornia.enhance\n\n\n\nIn this tutorial we are going to learn how using kornia components and Matplotlib we can visualize image histograms and later use kornia functionality to equalize images in batch and using the gpu.\n\n\n\n\n\nAug 31, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nResize anti-alias\n\n\n\n\n\n\nBasic\n\n\nRescale\n\n\nkornia.geometry\n\n\n\nIn this tutorial we are going to learn how to resize an image with anti-alias.\n\n\n\n\n\nAug 28, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage patch generation\n\n\n\n\n\n\nIntermediate\n\n\nPatches\n\n\nkornia.geometry\n\n\n\nIn this tutorial we are going to learn how to generate image patches using kornia.geometry components.\n\n\n\n\n\nAug 28, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage matching example with kornia local features: TFeat, MKD, OriNet, HyNet and OpenCV detector\n\n\n\n\n\n\nIntermediate\n\n\nLocal features\n\n\nkornia.feature\n\n\n\nIn this tutorial we will show how we can perform image matching using kornia local features\n\n\n\n\n\nAug 28, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage anti-alias with local features\n\n\n\n\n\n\nBasic\n\n\nHardNet\n\n\nPatches\n\n\nLocal features\n\n\nkornia.feature\n\n\n\nIn this example we will show the benefits of using anti-aliased patch extraction with kornia.\n\n\n\n\n\nAug 28, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage Alignment by Homography Optimization\n\n\n\n\n\n\nAdvanced\n\n\nHomography\n\n\nkornia.geometry\n\n\n\nIn this tutorial we are going to learn how to perform the task of image alignment by optimising the homography transformation between two images.\n\n\n\n\n\nAug 28, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric image and points transformations\n\n\n\n\n\n\nIntermediate\n\n\nKeypoints\n\n\nkornia.augmentation\n\n\nkornia.geometry\n\n\n\nIn this tutorial we will learn how to generate and manipulate geometrically synthetic images and use their transformations to manipulate 2D points and how to combine with torch components to perform data augmention.\n\n\n\n\n\nAug 28, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nRotate image using warp affine transform\n\n\n\n\n\n\nBasic\n\n\nAffine\n\n\nkornia.geometry\n\n\n\nIn this tutorial we are going to learn how to rotate an image using the kornia.gemetry components.\n\n\n\n\n\nJul 6, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering Operators\n\n\n\n\n\n\nBasic\n\n\nFilters\n\n\nBlur\n\n\nkornia.filters\n\n\n\nIn this tutorial we are going to learn how to apply blurring filters to images with kornia.filters components.\n\n\n\n\n\nJul 6, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nEdge Detection\n\n\n\n\n\n\nBasic\n\n\nEdge Detection\n\n\nkornia.filters\n\n\n\nIn this tutorial we are going to learn how to detect edges in images with kornia.filters components.\n\n\n\n\n\nJul 6, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nImage Enhancement\n\n\n\n\n\n\nBasic\n\n\nkornia.enhance\n\n\n\nIn this tutorial we are going to learn how to tweak image properties using the compoments from kornia.enhance.\n\n\n\n\n\nJul 5, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nPatch Sequential\n\n\n\n\n\n\nIntermediate\n\n\n2D\n\n\nData augmentation\n\n\nPatches\n\n\nkornia.augmentation\n\n\n\nIn this tutorial we will show how we can quickly perform patch processing using the features provided by the kornia.augmentation.PatchSequential API.\n\n\n\n\n\nJun 11, 2021\n\n\nJian Shi\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nObtaining Edges using the Canny operator\n\n\n\n\n\n\nBasic\n\n\nEdge Detection\n\n\nkornia.filters\n\n\n\nIn this tutorial we show how easily one can apply the typical canny edge detection using Kornia\n\n\n\n\n\nJun 8, 2021\n\n\nPau Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nDenoise image using total variation\n\n\n\n\n\n\nAdvanced\n\n\nDenoising\n\n\n\nIn this tutorial we are going to learn how to denoise an image using the differentiable total_variation loss.\n\n\n\n\n\nJun 7, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nZCA Whitening\n\n\n\n\n\n\nAdvanced\n\n\nkornia.enhance\n\n\n\nThe following tutorial will show you how to perform ZCA data whitening on a dataset using kornia.enhance.zca. The documentation for ZCA whitening can be found here.\n\n\n\n\n\nMay 30, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nSharpen image using unsharp mask\n\n\n\n\n\n\nBasic\n\n\nFilters\n\n\nkornia.filters\n\n\n\nIn this tutorial we are going to learn how to use the unsharp mask\n\n\n\n\n\nMay 30, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nAugmentation Sequential\n\n\n\n\n\n\nBasic\n\n\n2D\n\n\nData augmentation\n\n\nkornia.augmentation\n\n\n\nIn this tutorial we will show how we can quickly perform data augmentation for various tasks (segmentation, detection, regression) using the features provided by the kornia.augmentation.AugmentationSequential API.\n\n\n\n\n\nMay 30, 2021\n\n\nJian Shi\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nBlur image using GaussianBlur operator\n\n\n\n\n\n\nBasic\n\n\nBlur\n\n\nkornia.filters\n\n\n\nIn this tutorial we show how easily one can apply typical image transformations using Kornia.\n\n\n\n\n\nMay 18, 2021\n\n\nTakeshi Teshima\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nData Augmentation Semantic Segmentation\n\n\n\n\n\n\nBasic\n\n\n2D\n\n\nSegmentation\n\n\nData augmentation\n\n\nkornia.augmentation\n\n\n\nIn this tutorial we will show how we can quickly perform data augmentation for semantic segmentation using the kornia.augmentation API.\n\n\n\n\n\nMar 27, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nWarp image using perspective transform\n\n\n\n\n\n\nIntermediate\n\n\nWarp image\n\n\nkornia.geometry\n\n\n\nIn this tutorial we are going to learn how to use the functions kornia.get_perspective_transform and kornia.warp_perspective.\n\n\n\n\n\nMar 18, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nKornia and PyTorch Lightning GPU data augmentation\n\n\n\n\n\n\nBasic\n\n\nData augmentation\n\n\nPytorch lightning\n\n\nkornia.augmentation\n\n\n\nIn this tutorial we show how one can combine both Kornia and PyTorch Lightning to perform data augmentation to train a model using CPUs and GPUs in batch mode without additional effort.\n\n\n\n\n\nMar 18, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nColor space conversion\n\n\n\n\n\n\nBasic\n\n\nColor spaces\n\n\nkornia.color\n\n\n\nIn this tutorial we are going to learn how to convert image from different color spaces using kornia.color.\n\n\n\n\n\nMar 18, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Morphological Operators\n\n\n\n\n\n\nBasic\n\n\nMorphology\n\n\nkornia.morphology\n\n\n\nIn this tutorial you are gonna explore kornia.morphology, that’s Kornia’s module for differentiable Morphological Operators.\n\n\n\n\n\nMar 8, 2021\n\n\nJuclique\n\n\n11/12/24, 1:00:28 AM\n\n\n\n\n\n\n\n\n\n\n\n\nHello world: Planet Kornia\n\n\n\n\n\n\nBasic\n\n\nkornia.io\n\n\n\nWelcome to Planet Kornia: a set of tutorials to learn about Computer Vision in PyTorch.\n\n\n\n\n\nJan 21, 2021\n\n\nEdgar Riba\n\n\n11/12/24, 1:00:27 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "nbs/filtering_edges.html",
    "href": "nbs/filtering_edges.html",
    "title": "Edge Detection",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/doraemon.png\"\ndownload_image(url)\n\n'doraemon.png'\nimport cv2\nimport kornia as K\nimport numpy as np\nimport torch\nimport torchvision\nfrom matplotlib import pyplot as plt\nWe use Kornia to load an image to memory represented in a torch.tensor\nx_rgb: torch.Tensor = K.io.load_image(\"doraemon.png\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\nx_gray = K.color.rgb_to_grayscale(x_rgb)\ndef imshow(input: torch.Tensor):\n    out = torchvision.utils.make_grid(input, nrow=2, padding=5)\n    out_np: np.ndarray = K.utils.tensor_to_image(out)\n    plt.imshow(out_np)\n    plt.axis(\"off\")\n    plt.show()\nimshow(x_gray)"
  },
  {
    "objectID": "nbs/filtering_edges.html#st-order-derivates",
    "href": "nbs/filtering_edges.html#st-order-derivates",
    "title": "Edge Detection",
    "section": "1st order derivates",
    "text": "1st order derivates\n\ngrads: torch.Tensor = K.filters.spatial_gradient(x_gray, order=1)  # BxCx2xHxW\ngrads_x = grads[:, :, 0]\ngrads_y = grads[:, :, 1]\n\n\n# Show first derivatives in x\nimshow(1.0 - grads_x.clamp(0.0, 1.0))\n\n\n\n\n\n\n\n\n\n# Show first derivatives in y\nimshow(1.0 - grads_y.clamp(0.0, 1.0))"
  },
  {
    "objectID": "nbs/filtering_edges.html#nd-order-derivatives",
    "href": "nbs/filtering_edges.html#nd-order-derivatives",
    "title": "Edge Detection",
    "section": "2nd order derivatives",
    "text": "2nd order derivatives\n\ngrads: torch.Tensor = K.filters.spatial_gradient(x_gray, order=2)  # BxCx2xHxW\ngrads_x = grads[:, :, 0]\ngrads_y = grads[:, :, 1]\n\n\n# Show second derivatives in x\nimshow(1.0 - grads_x.clamp(0.0, 1.0))\n\n\n\n\n\n\n\n\n\n# Show second derivatives in y\nimshow(1.0 - grads_y.clamp(0.0, 1.0))"
  },
  {
    "objectID": "nbs/filtering_edges.html#sobel-edges",
    "href": "nbs/filtering_edges.html#sobel-edges",
    "title": "Edge Detection",
    "section": "Sobel Edges",
    "text": "Sobel Edges\nOnce with the gradients in the two directions we can computet the Sobel edges. However, in kornia we already have it implemented.\n\nx_sobel: torch.Tensor = K.filters.sobel(x_gray)\nimshow(1.0 - x_sobel)"
  },
  {
    "objectID": "nbs/filtering_edges.html#laplacian-edges",
    "href": "nbs/filtering_edges.html#laplacian-edges",
    "title": "Edge Detection",
    "section": "Laplacian edges",
    "text": "Laplacian edges\n\nx_laplacian: torch.Tensor = K.filters.laplacian(x_gray, kernel_size=5)\nimshow(1.0 - x_laplacian.clamp(0.0, 1.0))"
  },
  {
    "objectID": "nbs/filtering_edges.html#canny-edges",
    "href": "nbs/filtering_edges.html#canny-edges",
    "title": "Edge Detection",
    "section": "Canny edges",
    "text": "Canny edges\n\nx_laplacian: torch.Tensor = K.filters.canny(x_gray)[0]\nimshow(1.0 - x_laplacian.clamp(0.0, 1.0))"
  },
  {
    "objectID": "nbs/canny.html#preparation",
    "href": "nbs/canny.html#preparation",
    "title": "Obtaining Edges using the Canny operator",
    "section": "Preparation",
    "text": "Preparation\nWe first install Kornia.\n\n%%capture\n%matplotlib inline\n!pip install kornia\n!pip install kornia-rs\n\n\nimport kornia\n\nkornia.__version__\n\n'0.6.12-dev'\n\n\nNow we download the example image.\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/paranoia_agent.jpg\"\ndownload_image(url)\n\n'paranoia_agent.jpg'"
  },
  {
    "objectID": "nbs/canny.html#example",
    "href": "nbs/canny.html#example",
    "title": "Obtaining Edges using the Canny operator",
    "section": "Example",
    "text": "Example\nWe first import the required libraries and load the data.\n\nimport kornia\nimport matplotlib.pyplot as plt\nimport torch\n\n# read the image with Kornia\nimg_tensor = kornia.io.load_image(\"paranoia_agent.jpg\", kornia.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\nimg_array = kornia.tensor_to_image(img_tensor)\n\nplt.axis(\"off\")\nplt.imshow(img_array)\nplt.show()\n\n\n\n\n\n\n\n\nTo apply a filter, we create the Canny operator object and apply it to the data. It will provide the magnitudes as well as the edges after the hysteresis process. Note that the edges are a binary image which is not differentiable!\n\n# create the operator\ncanny = kornia.filters.Canny()\n\n# blur the image\nx_magnitude, x_canny = canny(img_tensor)\n\nThat’s it! We can compare the source image and the results from the magnitude as well as the edges:\n\n# convert back to numpy\nimg_magnitude = kornia.tensor_to_image(x_magnitude.byte())\nimg_canny = kornia.tensor_to_image(x_canny.byte())\n\n# Create the plot\nfig, axs = plt.subplots(1, 3, figsize=(16, 16))\naxs = axs.ravel()\n\naxs[0].axis(\"off\")\naxs[0].set_title(\"image source\")\naxs[0].imshow(img_array)\n\naxs[1].axis(\"off\")\naxs[1].set_title(\"canny magnitude\")\naxs[1].imshow(img_magnitude, cmap=\"Greys\")\n\naxs[2].axis(\"off\")\naxs[2].set_title(\"canny edges\")\naxs[2].imshow(img_canny, cmap=\"Greys\")\n\nplt.show()\n\n\n\n\n\n\n\n\nNote that our final result still recovers some edges whose magnitude is quite low. Let us increase the thresholds and compare the final edges.\n\n# create the operator\ncanny = kornia.filters.Canny(low_threshold=0.4, high_threshold=0.5)\n\n# blur the image\n_, x_canny_threshold = canny(img_tensor)\n\n\nimport torch.nn.functional as F\n\n# convert back to numpy\nimg_canny_threshold = kornia.tensor_to_image(x_canny_threshold.byte())\n\n# Create the plot\nfig, axs = plt.subplots(1, 3, figsize=(16, 16))\naxs = axs.ravel()\n\naxs[0].axis(\"off\")\naxs[0].set_title(\"image source\")\naxs[0].imshow(img_array)\n\naxs[1].axis(\"off\")\naxs[1].set_title(\"canny default\")\naxs[1].imshow(img_canny, cmap=\"Greys\")\n\naxs[2].axis(\"off\")\naxs[2].set_title(\"canny defined thresholds\")\naxs[2].imshow(img_canny_threshold, cmap=\"Greys\")\n\nplt.show()"
  },
  {
    "objectID": "nbs/image_registration.html",
    "href": "nbs/image_registration.html",
    "title": "Image Registration by Direct Optimization",
    "section": "",
    "text": "The images are courtesy of Dennis Sakva\n%%capture\n!pip install kornia\n!pip install kornia-rs\nimport io\n\nimport requests\n\n\ndef download_data(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_data(\"http://cmp.felk.cvut.cz/~mishkdmy/bee.zip\")\n\n'bee.zip'\n%%capture\n!unzip bee.zip\nImport needed libraries\nimport os\nfrom copy import deepcopy\nfrom typing import List\n\nimport imageio\nimport kornia as K\nimport kornia.geometry as KG\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\n\ndef get_data_directory(base):\n    path = os.path.join(\"../\", base)\n    if os.path.isdir(os.path.join(path, \"data\")):\n        return os.path.join(path, \"data/\")\n    return get_data_directory(path)"
  },
  {
    "objectID": "nbs/image_registration.html#images-preview",
    "href": "nbs/image_registration.html#images-preview",
    "title": "Image Registration by Direct Optimization",
    "section": "Images preview",
    "text": "Images preview\nLet’s check our images. There are almost 100 of them, so we will show only each 10th\n\nfnames = os.listdir(\"bee\")\nfnames = [f\"bee/{x}\" for x in sorted(fnames) if x.endswith(\"JPG\")]\nfig, axis = plt.subplots(2, 5, figsize=(12, 4), sharex=\"all\", sharey=\"all\", frameon=False)\nfor i, fname in enumerate(fnames):\n    if i % 10 != 0:\n        continue\n    j = i // 10\n    img = K.io.load_image(fname, K.io.ImageLoadType.RGB8)\n    axis[j // 5][j % 5].imshow(K.tensor_to_image(img), aspect=\"auto\")\nplt.subplots_adjust(wspace=0.05, hspace=0.05)\nfig.tight_layout()\n\n\n\n\n\n\n\n\nSo the focus goes from back to the front, so we have to match and merge them in the same order."
  },
  {
    "objectID": "nbs/image_registration.html#image-registration",
    "href": "nbs/image_registration.html#image-registration",
    "title": "Image Registration by Direct Optimization",
    "section": "Image registration",
    "text": "Image registration\nWe will need ImageRegistrator object to do the matching. Because the photos are takes so that only slight rotation, shift and scale change is possible, we will use similarity mode, which does exactly this.\n\nuse_cuda: bool = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nregistrator = KG.ImageRegistrator(\"similarity\", loss_fn=F.mse_loss, lr=8e-4, pyramid_levels=3, num_iterations=500).to(device)\nprint(device)\n\ncuda\n\n\nWe will register images sequentially with ImageRegistrator.\n\n%%capture\nmodels = []\nfor i, fname in tqdm(enumerate(fnames)):\n    if i == 0:\n        continue\n    prev_img = K.io.load_image(fnames[i - 1], K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    curr_img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    model = registrator.register(prev_img, curr_img)\n    models.append(deepcopy(model.detach()))\n\nLet’s take the final (the most close-focused) image as the reference - this means that we have to convert our image transforms from (between i and i+1) mode into (between i and last). We can do it by matrix multiplication.\n\nmodels_to_final = [torch.eye(3, device=device)[None]]\nfor m in models[::-1]:\n    models_to_final.append(m @ models_to_final[-1])\nmodels_to_final = models_to_final[::-1]\n\nLet’s check what do we got.\n\nfig, axis = plt.subplots(2, 5, figsize=(12, 4), sharex=\"all\", sharey=\"all\", frameon=False)\nfor i, fname in enumerate(fnames):\n    if i % 10 != 0:\n        continue\n    timg = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    j = i // 10\n    timg_dst = KG.homography_warp(timg, models_to_final[i], timg.shape[-2:])\n    axis[j // 5][j % 5].imshow(K.tensor_to_image(timg_dst * 255.0).astype(np.uint8), aspect=\"auto\")\nplt.subplots_adjust(wspace=0.05, hspace=0.05)\nfig.tight_layout()\n\n\n\n\n\n\n\n\nFinally we will merge the image sequence into single image. The idea is to detect the image parts, which are in focus from the current image and blend them into the final images. To get the sharp image part we can use kornia.filters.laplacian. Then we reproject image1 into image2, and merge them using mask we created.\n\ndef merge_sharp1_into2(timg1, timg2, trans1to2, verbose=False):\n    curr_img = timg2.clone()\n    warped = KG.homography_warp(timg1, torch.inverse(trans1to2), timg.shape[-2:])\n    mask1 = K.filters.laplacian(K.color.rgb_to_grayscale(timg1), 7).abs()\n    mask1_norm = (mask1 - mask1.min()) / (mask1.max() - mask1.min())\n    mask1_blur = K.filters.gaussian_blur2d(mask1_norm, (9, 9), (1.6, 1.6))\n    mask1_blur = mask1_blur / mask1_blur.max()\n    warped_mask = KG.homography_warp(mask1_blur.float(), torch.inverse(models_to_final[i]), timg1.shape[-2:])\n    curr_img = warped_mask * warped + (1 - warped_mask) * curr_img\n    if verbose:\n        fig, axis = plt.subplots(1, 4, figsize=(15, 6), sharex=\"all\", sharey=\"all\", frameon=False)\n        axis[0].imshow(K.tensor_to_image(timg1))\n        axis[1].imshow(K.tensor_to_image(mask1_blur))\n        axis[2].imshow(K.tensor_to_image(timg2))\n        axis[3].imshow(K.tensor_to_image(curr_img))\n        axis[0].set_title(\"Img1\")\n        axis[1].set_title(\"Sharp mask on img1\")\n        axis[2].set_title(\"Img2\")\n        axis[3].set_title(\"Blended image\")\n    return curr_img\n\n\ntimg1 = K.io.load_image(fnames[50], K.io.ImageLoadType.RGB32, device=device)[None, ...]\ntimg2 = K.io.load_image(fnames[-1], K.io.ImageLoadType.RGB32, device=device)[None, ...]\nout = merge_sharp1_into2(timg1, timg2, models_to_final[50], True)\n\n\n\n\n\n\n\n\nThe blending does not look really good, but that is because we are trying to merge non-consequtive images with very different focus. Let’s try to apply it sequentially and see, what happens.\nWe will also create a video of our sharpening process.\n\n%%capture\nbase_img = K.io.load_image(fnames[-1], K.io.ImageLoadType.RGB32, device=device)[None, ...]\ncurr_img = deepcopy(base_img)\n\ntry:\n    video_writer = imageio.get_writer(\"sharpening.avi\", fps=8)\n    video_writer.append_data((K.tensor_to_image(curr_img) * 255.0).astype(np.uint8))\n    video_ok = True\nexcept:\n    video_ok = False\n\n\nwith torch.no_grad():\n    for i, fname in tqdm(enumerate(fnames)):\n        timg = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n        curr_img = merge_sharp1_into2(timg.to(device), curr_img.to(device), models_to_final[i].to(device))\n        if video_ok:\n            video_writer.append_data((K.tensor_to_image(curr_img) * 255.0).astype(np.uint8))\nif video_ok:\n    video_writer.close()\n\n\nplt.imshow(K.tensor_to_image(curr_img.float()))\nplt.title(\"Final result\")\n\nText(0.5, 1.0, 'Final result')\n\n\n\n\n\n\n\n\n\nNow we can play the video of our sharpening. The code is ugly to allow running from Google Colab (as shown here)\n\nfrom base64 import b64encode\n\nfrom IPython.display import HTML\n\nif video_ok:\n    mp4 = open(\"sharpening.avi\", \"rb\").read()\nelse:\n    mp4 = open(get_data_directory(\"\") + \"sharpening.mp4\", \"rb\").read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n\n\nHTML(\n    f\"\"\"\n&lt;video width=400 controls&gt;\n      &lt;source src=\"{data_url}\" type=\"video/mp4\"&gt;\n&lt;/video&gt;\n\"\"\"\n)\n\n\n\n      \n\n\n\nResult looks quite nice and more detailed, although a bit soft. You can try yourself different blending parameters yourself (e.g. blur kernel size) in order to improve the final result."
  },
  {
    "objectID": "nbs/data_augmentation_2d.html",
    "href": "nbs/data_augmentation_2d.html",
    "title": "Data Augmentation 2D",
    "section": "",
    "text": "Just a simple examples showing the Augmentations available on Kornia.\nFor more information check the docs: https://kornia.readthedocs.io/en/latest/augmentation.module.html\n%%capture\n!pip install kornia\n!pip install kornia-rs\nimport kornia\nimport matplotlib.pyplot as plt\nfrom kornia.augmentation import (\n    CenterCrop,\n    ColorJiggle,\n    ColorJitter,\n    PadTo,\n    RandomAffine,\n    RandomBoxBlur,\n    RandomBrightness,\n    RandomChannelShuffle,\n    RandomContrast,\n    RandomCrop,\n    RandomCutMixV2,\n    RandomElasticTransform,\n    RandomEqualize,\n    RandomErasing,\n    RandomFisheye,\n    RandomGamma,\n    RandomGaussianBlur,\n    RandomGaussianNoise,\n    RandomGrayscale,\n    RandomHorizontalFlip,\n    RandomHue,\n    RandomInvert,\n    RandomJigsaw,\n    RandomMixUpV2,\n    RandomMosaic,\n    RandomMotionBlur,\n    RandomPerspective,\n    RandomPlanckianJitter,\n    RandomPlasmaBrightness,\n    RandomPlasmaContrast,\n    RandomPlasmaShadow,\n    RandomPosterize,\n    RandomResizedCrop,\n    RandomRGBShift,\n    RandomRotation,\n    RandomSaturation,\n    RandomSharpness,\n    RandomSolarize,\n    RandomThinPlateSpline,\n    RandomVerticalFlip,\n)"
  },
  {
    "objectID": "nbs/data_augmentation_2d.html#load-an-image",
    "href": "nbs/data_augmentation_2d.html#load-an-image",
    "title": "Data Augmentation 2D",
    "section": "Load an Image",
    "text": "Load an Image\nThe augmentations expects an image with shape BxCxHxW\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://raw.githubusercontent.com/kornia/data/main/panda.jpg\"\ndownload_image(url)\n\n'panda.jpg'\n\n\n\nimg_type = kornia.io.ImageLoadType.RGB32\nimg = kornia.io.load_image(\"panda.jpg\", img_type, \"cpu\")[None]\n\n\ndef plot_tensor(data, title=\"\"):\n    b, c, h, w = data.shape\n\n    fig, axes = plt.subplots(1, b, dpi=150, subplot_kw={\"aspect\": \"equal\"})\n    if b == 1:\n        axes = [axes]\n\n    for idx, ax in enumerate(axes):\n        ax.imshow(kornia.utils.tensor_to_image(data[idx, ...]))\n        ax.set_ylim(h, 0)\n        ax.set_xlim(0, w)\n        ax.tick_params(top=True, labeltop=True, bottom=False, labelbottom=False)\n    fig.suptitle(title)\n    plt.show()\n\n\nplot_tensor(img, \"panda\")"
  },
  {
    "objectID": "nbs/data_augmentation_2d.html#d-transforms",
    "href": "nbs/data_augmentation_2d.html#d-transforms",
    "title": "Data Augmentation 2D",
    "section": "2D transforms",
    "text": "2D transforms\nSometimes you may wish to apply the exact same transformations on all the elements in one batch. Here, we provided a same_on_batch keyword to all random generators for you to use. Instead of an element-wise parameter generating, it will generate exact same parameters across the whole batch.\n\n# Create a batched input\nnum_samples = 2\n\ninpt = img.repeat(num_samples, 1, 1, 1)\n\n\nIntensity\n\nRandom Planckian Jitter\n\nrandomplanckianjitter = RandomPlanckianJitter(\"blackbody\", same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomplanckianjitter(inpt), \"Planckian Jitter\")\n\n\n\n\n\n\n\n\n\n\nRandom Plasma Shadow\n\nrandomplasmashadow = RandomPlasmaShadow(\n    roughness=(0.1, 0.7), shade_intensity=(-1.0, 0.0), shade_quantity=(0.0, 1.0), same_on_batch=False, keepdim=False, p=1.0\n)\n\nplot_tensor(randomplasmashadow(inpt), \"Plasma Shadow\")\n\n\n\n\n\n\n\n\n\n\nRandom Plasma Brightness\n\nrandomplasmabrightness = RandomPlasmaBrightness(\n    roughness=(0.1, 0.7), intensity=(0.0, 1.0), same_on_batch=False, keepdim=False, p=1.0\n)\nplot_tensor(randomplasmabrightness(inpt), \"Plasma Brightness\")\n\n\n\n\n\n\n\n\n\n\nRandom Plasma Contrast\n\nrandomplasmacontrast = RandomPlasmaContrast(roughness=(0.1, 0.7), same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomplasmacontrast(inpt), \"Plasma Contrast\")\n\n\n\n\n\n\n\n\n\n\nColor Jiggle\n\ncolorjiggle = ColorJiggle(0.3, 0.3, 0.3, 0.3, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(colorjiggle(inpt), \"Color Jiggle\")\n\n\n\n\n\n\n\n\n\n\nColor Jitter\n\ncolorjitter = ColorJitter(0.3, 0.3, 0.3, 0.3, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(colorjitter(inpt), \"Color Jitter\")\n\n\n\n\n\n\n\n\n\n\nRandom Box Blur\n\nrandomboxblur = RandomBoxBlur((21, 5), \"reflect\", same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomboxblur(inpt), \"Box Blur\")\n\n\n\n\n\n\n\n\n\n\nRandom Brightness\n\nrandombrightness = RandomBrightness(brightness=(0.8, 1.2), clip_output=True, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randombrightness(inpt), \"Random Brightness\")\n\n\n\n\n\n\n\n\n\n\nRandom Channel Shuffle\n\nrandomchannelshuffle = RandomChannelShuffle(same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomchannelshuffle(inpt), \"Random Channel Shuffle\")\n\n\n\n\n\n\n\n\n\n\nRandom Contrast\n\nrandomcontrast = RandomContrast(contrast=(0.8, 1.2), clip_output=True, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomcontrast(inpt), \"Random Contrast\")\n\n\n\n\n\n\n\n\n\n\nRandom Equalize\n\nrandomequalize = RandomEqualize(same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomequalize(inpt), \"Random Equalize\")\n\n\n\n\n\n\n\n\n\n\nRandom Gamma\n\nrandomgamma = RandomGamma((0.2, 1.3), (1.0, 1.5), same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomgamma(inpt), \"Random Gamma\")\n\n\n\n\n\n\n\n\n\n\nRandom Grayscale\n\nrandomgrayscale = RandomGrayscale(same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomgrayscale(inpt), \"Random Grayscale\")\n\n\n\n\n\n\n\n\n\n\nRandom Gaussian Blur\n\nrandomgaussianblur = RandomGaussianBlur((21, 21), (0.2, 1.3), \"reflect\", same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomgaussianblur(inpt), \"Random Gaussian Blur\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n\nRandom Gaussian Noise\n\nrandomgaussiannoise = RandomGaussianNoise(mean=0.2, std=0.7, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomgaussiannoise(inpt), \"Random Gaussian Noise\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n\nRandom Hue\n\nrandomhue = RandomHue((-0.2, 0.4), same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomhue(inpt), \"Random Hue\")\n\n\n\n\n\n\n\n\n\n\nRandom Motion Blur\n\nrandommotionblur = RandomMotionBlur((7, 7), 35.0, 0.5, \"reflect\", \"nearest\", same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randommotionblur(inpt), \"Random Motion Blur\")\n\n\n\n\n\n\n\n\n\n\nRandom Posterize\n\nrandomposterize = RandomPosterize(bits=3, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomposterize(inpt), \"Random Posterize\")\n\n\n\n\n\n\n\n\n\n\nRandom RGB Shift\n\nrandomrgbshift = RandomRGBShift(\n    r_shift_limit=0.5, g_shift_limit=0.5, b_shift_limit=0.5, same_on_batch=False, keepdim=False, p=1.0\n)\nplot_tensor(randomrgbshift(inpt), \"Random RGB Shift\")\n\n\n\n\n\n\n\n\n\n\nRandom Saturation\n\nrandomsaturation = RandomSaturation((1.0, 1.0), same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomsaturation(inpt), \"Random Saturation\")\n\n\n\n\n\n\n\n\n\n\nRandom Sharpness\n\nrandomsharpness = RandomSharpness((0.5, 1.0), same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomsharpness(inpt), \"Random Sharpness\")\n\n\n\n\n\n\n\n\n\n\nRandom Solarize\n\nrandomsolarize = RandomSolarize(0.3, 0.1, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomsolarize(inpt), \"Random Solarize\")\n\n\n\n\n\n\n\n\n\n\n\nGeometric\n\nCenter Crop\n\ncentercrop = CenterCrop(150, resample=\"nearest\", cropping_mode=\"resample\", align_corners=True, keepdim=False, p=1.0)\n\nplot_tensor(centercrop(inpt), \"Center Crop\")\n\n\n\n\n\n\n\n\n\n\nPad To\n\npadto = PadTo((500, 500), \"constant\", 1, keepdim=False)\n\nplot_tensor(padto(inpt), \"Pad To\")\n\n\n\n\n\n\n\n\n\n\nRandom Affine\n\nrandomaffine = RandomAffine(\n    (-15.0, 5.0),\n    (0.3, 1.0),\n    (0.4, 1.3),\n    0.5,\n    resample=\"nearest\",\n    padding_mode=\"reflection\",\n    align_corners=True,\n    same_on_batch=False,\n    keepdim=False,\n    p=1.0,\n)\nplot_tensor(randomaffine(inpt), \"Random Affine\")\n\n\n\n\n\n\n\n\n\n\nRandom Crop\n\nrandomcrop = RandomCrop(\n    (150, 150),\n    10,\n    True,\n    1,\n    \"constant\",\n    \"nearest\",\n    cropping_mode=\"resample\",\n    same_on_batch=False,\n    align_corners=True,\n    keepdim=False,\n    p=1.0,\n)\n\nplot_tensor(randomcrop(inpt), \"Random Crop\")\n\n\n\n\n\n\n\n\n\n\nRandom Erasing\n\nrandomerasing = RandomErasing(scale=(0.02, 0.33), ratio=(0.3, 3.3), value=1, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomerasing(inpt), \"Random Erasing\")\n\n\n\n\n\n\n\n\n\n\nRandom Elastic Transform\n\nrandomelastictransform = RandomElasticTransform(\n    (27, 27), (33, 31), (0.5, 1.5), align_corners=True, padding_mode=\"reflection\", same_on_batch=False, keepdim=False, p=1.0\n)\n\nplot_tensor(randomelastictransform(inpt), \"Random Elastic Transform\")\n\n\n\n\n\n\n\n\n\n\nRandom Fish Eye\n\nc = kornia.core.tensor([-0.3, 0.3])\ng = kornia.core.tensor([0.9, 1.0])\nrandomfisheye = RandomFisheye(c, c, g, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomfisheye(inpt), \"Random Fish Eye\")\n\n\n\n\n\n\n\n\n\n\nRandom Horizontal Flip\n\nrandomhorizontalflip = RandomHorizontalFlip(same_on_batch=False, keepdim=False, p=0.7)\n\nplot_tensor(randomhorizontalflip(inpt), \"Random Horizontal Flip\")\n\n\n\n\n\n\n\n\n\n\nRandom Invert\n\nrandominvert = RandomInvert(same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randominvert(inpt), \"Random Invert\")\n\n\n\n\n\n\n\n\n\n\nRandom Perspective\n\nrandomperspective = RandomPerspective(0.5, \"nearest\", align_corners=True, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomperspective(inpt), \"Random Perspective\")\n\n\n\n\n\n\n\n\n\n\nRandom Resized Crop\n\nrandomresizedcrop = RandomResizedCrop(\n    (200, 200),\n    (0.4, 1.0),\n    (2.0, 2.0),\n    \"nearest\",\n    align_corners=True,\n    cropping_mode=\"resample\",\n    same_on_batch=False,\n    keepdim=False,\n    p=1.0,\n)\n\nplot_tensor(randomresizedcrop(inpt), \"Random Resized Crop\")\n\n\n\n\n\n\n\n\n\n\nRandom Rotation\n\nrandomrotation = RandomRotation(15.0, \"nearest\", align_corners=True, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomrotation(inpt), \"Random Rotation\")\n\n\n\n\n\n\n\n\n\n\nRandom Vertical Flip\n\nrandomverticalflip = RandomVerticalFlip(same_on_batch=False, keepdim=False, p=0.6, p_batch=1.0)\n\nplot_tensor(randomverticalflip(inpt), \"Random Vertical Flip\")\n\n\n\n\n\n\n\n\n\n\nRandom Thin Plate Spline\n\nrandomthinplatespline = RandomThinPlateSpline(0.6, align_corners=True, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomverticalflip(inpt), \"Random Thin Plate Spline\")\n\n\n\n\n\n\n\n\n\n\n\nMix\n\nRandom Cut Mix\n\nrandomcutmixv2 = RandomCutMixV2(4, (0.2, 0.9), 0.1, same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randomcutmixv2(inpt), \"Random Cut Mix\")\n\n\n\n\n\n\n\n\n\n\nRandom Mix Up\n\nrandommixupv2 = RandomMixUpV2((0.1, 0.9), same_on_batch=False, keepdim=False, p=1.0)\n\nplot_tensor(randommixupv2(inpt), \"Random Mix Up\")\n\n\n\n\n\n\n\n\n\n\nRandom Mosaic\n\nrandommosaic = RandomMosaic(\n    (250, 125),\n    (4, 4),\n    (0.3, 0.7),\n    align_corners=True,\n    cropping_mode=\"resample\",\n    padding_mode=\"reflect\",\n    resample=\"nearest\",\n    keepdim=False,\n    p=1.0,\n)\nplot_tensor(randommosaic(inpt), \"Random Mosaic\")\n\n\n\n\n\n\n\n\n\n\nRandom Jigsaw\n\n# randomjigsaw = RandomJigsaw((2, 2), ensure_perm=False, same_on_batch=False, keepdim=False, p=1.0)\n\n\n# plot_tensor(randomjigsaw(inpt), \"Random Jigsaw\")"
  },
  {
    "objectID": "nbs/resize_antialias.html#install-kornia",
    "href": "nbs/resize_antialias.html#install-kornia",
    "title": "Resize anti-alias",
    "section": "Install Kornia",
    "text": "Install Kornia\n\n%%capture\n!pip install kornia\n!pip install kornia-rs"
  },
  {
    "objectID": "nbs/resize_antialias.html#prepare-the-data",
    "href": "nbs/resize_antialias.html#prepare-the-data",
    "title": "Resize anti-alias",
    "section": "Prepare the data",
    "text": "Prepare the data\nDownload an example image\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/drslump.jpg\"\ndownload_image(url)\n\n'drslump.jpg'\n\n\n\nimport kornia as K\nimport torch\nfrom matplotlib import pyplot as plt\n\n\ndef imshow(input: torch.Tensor):\n    B = input.shape[0]\n    fig, axes = plt.subplots(ncols=B, nrows=1, figsize=(20, 10))\n    axes = axes if B &gt; 1 else [axes]\n    for idx, ax in enumerate(axes):\n        ax.imshow(K.utils.tensor_to_image(input[idx]))\n        ax.axis(\"off\")\n\n\ndata: torch.Tensor = K.io.load_image(\"drslump.jpg\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\n# plot\nimshow(data)"
  },
  {
    "objectID": "nbs/resize_antialias.html#plain-resize-vs-antializased-resize",
    "href": "nbs/resize_antialias.html#plain-resize-vs-antializased-resize",
    "title": "Resize anti-alias",
    "section": "Plain resize vs Antializased resize",
    "text": "Plain resize vs Antializased resize\n\nx_025: torch.Tensor = K.geometry.rescale(data, (0.125, 0.125))\nx_025AA: torch.Tensor = K.geometry.rescale(data, (0.125, 0.125), antialias=True)\nout = torch.stack([x_025, x_025AA], dim=0)\nimshow(out)"
  },
  {
    "objectID": "nbs/data_augmentation_kornia_lightning.html#install-kornia-and-pytorch-lightning",
    "href": "nbs/data_augmentation_kornia_lightning.html#install-kornia-and-pytorch-lightning",
    "title": "Kornia and PyTorch Lightning GPU data augmentation",
    "section": "Install Kornia and PyTorch Lightning",
    "text": "Install Kornia and PyTorch Lightning\nWe first install Kornia and PyTorch Lightning\n\n%%capture\n!pip install kornia\n!pip install kornia-rs\n!pip install pytorch_lightning torchmetrics\n\nImport the needed libraries\n\nimport os\n\nimport kornia as K\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torchmetrics\nfrom PIL import Image\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10"
  },
  {
    "objectID": "nbs/data_augmentation_kornia_lightning.html#define-data-augmentations-module",
    "href": "nbs/data_augmentation_kornia_lightning.html#define-data-augmentations-module",
    "title": "Kornia and PyTorch Lightning GPU data augmentation",
    "section": "Define Data Augmentations module",
    "text": "Define Data Augmentations module\n\nclass DataAugmentation(nn.Module):\n    \"\"\"Module to perform data augmentation using Kornia on torch tensors.\"\"\"\n\n    def __init__(self, apply_color_jitter: bool = False) -&gt; None:\n        super().__init__()\n        self._apply_color_jitter = apply_color_jitter\n\n        self._max_val: float = 255.0\n\n        self.transforms = nn.Sequential(K.enhance.Normalize(0.0, self._max_val), K.augmentation.RandomHorizontalFlip(p=0.5))\n\n        self.jitter = K.augmentation.ColorJitter(0.5, 0.5, 0.5, 0.5)\n\n    @torch.no_grad()  # disable gradients for effiency\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x_out = self.transforms(x)  # BxCxHxW\n        if self._apply_color_jitter:\n            x_out = self.jitter(x_out)\n        return x_out"
  },
  {
    "objectID": "nbs/data_augmentation_kornia_lightning.html#define-a-pre-processing-model",
    "href": "nbs/data_augmentation_kornia_lightning.html#define-a-pre-processing-model",
    "title": "Kornia and PyTorch Lightning GPU data augmentation",
    "section": "Define a Pre-processing model",
    "text": "Define a Pre-processing model\n\nclass PreProcess(nn.Module):\n    \"\"\"Module to perform pre-process using Kornia on torch tensors.\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    @torch.no_grad()  # disable gradients for effiency\n    def forward(self, x: Image) -&gt; torch.Tensor:\n        x_tmp: np.ndarray = np.array(x)  # HxWxC\n        x_out: torch.Tensor = K.image_to_tensor(x_tmp, keepdim=True)  # CxHxW\n        return x_out.float()"
  },
  {
    "objectID": "nbs/data_augmentation_kornia_lightning.html#define-pytorch-lightning-model",
    "href": "nbs/data_augmentation_kornia_lightning.html#define-pytorch-lightning-model",
    "title": "Kornia and PyTorch Lightning GPU data augmentation",
    "section": "Define PyTorch Lightning model",
    "text": "Define PyTorch Lightning model\n\nclass CoolSystem(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # not the best model...\n        self.l1 = torch.nn.Linear(3 * 32 * 32, 10)\n\n        self.preprocess = PreProcess()\n\n        self.transform = DataAugmentation()\n\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n\n    def forward(self, x):\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n    def training_step(self, batch, batch_idx):\n        # REQUIRED\n        x, y = batch\n        x_aug = self.transform(x)  # =&gt; we perform GPU/Batched data augmentation\n        logits = self.forward(x_aug)\n        loss = F.cross_entropy(logits, y)\n        self.log(\"train_acc_step\", self.accuracy(logits.argmax(1), y))\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        # OPTIONAL\n        x, y = batch\n        logits = self.forward(x)\n        self.log(\"val_acc_step\", self.accuracy(logits.argmax(1), y))\n        return F.cross_entropy(logits, y)\n\n    def test_step(self, batch, batch_idx):\n        # OPTIONAL\n        x, y = batch\n        logits = self.forward(x)\n        acc = self.accuracy(logits.argmax(1), y)\n        self.log(\"test_acc_step\", acc)\n        return acc\n\n    def configure_optimizers(self):\n        # REQUIRED\n        # can return multiple optimizers and learning_rate schedulers\n        # (LBFGS it is automatically supported, no need for closure function)\n        return torch.optim.Adam(self.parameters(), lr=0.0004)\n\n    def prepare_data(self):\n        CIFAR10(os.getcwd(), train=True, download=True, transform=self.preprocess)\n        CIFAR10(os.getcwd(), train=False, download=True, transform=self.preprocess)\n\n    def train_dataloader(self):\n        # REQUIRED\n        dataset = CIFAR10(os.getcwd(), train=True, download=False, transform=self.preprocess)\n        loader = DataLoader(dataset, batch_size=32, num_workers=1)\n        return loader\n\n    def val_dataloader(self):\n        dataset = CIFAR10(os.getcwd(), train=True, download=False, transform=self.preprocess)\n        loader = DataLoader(dataset, batch_size=32, num_workers=1)\n        return loader\n\n    def test_dataloader(self):\n        dataset = CIFAR10(os.getcwd(), train=False, download=False, transform=self.preprocess)\n        loader = DataLoader(dataset, batch_size=16, num_workers=1)\n        return loader"
  },
  {
    "objectID": "nbs/data_augmentation_kornia_lightning.html#run-training",
    "href": "nbs/data_augmentation_kornia_lightning.html#run-training",
    "title": "Kornia and PyTorch Lightning GPU data augmentation",
    "section": "Run training",
    "text": "Run training\n\nfrom pytorch_lightning import Trainer\n\n# init model\nmodel = CoolSystem()\n\n# Initialize a trainer\naccelerator = \"cpu\"  # can be 'gpu'\n\ntrainer = Trainer(accelerator=accelerator, max_epochs=1, enable_progress_bar=False)\n\n# Train the model ⚡\ntrainer.fit(model)\n\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n\n\n\n\n\n  | Name       | Type               | Params\n--------------------------------------------------\n0 | l1         | Linear             | 30.7 K\n1 | preprocess | PreProcess         | 0     \n2 | transform  | DataAugmentation   | 0     \n3 | accuracy   | MulticlassAccuracy | 0     \n--------------------------------------------------\n30.7 K    Trainable params\n0         Non-trainable params\n30.7 K    Total params\n0.123     Total estimated model params size (MB)\n`Trainer.fit` stopped: `max_epochs=1` reached."
  },
  {
    "objectID": "nbs/data_augmentation_kornia_lightning.html#visualize",
    "href": "nbs/data_augmentation_kornia_lightning.html#visualize",
    "title": "Kornia and PyTorch Lightning GPU data augmentation",
    "section": "Visualize",
    "text": "Visualize\n\n# # Start tensorboard.\n# %load_ext tensorboard\n# %tensorboard --logdir lightning_logs/"
  },
  {
    "objectID": "nbs/image_points_transforms.html#install-and-get-data",
    "href": "nbs/image_points_transforms.html#install-and-get-data",
    "title": "Image and Keypoints augmentations",
    "section": "Install and get data",
    "text": "Install and get data\n\n%%capture\n!pip install kornia\n!pip install kornia-rs matplotlib\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_image(\"https://github.com/kornia/data/raw/main/arturito.jpg\")\n\n'arturito.jpg'\n\n\n\nimport kornia as K\nimport torch\nfrom matplotlib import pyplot as plt\n\n\nimg = K.io.load_image(\"arturito.jpg\", K.io.ImageLoadType.RGB32)\nimg = img[None]  # 1xCxHxW / fp32 / [0, 1]\nprint(img.shape)\n\ntorch.Size([1, 3, 144, 256])"
  },
  {
    "objectID": "nbs/image_points_transforms.html#draw-points-and-show-image",
    "href": "nbs/image_points_transforms.html#draw-points-and-show-image",
    "title": "Image and Keypoints augmentations",
    "section": "Draw points and show image",
    "text": "Draw points and show image\n\ncoords = torch.tensor([[[125, 40.0], [185.0, 75.0]]])  # BxNx2 [x,y]\n\nfig, ax = plt.subplots()\n\nax.add_patch(plt.Circle((coords[0, 0, 0], coords[0, 0, 1]), color=\"r\"))\nax.add_patch(plt.Circle((coords[0, 1, 0], coords[0, 1, 1]), color=\"r\"))\n\nax.imshow(K.tensor_to_image(img))"
  },
  {
    "objectID": "nbs/image_points_transforms.html#resize-points-and-show",
    "href": "nbs/image_points_transforms.html#resize-points-and-show",
    "title": "Image and Keypoints augmentations",
    "section": "Resize points and show",
    "text": "Resize points and show\n\nresize_op = K.augmentation.AugmentationSequential(\n    K.augmentation.Resize((100, 200), antialias=True), data_keys=[\"input\", \"keypoints\"]\n)\n\nprint(resize_op.transform_matrix)\n\nimg_resize, coords_resize = resize_op(img, coords)\n\n\nfig, ax = plt.subplots()\n\nax.add_patch(plt.Circle((coords_resize[0, 0, 0], coords_resize[0, 0, 1]), color=\"r\"))\nax.add_patch(plt.Circle((coords_resize[0, 1, 0], coords_resize[0, 1, 1]), color=\"r\"))\n\nax.imshow(K.tensor_to_image(img_resize))\n\nNone"
  },
  {
    "objectID": "nbs/image_points_transforms.html#crop-image-and-points",
    "href": "nbs/image_points_transforms.html#crop-image-and-points",
    "title": "Image and Keypoints augmentations",
    "section": "Crop image and points",
    "text": "Crop image and points\n\ncrop_op = K.augmentation.AugmentationSequential(K.augmentation.CenterCrop((100, 200)), data_keys=[\"input\", \"keypoints\"])\nprint(crop_op.transform_matrix)\n\nimg_resize, coords_resize = crop_op(img, coords)\n\n\nfig, ax = plt.subplots()\n\nax.add_patch(plt.Circle((coords_resize[0, 0, 0], coords_resize[0, 0, 1]), color=\"r\"))\nax.add_patch(plt.Circle((coords_resize[0, 1, 0], coords_resize[0, 1, 1]), color=\"r\"))\n\nax.imshow(K.tensor_to_image(img_resize))\n\nNone"
  },
  {
    "objectID": "nbs/face_detection.html",
    "href": "nbs/face_detection.html",
    "title": "Face Detection and blurring",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://raw.githubusercontent.com/kornia/data/main/crowd.jpg\"\ndownload_image(url)\nImport the needed libraries\nimport cv2\nimport kornia as K\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom kornia.contrib import FaceDetector, FaceDetectorResult\n\n# select the device and type\ndevice = torch.device(\"cpu\")  # use 'cuda:0'\ndtype = torch.float32\nRead the image with kornia\n# load the image (face detector expects a image in rage 0-255 (8 bits))\nimg = K.io.load_image(\"crowd.jpg\", K.io.ImageLoadType.RGB8, device=device)[None, ...].to(dtype=dtype)  # BxCxHxW\nimg_vis = K.tensor_to_image(img.byte())  # to later visualize\nplt.figure(figsize=(8, 8))\nplt.imshow(img_vis)\nplt.axis(\"off\")\nplt.show()\nCreate the FaceDetector object and apply to the image\n# create the detector and find the faces !\nface_detection = FaceDetector().to(device, dtype)\n\nwith torch.no_grad():\n    dets = face_detection(img)\n\n# to decode later the detections\ndets = [FaceDetectorResult(o) for o in dets]\nCreate a function to crop the faces from the original image and apply blurring using the gaussian_blurd2d operator.\nAlternatively, explore other blur operator in kornia.filters.\n# blurring paramters\nk: int = 21  # kernel_size\ns: float = 35.0  # sigma\n\n\ndef apply_blur_face(img: torch.Tensor, img_vis: np.ndarray, x1, y1, x2, y2):\n    # crop the face\n    roi = img[..., y1:y2, x1:x2]\n\n    # apply blurring and put back to the visualisation image\n    roi = K.filters.gaussian_blur2d(roi, (k, k), (s, s))\n    img_vis[y1:y2, x1:x2] = K.tensor_to_image(roi)\nLet draw the detections and save/visualize the image\nfor b in dets:\n    # draw face bounding box around each detected face\n    top_left = b.top_left.int().tolist()\n    bottom_right = b.bottom_right.int().tolist()\n    scores = b.score.tolist()\n\n    for score, tp, br in zip(scores, top_left, bottom_right):\n        x1, y1 = tp\n        x2, y2 = br\n\n        if score &lt; 0.7:\n            continue  # skip detection with low score\n        img_vis = cv2.rectangle(img_vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        # blur the detected faces\n        apply_blur_face(img, img_vis, x1, y1, x2, y2)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(img_vis)\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "nbs/face_detection.html#play-with-the-real-time-demo",
    "href": "nbs/face_detection.html#play-with-the-real-time-demo",
    "title": "Face Detection and blurring",
    "section": "Play with the Real Time Demo",
    "text": "Play with the Real Time Demo\nYou can achieve 60 FPS in CPU using a standard WebCam.\nSee: https://github.com/kornia/kornia/blob/master/examples/face_detection/main_video.py\n\nfrom IPython.display import YouTubeVideo\n\nYouTubeVideo(\"hzQroGp5FSQ\")"
  },
  {
    "objectID": "nbs/warp_perspective.html#install-libraries-and-get-the-data",
    "href": "nbs/warp_perspective.html#install-libraries-and-get-the-data",
    "title": "Warp image using perspective transform",
    "section": "Install libraries and get the data",
    "text": "Install libraries and get the data\n\n%%capture\n!pip install kornia\n!pip install kornia-rs\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/bruce.png\"\ndownload_image(url)\n\n'bruce.png'"
  },
  {
    "objectID": "nbs/warp_perspective.html#import-libraries-and-load-the-data",
    "href": "nbs/warp_perspective.html#import-libraries-and-load-the-data",
    "title": "Warp image using perspective transform",
    "section": "Import libraries and load the data",
    "text": "Import libraries and load the data\n\nimport cv2\nimport kornia as K\nimport matplotlib.pyplot as plt\nimport torch\n\n\nimg = K.io.load_image(\"bruce.png\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\nprint(img.shape)\n\ntorch.Size([1, 3, 372, 600])"
  },
  {
    "objectID": "nbs/warp_perspective.html#define-the-points-to-warp-compute-the-homography-and-warp",
    "href": "nbs/warp_perspective.html#define-the-points-to-warp-compute-the-homography-and-warp",
    "title": "Warp image using perspective transform",
    "section": "Define the points to warp, compute the homography and warp",
    "text": "Define the points to warp, compute the homography and warp\n\n# the source points are the region to crop corners\npoints_src = torch.tensor(\n    [\n        [\n            [125.0, 150.0],\n            [562.0, 40.0],\n            [562.0, 282.0],\n            [54.0, 328.0],\n        ]\n    ]\n)\n\n# the destination points are the image vertexes\nh, w = 64, 128  # destination size\npoints_dst = torch.tensor(\n    [\n        [\n            [0.0, 0.0],\n            [w - 1.0, 0.0],\n            [w - 1.0, h - 1.0],\n            [0.0, h - 1.0],\n        ]\n    ]\n)\n\n# compute perspective transform\nM: torch.tensor = K.geometry.get_perspective_transform(points_src, points_dst)\n\n# warp the original image by the found transform\nimg_warp: torch.tensor = K.geometry.warp_perspective(img.float(), M, dsize=(h, w))\nprint(img_warp.shape)\n\ntorch.Size([1, 3, 64, 128])"
  },
  {
    "objectID": "nbs/warp_perspective.html#plot-the-warped-data",
    "href": "nbs/warp_perspective.html#plot-the-warped-data",
    "title": "Warp image using perspective transform",
    "section": "Plot the warped data",
    "text": "Plot the warped data\n\n# convert back to numpy\nimg_np = K.tensor_to_image(img)\nimg_warp_np = K.tensor_to_image(img_warp)\n\n# draw points into original image\nfor i in range(4):\n    center = tuple(points_src[0, i].long().numpy())\n    img_np = cv2.circle(img_np.copy(), center, 5, (0, 255, 0), -1)\n\n# create the plot\nfig, axs = plt.subplots(1, 2, figsize=(16, 10))\naxs = axs.ravel()\n\naxs[0].axis(\"off\")\naxs[0].set_title(\"image source\")\naxs[0].imshow(img_np)\n\naxs[1].axis(\"off\")\naxs[1].set_title(\"image destination\")\naxs[1].imshow(img_warp_np)\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "nbs/visual_prompter.html",
    "href": "nbs/visual_prompter.html",
    "title": "Visual Prompter: Segment Anything",
    "section": "",
    "text": "This tutorials shows how to use our high-level API Visual Prompter. This API allow to set an image, and run multiple queries multiple times on this image. These query can be done with three types of prompt:\nRead more on our docs: https://kornia.readthedocs.io/en/latest/models/segment_anything.html\nThis tutorials steps:"
  },
  {
    "objectID": "nbs/visual_prompter.html#setup",
    "href": "nbs/visual_prompter.html#setup",
    "title": "Visual Prompter: Segment Anything",
    "section": "Setup",
    "text": "Setup\n\n%%capture\n!pip install kornia\n!pip install kornia-rs\n\nFirst let’s choose the SAM type to be used on our Visual Prompter.\nThe options are (smaller to bigger):\n\n\n\n\n\n\n\nmodel_type\ncheckpoint official\n\n\n\n\nvit_b\nhttps://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n\n\nvit_l\nhttps://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n\n\nvit_h\nhttps://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n\n\n\n\nmodel_type = \"vit_h\"\ncheckpoint = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n\nThen let’s import all necessary packages and modules\n\nfrom __future__ import annotations\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom kornia.contrib.models.sam import SamConfig\nfrom kornia.contrib.visual_prompter import VisualPrompter\nfrom kornia.geometry.boxes import Boxes\nfrom kornia.geometry.keypoints import Keypoints\nfrom kornia.io import ImageLoadType, load_image\nfrom kornia.utils import get_cuda_or_mps_device_if_available, tensor_to_image\n\n\ndevice = get_cuda_or_mps_device_if_available()\nprint(device)\n\nNone\n\n\n\nUtilities functions\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nsoccer_image_path = download_image(\"https://raw.githubusercontent.com/kornia/data/main/soccer.jpg\")\ncar_image_path = download_image(\"https://raw.githubusercontent.com/kornia/data/main/simple_car.jpg\")\nsatellite_image_path = download_image(\"https://raw.githubusercontent.com/kornia/data/main/satellite_sentinel2_example.tif\")\nsoccer_image_path, car_image_path, satellite_image_path\n\n('soccer.jpg', 'simple_car.jpg', 'satellite_sentinel2_example.tif')\n\n\n\ndef colorize_masks(binary_masks: torch.Tensor, merge: bool = True, alpha: None | float = None) -&gt; list[torch.Tensor]:\n    \"\"\"Convert binary masks (B, C, H, W), boolean tensors, into masks with colors (B, (3, 4) , H, W) - RGB or RGBA. Where C refers to the number of masks.\n    Args:\n        binary_masks: a batched boolean tensor (B, C, H, W)\n        merge: If true, will join the batch dimension into a unique mask.\n        alpha: alpha channel value. If None, will generate RGB images\n\n    Returns:\n        A list of `C` colored masks.\n    \"\"\"\n    B, C, H, W = binary_masks.shape\n    OUT_C = 4 if alpha else 3\n\n    output_masks = []\n\n    for idx in range(C):\n        _out = torch.zeros(B, OUT_C, H, W, device=binary_masks.device, dtype=torch.float32)\n        for b in range(B):\n            color = torch.rand(1, 3, 1, 1, device=binary_masks.device, dtype=torch.float32)\n            if alpha:\n                color = torch.cat([color, torch.tensor([[[[alpha]]]], device=binary_masks.device, dtype=torch.float32)], dim=1)\n\n            to_colorize = binary_masks[b, idx, ...].view(1, 1, H, W).repeat(1, OUT_C, 1, 1)\n            _out[b, ...] = torch.where(to_colorize, color, _out[b, ...])\n        output_masks.append(_out)\n\n    if merge:\n        output_masks = [c.max(dim=0)[0] for c in output_masks]\n\n    return output_masks\n\n\ndef show_binary_masks(binary_masks: torch.Tensor, axes) -&gt; None:\n    \"\"\"plot binary masks, with shape (B, C, H, W), where C refers to the number of masks.\n\n    will merge the `B` channel into a unique mask.\n    Args:\n        binary_masks: a batched boolean tensor (B, C, H, W)\n        ax: a list of matplotlib axes with lenght of C\n    \"\"\"\n    colored_masks = colorize_masks(binary_masks, True, 0.6)\n\n    for ax, mask in zip(axes, colored_masks):\n        ax.imshow(tensor_to_image(mask))\n\n\ndef show_boxes(boxes: Boxes, ax) -&gt; None:\n    boxes_tensor = boxes.to_tensor(mode=\"xywh\").detach().cpu().numpy()\n    for box in boxes_tensor:\n        x0, y0, w, h = box\n        ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=\"orange\", facecolor=(0, 0, 0, 0), lw=2))\n\n\ndef show_points(points: tuple[Keypoints, torch.Tensor], ax, marker_size=200):\n    coords, labels = points\n    pos_points = coords[labels == 1].to_tensor().detach().cpu().numpy()\n    neg_points = coords[labels == 0].to_tensor().detach().cpu().numpy()\n\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color=\"green\", marker=\"+\", s=marker_size, linewidth=2)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color=\"red\", marker=\"x\", s=marker_size, linewidth=2)\n\n\nfrom kornia.contrib.models import SegmentationResults\n\n\ndef show_image(image: torch.Tensor):\n    plt.imshow(tensor_to_image(image))\n    plt.axis(\"off\")\n    plt.show()\n\n\ndef show_predictions(\n    image: torch.Tensor,\n    predictions: SegmentationResults,\n    points: tuple[Keypoints, torch.Tensor] | None = None,\n    boxes: Boxes | None = None,\n) -&gt; None:\n    n_masks = predictions.logits.shape[1]\n\n    fig, axes = plt.subplots(1, n_masks, figsize=(21, 16))\n    axes = [axes] if n_masks == 1 else axes\n\n    for idx, ax in enumerate(axes):\n        score = predictions.scores[:, idx, ...].mean()\n        ax.imshow(tensor_to_image(image))\n        ax.set_title(f\"Mask {idx+1}, Score: {score:.3f}\", fontsize=18)\n\n        if points:\n            show_points(points, ax)\n\n        if boxes:\n            show_boxes(boxes, ax)\n\n        ax.axis(\"off\")\n\n    show_binary_masks(predictions.binary_masks, axes)\n    plt.show()"
  },
  {
    "objectID": "nbs/visual_prompter.html#exploring-the-visual-prompter",
    "href": "nbs/visual_prompter.html#exploring-the-visual-prompter",
    "title": "Visual Prompter: Segment Anything",
    "section": "Exploring the Visual Prompter",
    "text": "Exploring the Visual Prompter\nThe VisualPrompter can be initialized from a ModelConfig structure, where now we just have support for the SAM model through the SamConfig. Through this config the VisualPrompter will initialize the SAM model and load the weights (from a path or a URL).\nWhat the VisualPrompter can do? 1. Based on the ModelConfig, besides the model initialization, we will setup the required transformations for the images and prompts using the kornia.augmentation API within the Augmentation sequential container. 1. You can benefit from using the torch.compile(...) API (dynamo) for torch &gt;= 2.0.0 versions. To compile with dynamo we provide the method VisualPrompter.compile(...) which will optimize the right parts of the backend model and the prompter itself. 1. Caching the image features and transformations. With the VisualPrompter.set_image(...) method, we transform the image and already encode it using the model, caching it’s embeddings to query later. 1. Query multiple times with multiple prompts. Using the VisualPrompter.predict(...), where we will query on our cached embeddings using Keypoints, Boxes and Masks as prompt.\nWhat the VisualPrompter and Kornia provides? Easy high-levels structures to be used as prompt, also as the result of the prediction. Using the kornia geometry module you can easily encapsulate the Keypoints and Boxes, which allow the API to be more flexible about the desired mode (mainly for boxes, where we had multiple modes of represent it).\nThe Kornia VisualPrompter and model config for SAM can be imported as follow:\nfrom kornia.contrib.image_prompter import VisualPrompter\nfrom kornia.contrib.models import SamConfig\n\n# Setting up a SamConfig with the model type and checkpoint desired\nconfig = SamConfig(model_type, checkpoint)\n\n# Initialize the VisualPrompter\nprompter = VisualPrompter(config, device=device)\n\n\nSet image\nFirst, before adding the image to the prompter, we need to read the image. For that, we can use kornia.io, which internally uses kornia-rs. If you do not have kornia-rs installed, you can install it with pip install kornia_rs. This API implements the DLPack protocol natively in Rust to reduce the memory footprint during the decoding and type conversion. Allowing us to read the image from the disk directly to a tensor. Note that the image should be scaled within the range [0,1].\n\n# Load the image\nimage = load_image(soccer_image_path, ImageLoadType.RGB32, device)  # 3 x H x W\n\n# Display the loaded image\nshow_image(image)\n\n\n\n\n\n\n\n\nWith the image loaded onto the same device as the model, and with the right shape 3xHxW, we can now set the image in our image prompter. Attention: when doing this, the model will compute the embeddings of this image; this means, we will pass this image through the encoder, which will use a lot of memory. It is possible to use the largest model (vit-h) with a graphic card (GPU) that has at least 8Gb of VRAM.\n\nprompter.set_image(image)\n\nIf no error occurred, the features needed to run queries are now cached. If you want to check this, you can see the status of the prompter.is_image_set property.\n\nprompter.is_image_set\n\nTrue\n\n\n\n\nExamples of prompts\nThe VisualPrompter output will have the same Batch Size that its prompts. Where the output shape will be (B, C, H, W). Where B is the number of input prompts, C is determined by multimask output parameter. If multimask_output is True than C=3, otherwise C=1\n\nKeypoints\nKeypoints prompts is a tensor or a Keypoint structure within coordinates into (x, y). With shape BxNx2.\nFor each coordinate pair, should have a corresponding label, where 0 indicates a background point; 1 indicates a foreground point; These labels should be in a tensor with shape BxN\nThe model will try to find a object within all the foreground points, and without the background points. In other words, the foreground points can be used to select the desired type of data, and the background point to exclude the type of data.\n\nkeypoints_tensor = torch.tensor([[[960, 540]]], device=device, dtype=torch.float32)\nkeypoints = Keypoints(keypoints_tensor)\n\nlabels = torch.tensor([[1]], device=device, dtype=torch.float32)\n\n\n\nBoxes\nBoxes prompts is a tensor a with boxes on “xyxy” format/mode, or a Boxes structure. Tensor should have a shape of BxNx4.\n\nboxes_tensor = torch.tensor([[[1841.7, 739.0, 1906.5, 890.6]]], device=device, dtype=torch.float32)\nboxes = Boxes.from_tensor(boxes_tensor, mode=\"xyxy\")\n\n\n\nMasks\nMasks prompts should be provide from a previous model output, with shape Bx1x256x256\n# first run\npredictions = prompter.prediction(...)\n\n# use previous results as prompt\npredictions = prompter.prediction(..., mask=predictions.logits)\n\n\n\nExample of prediction\n\n# using keypoints\nprediction_by_keypoint = prompter.predict(keypoints, labels, multimask_output=False)\n\nshow_image(prediction_by_keypoint.binary_masks)\n\n\n\n\n\n\n\n\n\n# Using boxes\nprediction_by_boxes = prompter.predict(boxes=boxes, multimask_output=False)\n\nshow_image(prediction_by_boxes.binary_masks)\n\n\n\n\n\n\n\n\n\n\nExploring the prediction result structure\nThe VisualPrompter prediction structure, is a SegmentationResults which has the upscaled (default) logits when output_original_size=True is passed on the predict.\nThe segmentation results have: - logits: Results logits with shape (B, C, H, W), where C refers to the number of predicted masks - scores: The scores from the logits. Shape (B,) - Binary mask generated from logits considering the mask_threshold. The size depends on original_res_logits=True, if false, the binary mask will have the same shape of the logits Bx1x256x256\n\nprediction_by_boxes.scores\n\ntensor([[0.9317]])\n\n\n\nprediction_by_boxes.binary_masks.shape\n\ntorch.Size([1, 1, 1080, 1920])\n\n\n\nprediction_by_boxes.logits.shape\n\ntorch.Size([1, 1, 256, 256])"
  },
  {
    "objectID": "nbs/visual_prompter.html#using-the-visual-prompter-on-examples",
    "href": "nbs/visual_prompter.html#using-the-visual-prompter-on-examples",
    "title": "Visual Prompter: Segment Anything",
    "section": "Using the Visual Prompter on examples",
    "text": "Using the Visual Prompter on examples\n\nSoccer players\nUsing an example image from the dataset: https://www.kaggle.com/datasets/ihelon/football-player-segmentation\nLets segment the persons on the field using boxes\n\n# Prompts\nboxes = Boxes.from_tensor(\n    torch.tensor(\n        [\n            [1841.7000, 739.0000, 1906.5000, 890.6000],\n            [879.3000, 545.9000, 948.2000, 669.2000],\n            [55.7000, 595.0000, 127.4000, 745.9000],\n            [1005.4000, 128.7000, 1031.5000, 212.0000],\n            [387.4000, 424.1000, 438.2000, 539.0000],\n            [921.0000, 377.7000, 963.3000, 483.0000],\n            [1213.2000, 885.8000, 1276.2000, 1060.1000],\n            [40.8900, 725.9600, 105.4100, 886.5800],\n            [848.9600, 283.6200, 896.0600, 368.6200],\n            [1109.6500, 499.0400, 1153.0400, 622.1700],\n            [576.3000, 860.8000, 671.7000, 1018.8000],\n            [1039.8000, 389.9000, 1072.5000, 493.2000],\n            [1647.1000, 315.1000, 1694.0000, 406.0000],\n            [1231.2000, 214.0000, 1294.1000, 297.3000],\n        ],\n        device=device,\n    ),\n    mode=\"xyxy\",\n)\n\n\n# Load the image\nimage = load_image(soccer_image_path, ImageLoadType.RGB32, device)  # 3 x H x W\n\n# Set the image\nprompter.set_image(image)\n\n\npredictions = prompter.predict(boxes=boxes, multimask_output=True)\n\nlet’s see the results, since we used multimask_output=True, the model outputted 3 masks.\n\nshow_predictions(image, predictions, boxes=boxes)\n\n\n\n\n\n\n\n\n\n\nCar parts\nSegmenting car parts of an example from the dataset: https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset\n\n# Prompts\nboxes = Boxes.from_tensor(\n    torch.tensor(\n        [\n            [56.2800, 369.1100, 187.3000, 579.4300],\n            [412.5600, 426.5800, 592.9900, 608.1600],\n            [609.0800, 366.8200, 682.6400, 431.1700],\n            [925.1300, 366.8200, 959.6100, 423.1300],\n            [756.1900, 416.2300, 904.4400, 473.7000],\n            [489.5600, 285.2200, 676.8900, 343.8300],\n        ],\n        device=device,\n    ),\n    mode=\"xyxy\",\n)\n\nkeypoints = Keypoints(\n    torch.tensor(\n        [[[535.0, 227.0], [349.0, 215.0], [237.0, 219.0], [301.0, 373.0], [641.0, 397.0], [489.0, 513.0]]], device=device\n    )\n)\nlabels = torch.ones(keypoints.shape[:2], device=device, dtype=torch.float32)\n\n\n# Image\nimage = load_image(car_image_path, ImageLoadType.RGB32, device)\n\n# Set the image\nprompter.set_image(image)\n\n\nQuerying with boxes\n\npredictions = prompter.predict(boxes=boxes, multimask_output=True)\n\n\nshow_predictions(image, predictions, boxes=boxes)\n\n\n\n\n\n\n\n\n\n\nQuerying with keypoints\n\nConsidering N points into 1 Batch\nThis way the model will kinda find the object within all the points\n\npredictions = prompter.predict(keypoints=keypoints, keypoints_labels=labels)\n\n\nshow_predictions(image, predictions, points=(keypoints, labels))\n\n\n\n\n\n\n\n\n\n\nConsidering 1 point into N Batch\nPrompter encoder not working for a batch of points :/\n\n\n\nConsidering 1 point for batch into N queries\nThis way the model will find an object for each point\n\nk = 2  # number of times/points to query\n\nfor idx in range(min(keypoints.data.size(1), k)):\n    print(\"-\" * 79, f\"\\nQuery {idx}:\")\n    _kpts = keypoints[:, idx, ...][None, ...]\n    _lbl = labels[:, idx, ...][None, ...]\n\n    predictions = prompter.predict(keypoints=_kpts, keypoints_labels=_lbl)\n\n    show_predictions(image, predictions, points=(_kpts, _lbl))\n\n------------------------------------------------------------------------------- \nQuery 0:\n\n\n\n\n\n\n\n\n\n------------------------------------------------------------------------------- \nQuery 1:\n\n\n\n\n\n\n\n\n\n\n\n\nSatellite image\nImage from Sentinel-2\nProduct: A tile of the TCI (px of 10m). Product name: S2B_MSIL1C_20230324T130249_N0509_R095_T23KPQ_20230324T174312\n\n# Prompts\nkeypoints = Keypoints(\n    torch.tensor(\n        [\n            [\n                # Urban\n                [74.0, 104.5],\n                [335, 110],\n                [702, 65],\n                [636, 479],\n                [408, 820],\n                # Forest\n                [40, 425],\n                [680, 566],\n                [405, 439],\n                [73, 689],\n                [865, 460],\n                # Ocean/water\n                [981, 154],\n                [705, 714],\n                [357, 683],\n                [259, 908],\n                [1049, 510],\n            ]\n        ],\n        device=device,\n    )\n)\nlabels = torch.ones(keypoints.shape[:2], device=device, dtype=torch.float32)\n\n\n# Image\nimage = load_image(satellite_image_path, ImageLoadType.RGB32, device)\n\n# Set the image\nprompter.set_image(image)\n\n\nQuery urban points\n\n# Query the prompts\nlabels_to_query = labels.clone()\nlabels_to_query[..., 5:] = 0\n\npredictions = prompter.predict(keypoints=keypoints, keypoints_labels=labels_to_query)\n\n\nshow_predictions(image, predictions, points=(keypoints, labels_to_query))\n\n\n\n\n\n\n\n\n\n\nQuery Forest points\n\n# Query the prompts\nlabels_to_query = labels.clone()\nlabels_to_query[..., :5] = 0\nlabels_to_query[..., 10:] = 0\n\npredictions = prompter.predict(keypoints=keypoints, keypoints_labels=labels_to_query)\n\n\nshow_predictions(image, predictions, points=(keypoints, labels_to_query))\n\n\n\n\n\n\n\n\n\n\nQuery ocean/water points\n\n# Query the prompts\nlabels_to_query = labels.clone()\nlabels_to_query[..., :10] = 0\n\npredictions = prompter.predict(keypoints=keypoints, keypoints_labels=labels_to_query)\n\n\nshow_predictions(image, predictions, points=(keypoints, labels_to_query))"
  },
  {
    "objectID": "nbs/color_raw_to_rgb.html#download-necessary-files-and-libraries",
    "href": "nbs/color_raw_to_rgb.html#download-necessary-files-and-libraries",
    "title": "Convert RGB to RAW",
    "section": "Download necessary files and libraries",
    "text": "Download necessary files and libraries\n\n%%capture\n!pip install kornia\n!pip install rawpy\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://docs.google.com/uc?export=download&id=1nSM_FYJ7i9-_57ecPY5sCG2s8zt9dRhF\"\ndownload_image(url, \"raw.dng\")\n\n'raw.dng'"
  },
  {
    "objectID": "nbs/color_raw_to_rgb.html#import-necessary-libraries",
    "href": "nbs/color_raw_to_rgb.html#import-necessary-libraries",
    "title": "Convert RGB to RAW",
    "section": "Import necessary libraries",
    "text": "Import necessary libraries\n\nimport kornia\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport rawpy\nimport torch"
  },
  {
    "objectID": "nbs/color_raw_to_rgb.html#prepare-the-raw-file-through-rawpy",
    "href": "nbs/color_raw_to_rgb.html#prepare-the-raw-file-through-rawpy",
    "title": "Convert RGB to RAW",
    "section": "Prepare the raw file through rawpy",
    "text": "Prepare the raw file through rawpy\n\npath = \"raw.dng\"\nraw = rawpy.imread(path)\ncfa = \"\".join([chr(raw.color_desc[i]) for i in raw.raw_pattern.flatten()])\n\n# Figure out which cfa we are using by looking at the component order from rawpy\n# if we do this directly from a camera this would of course be known ahead\n# of time\nif cfa == \"GRBG\":\n    korniacfa = kornia.color.CFA.GB\nelif cfa == \"GBRG\":\n    korniacfa = kornia.color.CFA.GR\nelif cfa == \"BGGR\":\n    korniacfa = kornia.color.CFA.RG\nelif cfa == \"RGGB\":\n    korniacfa = kornia.color.CFA.BG\n\n# This is a GB file i.e. top left pixel is Green follow by Red (and the pair\n# starting at (1,1) is Green, Blue)\nprint(cfa)\nprint(korniacfa)\n\nGRBG\nCFA.GB"
  },
  {
    "objectID": "nbs/color_raw_to_rgb.html#get-the-data-into-kornia-by-doing-the-conversion",
    "href": "nbs/color_raw_to_rgb.html#get-the-data-into-kornia-by-doing-the-conversion",
    "title": "Convert RGB to RAW",
    "section": "Get the data into kornia by doing the conversion",
    "text": "Get the data into kornia by doing the conversion\n\n# We find the data inside raw.raw_image\nrawdata = raw.raw_image\n# white level gives maximum value for a pixel\nrawtensor = torch.Tensor(rawdata.astype(np.float32) / raw.white_level).reshape(\n    1, 1, raw.raw_image.shape[0], raw.raw_image.shape[1]\n)\nrgbtensor = kornia.color.raw.raw_to_rgb(rawtensor, korniacfa)"
  },
  {
    "objectID": "nbs/color_raw_to_rgb.html#visualize",
    "href": "nbs/color_raw_to_rgb.html#visualize",
    "title": "Convert RGB to RAW",
    "section": "Visualize",
    "text": "Visualize\n\nnpimg = np.moveaxis(np.squeeze((rgbtensor * 255.0).numpy().astype(np.uint8)), 0, 2)\nplt.figure()\n\n# Show the image\n# Colors will look a little funky because they need to be balanced properly, but\n# the leaves are supposed to be redm berries blue and grass green\nplt.imshow(npimg)"
  },
  {
    "objectID": "nbs/color_raw_to_rgb.html#gotchas-rotation-gives-a-different-cfa",
    "href": "nbs/color_raw_to_rgb.html#gotchas-rotation-gives-a-different-cfa",
    "title": "Convert RGB to RAW",
    "section": "Gotchas: Rotation gives a different cfa",
    "text": "Gotchas: Rotation gives a different cfa\n\n# if we do a pipeline were we first rotate the image, it will end up with a\n# different cfa that isn't possible to describe since we are assuming all red\n# samples are on t.he same row while they would not be rotated\nrgbtensor = kornia.color.raw.raw_to_rgb(torch.rot90(rawtensor, 1, [2, 3]), korniacfa)\nnpimg = np.moveaxis(np.squeeze((rgbtensor * 255.0).numpy().astype(np.uint8)), 0, 2)\nplt.figure()\nplt.imshow(npimg)\n\n\n\n\n\n\n\n\n\n# If we crop, we can adjust for this by using a different cfa\nrgbtensor = kornia.color.raw.raw_to_rgb(rawtensor[:, :, 1:1023, 1:1023], kornia.color.raw.CFA.GR)\nnpimg = np.moveaxis(np.squeeze((rgbtensor * 255.0).numpy().astype(np.uint8)), 0, 2)\nplt.figure()\nplt.imshow(npimg)"
  },
  {
    "objectID": "nbs/extract_combine_patches.html#install-and-get-data",
    "href": "nbs/extract_combine_patches.html#install-and-get-data",
    "title": "Extracting and Combining Tensor Patches",
    "section": "Install and get data",
    "text": "Install and get data\n\n%%capture\n%matplotlib inline\n# Install latest kornia\n!pip install kornia\n!pip install kornia-rs\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://raw.githubusercontent.com/kornia/data/main/panda.jpg\"\ndownload_image(url)\n\n'panda.jpg'\n\n\n\nimport kornia as K\nimport matplotlib.pyplot as plt\nimport torch\nfrom kornia.contrib import (\n    CombineTensorPatches,\n    ExtractTensorPatches,\n    combine_tensor_patches,\n    compute_padding,\n    extract_tensor_patches,\n)"
  },
  {
    "objectID": "nbs/extract_combine_patches.html#using-modules",
    "href": "nbs/extract_combine_patches.html#using-modules",
    "title": "Extracting and Combining Tensor Patches",
    "section": "Using Modules",
    "text": "Using Modules\n\nh, w = 8, 8\nwin = 4\npad = 2\n\nimage = torch.randn(2, 3, h, w)\nprint(image.shape)\ntiler = ExtractTensorPatches(window_size=win, stride=win, padding=pad)\nmerger = CombineTensorPatches(original_size=(h, w), window_size=win, stride=win, unpadding=pad)\nimage_tiles = tiler(image)\nprint(image_tiles.shape)\nnew_image = merger(image_tiles)\nprint(new_image.shape)\nassert torch.allclose(image, new_image)\n\ntorch.Size([2, 3, 8, 8])\ntorch.Size([2, 9, 3, 4, 4])\ntorch.Size([2, 3, 8, 8])"
  },
  {
    "objectID": "nbs/extract_combine_patches.html#using-functions",
    "href": "nbs/extract_combine_patches.html#using-functions",
    "title": "Extracting and Combining Tensor Patches",
    "section": "Using Functions",
    "text": "Using Functions\n\nh, w = 8, 8\nwin = 4\npad = 2\n\nimage = torch.randn(1, 1, h, w)\nprint(image.shape)\npatches = extract_tensor_patches(image, window_size=win, stride=win, padding=pad)\nprint(patches.shape)\nrestored_img = combine_tensor_patches(patches, original_size=(h, w), window_size=win, stride=win, unpadding=pad)\nprint(restored_img.shape)\nassert torch.allclose(image, restored_img)\n\ntorch.Size([1, 1, 8, 8])\ntorch.Size([1, 9, 1, 4, 4])\ntorch.Size([1, 1, 8, 8])"
  },
  {
    "objectID": "nbs/extract_combine_patches.html#padding",
    "href": "nbs/extract_combine_patches.html#padding",
    "title": "Extracting and Combining Tensor Patches",
    "section": "Padding",
    "text": "Padding\nAll parameters of extract and combine functions accept a single int or tuple of two ints. Since padding is an integral part of these functions, it’s important to note the following:\n\nIf padding is p -&gt; it means both height and width are padded by 2*p\nIf padding is (ph, pw) -&gt; it means height is padded by 2*ph and width is padded by 2*pw\n\nIt is recommended to use the existing function compute_padding to ensure the required padding is added.\n\nExamples\n\ndef extract_and_combine(image, window_size, padding):\n    h, w = image.shape[-2:]\n    tiler = ExtractTensorPatches(window_size=window_size, stride=window_size, padding=padding)\n    merger = CombineTensorPatches(original_size=(h, w), window_size=window_size, stride=window_size, unpadding=padding)\n    image_tiles = tiler(image)\n    print(f\"Shape of tensor patches = {image_tiles.shape}\")\n    merged_image = merger(image_tiles)\n    print(f\"Shape of merged image = {merged_image.shape}\")\n    assert torch.allclose(image, merged_image)\n    return merged_image\n\n\nimage = torch.randn(2, 3, 9, 9)\n_ = extract_and_combine(image, window_size=(4, 4), padding=(2, 2))\n\nShape of tensor patches = torch.Size([2, 9, 3, 4, 4])\nShape of merged image = torch.Size([2, 3, 9, 9])\n\n\nThese functions also work with rectangular images\n\nrect_image = torch.randn(1, 1, 8, 6)\nprint(rect_image.shape)\n\ntorch.Size([1, 1, 8, 6])\n\n\n\nrestored_image = extract_and_combine(rect_image, window_size=(4, 4), padding=compute_padding((8, 6), 4))\n\nShape of tensor patches = torch.Size([1, 4, 1, 4, 4])\nShape of merged image = torch.Size([1, 1, 8, 6])\n\n\nRecall that when padding is a tuple of ints (ph, pw), the height and width are padded by 2*ph and 2*pw respectively.\n\n# Confirm that the original image and restored image are the same\nassert (restored_image == rect_image).all()\n\nLet’s now visualize how extraction and combining works.\n\n# Load sample image\nimg_tensor = K.io.load_image(\"panda.jpg\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\nh, w = img_tensor.shape[-2:]\nprint(f\"Shape of image = {img_tensor.shape}\")\n\nplt.axis(\"off\")\nplt.imshow(K.tensor_to_image(img_tensor))\nplt.show()\n\nShape of image = torch.Size([1, 3, 510, 1020])\n\n\n\n\n\n\n\n\n\nWe will use window_size = (400, 400) with stride = 200 to extract 15 overlapping tiles of shape (400, 400) and visualize them.\n\n# Set window size\nwin = 400\n# Set stride\nstride = 200\n# Calculate required padding\npad = compute_padding(original_size=(510, 1020), window_size=win)\n\ntiler = ExtractTensorPatches(window_size=win, stride=stride, padding=pad)\nimage_tiles = tiler(img_tensor)\nprint(f\"Shape of image tiles = {image_tiles.shape}\")\n\nShape of image tiles = torch.Size([1, 15, 3, 400, 400])\n\n\n\n# Create the plot\nfig, axs = plt.subplots(5, 3, figsize=(8, 8))\naxs = axs.ravel()\n\nfor i in range(len(image_tiles[0])):\n    axs[i].axis(\"off\")\n    axs[i].imshow(K.tensor_to_image(image_tiles[0][i]))\n\nplt.show()\n\n\n\n\n\n\n\n\nFinally, let’s combine the patches and visualize the resulting image\n\nmerger = CombineTensorPatches(original_size=(h, w), window_size=win, stride=stride, unpadding=pad)\nmerged_image = merger(image_tiles)\nprint(f\"Shape of restored image = {merged_image.shape}\")\n\nplt.imshow(K.tensor_to_image(merged_image[0]))\nplt.axis(\"off\")\nplt.show()\n\nShape of restored image = torch.Size([1, 3, 510, 1020])"
  },
  {
    "objectID": "nbs/data_augmentation_mosiac.html",
    "href": "nbs/data_augmentation_mosiac.html",
    "title": "Random Mosaic Augmentation",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs"
  },
  {
    "objectID": "nbs/data_augmentation_mosiac.html#install-and-get-data",
    "href": "nbs/data_augmentation_mosiac.html#install-and-get-data",
    "title": "Random Mosaic Augmentation",
    "section": "Install and get data",
    "text": "Install and get data\nWe install Kornia and some dependencies, and download a simple data sample\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://raw.githubusercontent.com/kornia/data/main/panda.jpg\"\ndownload_image(url)\n\n'panda.jpg'\n\n\n\nimport kornia as K\nimport torch\nfrom matplotlib import pyplot as plt\n\n\ndef plot(img, box):\n    img_vis = img.clone()\n    img_vis = K.utils.draw_rectangle(img_vis, box, color=torch.tensor([255, 0, 0]))\n    plt.imshow(K.tensor_to_image(img_vis))\n    plt.show()\n\n\nimg1 = K.io.load_image(\"panda.jpg\", K.io.ImageLoadType.RGB32)\nimg2 = K.augmentation.RandomEqualize(p=1.0, keepdim=True)(img1)\nimg3 = K.augmentation.RandomInvert(p=1.0, keepdim=True)(img1)\nimg4 = K.augmentation.RandomChannelShuffle(p=1.0, keepdim=True)(img1)\n\nplt.figure(figsize=(21, 9))\nplt.imshow(K.tensor_to_image(torch.cat([img1, img2, img3, img4], dim=-1)))\nplt.show()\n\n\n\n\n\n\n\n\n\nimport kornia as K\nimport torch\nfrom kornia.augmentation import RandomMosaic\n\nx = K.core.concatenate(\n    [\n        K.geometry.resize(img1[None], (224, 224)),\n        K.geometry.resize(img2[None], (224, 224)),\n        K.geometry.resize(img3[None], (224, 224)),\n        K.geometry.resize(img4[None], (224, 224)),\n    ]\n)\n\nboxes = torch.tensor(\n    [\n        [\n            [70.0, 5, 150, 100],  # head\n            [60, 180, 175, 220],  # feet\n        ]\n    ]\n).repeat(4, 1, 1)\n\naug = RandomMosaic(\n    (224, 224), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.5), p=1.0, min_bbox_size=300, data_keys=[\"input\", \"bbox_xyxy\"]\n)\n\ny, y1 = aug(x, boxes)\n\nplot(y[:1], y1[:1])\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "nbs/rotate_affine.html",
    "href": "nbs/rotate_affine.html",
    "title": "Rotate image using warp affine transform",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/bennett_aden.png\"\ndownload_image(url)\n\n'bennett_aden.png'\nimport cv2\nimport kornia as K\nimport numpy as np\nimport torch\nimport torchvision\nfrom matplotlib import pyplot as plt\nload the image using kornia\nx_img = K.io.load_image(\"bennett_aden.png\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\ndef imshow(input: torch.Tensor, size: tuple = None):\n    out = torchvision.utils.make_grid(input, nrow=4, padding=5)\n    out_np: np.ndarray = K.utils.tensor_to_image(out)\n    plt.figure(figsize=size)\n    plt.imshow(out_np)\n    plt.axis(\"off\")\n    plt.show()\nimshow(x_img)"
  },
  {
    "objectID": "nbs/rotate_affine.html#define-the-rotation-matrix",
    "href": "nbs/rotate_affine.html#define-the-rotation-matrix",
    "title": "Rotate image using warp affine transform",
    "section": "Define the rotation matrix",
    "text": "Define the rotation matrix\n\n# create transformation (rotation)\nalpha: float = 45.0  # in degrees\nangle: torch.tensor = torch.ones(1) * alpha\n\n# define the rotation center\ncenter: torch.tensor = torch.ones(1, 2)\ncenter[..., 0] = x_img.shape[3] / 2  # x\ncenter[..., 1] = x_img.shape[2] / 2  # y\n\n# define the scale factor\nscale: torch.tensor = torch.ones(1, 2)\n\n# compute the transformation matrix\nM: torch.tensor = K.geometry.get_rotation_matrix2d(center, angle, scale)  # 1x2x3"
  },
  {
    "objectID": "nbs/rotate_affine.html#apply-the-transformation-to-the-original-image",
    "href": "nbs/rotate_affine.html#apply-the-transformation-to-the-original-image",
    "title": "Rotate image using warp affine transform",
    "section": "Apply the transformation to the original image",
    "text": "Apply the transformation to the original image\n\n_, _, h, w = x_img.shape\nx_warped: torch.tensor = K.geometry.warp_affine(x_img, M, dsize=(h, w))\n\nimshow(x_warped)"
  },
  {
    "objectID": "nbs/rotate_affine.html#rotate-a-batch-of-images",
    "href": "nbs/rotate_affine.html#rotate-a-batch-of-images",
    "title": "Rotate image using warp affine transform",
    "section": "Rotate a batch of images",
    "text": "Rotate a batch of images\n\nx_batch = x_img.repeat(16, 1, 1, 1)\nx_rot = K.geometry.rotate(x_batch, torch.linspace(0.0, 360.0, 16))\n\nimshow(x_rot, (16, 16))"
  },
  {
    "objectID": "nbs/filtering_operators.html",
    "href": "nbs/filtering_operators.html",
    "title": "Filtering Operators",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/drslump.jpg\"\ndownload_image(url)\n\n'drslump.jpg'\nimport kornia as K\nimport torch\nimport torchvision\nfrom matplotlib import pyplot as plt\nWe use Kornia to load an image to memory represented directly in a tensor\nx_rgb: torch.Tensor = K.io.load_image(\"doraemon.png\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\nx_gray = K.color.rgb_to_grayscale(x_rgb)\ndef imshow(input: torch.Tensor):\n    if input.shape != x_rgb.shape:\n        input = K.geometry.resize(input, size=(x_rgb.shape[-2:]))\n    out = torch.cat([x_rgb, input], dim=-1)\n    out = torchvision.utils.make_grid(out, nrow=2, padding=5)\n    out_np = K.utils.tensor_to_image(out)\n    plt.imshow(out_np)\n    plt.axis(\"off\")\n    plt.show()\nimshow(x_rgb)"
  },
  {
    "objectID": "nbs/filtering_operators.html#box-blur",
    "href": "nbs/filtering_operators.html#box-blur",
    "title": "Filtering Operators",
    "section": "Box Blur",
    "text": "Box Blur\n\nx_blur: torch.Tensor = K.filters.box_blur(x_rgb, (9, 9))\nimshow(x_blur)"
  },
  {
    "objectID": "nbs/filtering_operators.html#blur-pool",
    "href": "nbs/filtering_operators.html#blur-pool",
    "title": "Filtering Operators",
    "section": "Blur Pool",
    "text": "Blur Pool\n\nx_blur: torch.Tensor = K.filters.blur_pool2d(x_rgb, kernel_size=9)\nimshow(x_blur)"
  },
  {
    "objectID": "nbs/filtering_operators.html#gaussian-blur",
    "href": "nbs/filtering_operators.html#gaussian-blur",
    "title": "Filtering Operators",
    "section": "Gaussian Blur",
    "text": "Gaussian Blur\n\nx_blur: torch.Tensor = K.filters.gaussian_blur2d(x_rgb, (11, 11), (11.0, 11.0))\nimshow(x_blur)"
  },
  {
    "objectID": "nbs/filtering_operators.html#max-pool",
    "href": "nbs/filtering_operators.html#max-pool",
    "title": "Filtering Operators",
    "section": "Max Pool",
    "text": "Max Pool\n\nx_blur: torch.Tensor = K.filters.max_blur_pool2d(x_rgb, kernel_size=11)\nimshow(x_blur)"
  },
  {
    "objectID": "nbs/filtering_operators.html#median-blur",
    "href": "nbs/filtering_operators.html#median-blur",
    "title": "Filtering Operators",
    "section": "Median Blur",
    "text": "Median Blur\n\nx_blur: torch.Tensor = K.filters.median_blur(x_rgb, (5, 5))\nimshow(x_blur)"
  },
  {
    "objectID": "nbs/filtering_operators.html#motion-blur",
    "href": "nbs/filtering_operators.html#motion-blur",
    "title": "Filtering Operators",
    "section": "Motion Blur",
    "text": "Motion Blur\n\nx_blur: torch.Tensor = K.filters.motion_blur(x_rgb, 9, 90.0, 1)\nimshow(x_blur)"
  },
  {
    "objectID": "nbs/total_variation_denoising.html",
    "href": "nbs/total_variation_denoising.html",
    "title": "Denoise image using total variation",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/doraemon.png\"\ndownload_image(url)\n\n'doraemon.png'\n\n\n\nimport kornia as K\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\n\ndef imshow(input: torch.Tensor):\n    out = torchvision.utils.make_grid(input, nrow=2, padding=5)\n    out_np = K.utils.tensor_to_image(out)\n    plt.imshow(out_np)\n    plt.axis(\"off\")\n    plt.show()\n\n\n# read the image with kornia and add a random noise to it\nimg = K.io.load_image(\"doraemon.png\", K.io.ImageLoadType.RGB32)  # CxHxW\n\nnoisy_image = (img + torch.normal(torch.zeros_like(img), 0.1)).clamp(0, 1)\nimshow(noisy_image)\n\n\n\n\n\n\n\n\nWe define the total variation denoising network and the optimizer\n\n# define the total variation denoising network\n\n\nclass TVDenoise(torch.nn.Module):\n    def __init__(self, noisy_image):\n        super().__init__()\n        self.l2_term = torch.nn.MSELoss(reduction=\"mean\")\n        self.regularization_term = K.losses.TotalVariation()\n        # create the variable which will be optimized to produce the noise free image\n        self.clean_image = torch.nn.Parameter(data=noisy_image.clone(), requires_grad=True)\n        self.noisy_image = noisy_image\n\n    def forward(self):\n        return self.l2_term(self.clean_image, self.noisy_image) + 0.0001 * self.regularization_term(self.clean_image)\n\n    def get_clean_image(self):\n        return self.clean_image\n\n\ntv_denoiser = TVDenoise(noisy_image)\n\n# define the optimizer to optimize the 1 parameter of tv_denoiser\noptimizer = torch.optim.SGD(tv_denoiser.parameters(), lr=0.1, momentum=0.9)\n\nRun the the optimization loop\n\nnum_iters: int = 500\nfor i in range(num_iters):\n    optimizer.zero_grad()\n    loss = tv_denoiser().sum()\n    if i % 50 == 0:\n        print(f\"Loss in iteration {i} of {num_iters}: {loss.item():.3f}\")\n    loss.backward()\n    optimizer.step()\n\nLoss in iteration 0 of 500: 3.081\nLoss in iteration 50 of 500: 2.723\nLoss in iteration 100 of 500: 2.359\nLoss in iteration 150 of 500: 2.064\nLoss in iteration 200 of 500: 1.828\nLoss in iteration 250 of 500: 1.642\nLoss in iteration 300 of 500: 1.497\nLoss in iteration 350 of 500: 1.384\nLoss in iteration 400 of 500: 1.297\nLoss in iteration 450 of 500: 1.229\n\n\nVisualize the noisy and resulting cleaned image\n\n# convert back to numpy\nimg_clean = K.utils.tensor_to_image(tv_denoiser.get_clean_image())\n\n# Create the plot\nfig, axs = plt.subplots(1, 2, figsize=(16, 10))\naxs = axs.ravel()\n\naxs[0].axis(\"off\")\naxs[0].set_title(\"Noisy image\")\naxs[0].imshow(K.tensor_to_image(noisy_image))\n\naxs[1].axis(\"off\")\naxs[1].set_title(\"Cleaned image\")\naxs[1].imshow(img_clean)\n\nplt.show()"
  },
  {
    "objectID": "nbs/hello_world_tutorial.html",
    "href": "nbs/hello_world_tutorial.html",
    "title": "Hello world: Planet Kornia",
    "section": "",
    "text": "Welcome to Planet Kornia: a set of tutorials to learn about Computer Vision in PyTorch.\nThis is the first tutorial that show how one can simply start loading images with Kornia and OpenCV.\n%%capture\n!pip install kornia\n!pip install kornia-rs\nimport cv2\nimport kornia as K\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nDownload first an image form internet to start to work.\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_image(\"https://github.com/kornia/data/raw/main/arturito.jpg\")\n\n'arturito.jpg'"
  },
  {
    "objectID": "nbs/hello_world_tutorial.html#load-an-image-with-kornia",
    "href": "nbs/hello_world_tutorial.html#load-an-image-with-kornia",
    "title": "Hello world: Planet Kornia",
    "section": "Load an image with Kornia",
    "text": "Load an image with Kornia\nWith Kornia, we can read the image which returns the images in a torch.Tensor in the shape (C,H,W). Also, we can convert a image (array) into a Tensor.\nWe have a couple of utilities to cast the image to a torch.Tensor to make it compliant to the other Kornia components and arrange the data in (B,C,H,W).\nThe read function is kornia.io.load_image, to use it, you need to pip install kornia_rs.\nThe package internally implements kornia_rs which contains a low level implementation for Computer Vision in the Rust language. In addition, we implement the DLPack protocol natively in Rust to reduce the memory footprint during the decoding and types conversion.\nYou can define the type you can load into the tensor, with K.io.ImageLoadType.RGB32 mode the image will be loaded as float32 with values between 0~1.0. Also can define the desired device you want to read the image.\n\nimg_bgr_tensor = K.io.load_image(\"arturito.jpg\", K.io.ImageLoadType.RGB32, device=\"cpu\")\n\nimg_bgr_tensor\n\ntensor([[[0.9412, 0.9412, 0.9412,  ..., 0.9412, 0.9412, 0.9412],\n         [0.9961, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n         ...,\n         [0.6471, 0.6471, 0.6471,  ..., 0.6471, 0.6471, 0.6471],\n         [0.6392, 0.6392, 0.6392,  ..., 0.6392, 0.6392, 0.6392],\n         [0.6510, 0.6510, 0.6510,  ..., 0.6510, 0.6510, 0.6510]],\n\n        [[0.9412, 0.9412, 0.9412,  ..., 0.9412, 0.9412, 0.9412],\n         [0.9961, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n         ...,\n         [0.6471, 0.6471, 0.6471,  ..., 0.6471, 0.6471, 0.6471],\n         [0.6392, 0.6392, 0.6392,  ..., 0.6392, 0.6392, 0.6392],\n         [0.6510, 0.6510, 0.6510,  ..., 0.6510, 0.6510, 0.6510]],\n\n        [[0.9412, 0.9412, 0.9412,  ..., 0.9412, 0.9412, 0.9412],\n         [0.9961, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n         ...,\n         [0.6471, 0.6471, 0.6471,  ..., 0.6471, 0.6471, 0.6471],\n         [0.6392, 0.6392, 0.6392,  ..., 0.6392, 0.6392, 0.6392],\n         [0.6510, 0.6510, 0.6510,  ..., 0.6510, 0.6510, 0.6510]]])\n\n\n\nplt.imshow(K.tensor_to_image(img_bgr_tensor))\nplt.axis(\"off\");"
  },
  {
    "objectID": "nbs/hello_world_tutorial.html#load-an-image-with-opencv",
    "href": "nbs/hello_world_tutorial.html#load-an-image-with-opencv",
    "title": "Hello world: Planet Kornia",
    "section": "Load an image with OpenCV",
    "text": "Load an image with OpenCV\nWe can use OpenCV to load an image. By default, OpenCV loads images in BGR format and casts to a numpy.ndarray with the data layout (H,W,C).\nHowever, because matplotlib saves an image in RGB format, in OpenCV you need to change the BGR to RGB so that an image is displayed properly.\n\nimg_bgr: np.array = cv2.imread(\"arturito.jpg\")  # HxWxC / np.uint8\nimg_rgb: np.array = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\nplt.axis(\"off\");\n\n\n\n\n\n\n\n\nThe utility is kornia.image_to_tensor which casts a numpy.ndarray to a torch.Tensor and permutes the channels to leave the image ready for being used with any other PyTorch or Kornia component.\nThe image is casted into a 4D torch.Tensor with zero-copy.\n\nx_bgr: torch.tensor = K.image_to_tensor(img_bgr)  # CxHxW / torch.uint8\nx_bgr = x_bgr.unsqueeze(0)  # 1xCxHxW\nprint(f\"convert from '{img_bgr.shape}' to '{x_bgr.shape}'\")\n\nconvert from '(144, 256, 3)' to 'torch.Size([1, 3, 144, 256])'\n\n\nWe can convert from BGR to RGB with a kornia.color component.\n\nx_rgb: torch.tensor = K.color.bgr_to_rgb(x_bgr)  # 1xCxHxW / torch.uint8"
  },
  {
    "objectID": "nbs/hello_world_tutorial.html#visualize-an-image-with-matplotib",
    "href": "nbs/hello_world_tutorial.html#visualize-an-image-with-matplotib",
    "title": "Hello world: Planet Kornia",
    "section": "Visualize an image with Matplotib",
    "text": "Visualize an image with Matplotib\nWe will use Matplotlib for the visualisation inside the notebook. Matplotlib requires a numpy.ndarray in the (H,W,C) format, and for doing so we will go back with kornia.tensor_to_image which will convert the image to the correct format.\n\nimg_bgr: np.array = K.tensor_to_image(x_bgr)\nimg_rgb: np.array = K.tensor_to_image(x_rgb)\n\nCreate a subplot to visualize the original an a modified image\n\nfig, axs = plt.subplots(1, 2, figsize=(32, 16))\naxs = axs.ravel()\n\naxs[0].axis(\"off\")\naxs[0].imshow(img_rgb)\n\naxs[1].axis(\"off\")\naxs[1].imshow(img_bgr)\n\nplt.show()"
  },
  {
    "objectID": "nbs/image_matching.html",
    "href": "nbs/image_matching.html",
    "title": "Image matching example with LoFTR",
    "section": "",
    "text": "First, we will install everything needed:\n%%capture\n!pip install kornia\n!pip install kornia-rs\n!pip install kornia_moons\n!pip install opencv-python --upgrade\nNow let’s download an image pair\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\nurl_a = \"https://github.com/kornia/data/raw/main/matching/kn_church-2.jpg\"\nurl_b = \"https://github.com/kornia/data/raw/main/matching/kn_church-8.jpg\"\n\ndownload_image(url_a)\ndownload_image(url_b)\n\n'kn_church-8.jpg'\nFirst, we will define image matching pipeline with OpenCV SIFT features. We will also use kornia for the state-of-the-art match filtering – Lowe ratio + mutual nearest neighbor check and MAGSAC++ as RANSAC.\nimport cv2\nimport kornia as K\nimport kornia.feature as KF\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom kornia_moons.viz import draw_LAF_matches\n%%capture\nfname1 = \"kn_church-2.jpg\"\nfname2 = \"kn_church-8.jpg\"\n\nimg1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32)[None, ...]\nimg2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32)[None, ...]\n\nimg1 = K.geometry.resize(img1, (600, 375), antialias=True)\nimg2 = K.geometry.resize(img2, (600, 375), antialias=True)\n\n\nmatcher = KF.LoFTR(pretrained=\"outdoor\")\n\ninput_dict = {\n    \"image0\": K.color.rgb_to_grayscale(img1),  # LofTR works on grayscale images only\n    \"image1\": K.color.rgb_to_grayscale(img2),\n}\n\nwith torch.inference_mode():\n    correspondences = matcher(input_dict)\nfor k, v in correspondences.items():\n    print(k)\n\nkeypoints0\nkeypoints1\nconfidence\nbatch_indexes\nNow let’s clean-up the correspondences with modern RANSAC and estimate fundamental matrix between two images\nmkpts0 = correspondences[\"keypoints0\"].cpu().numpy()\nmkpts1 = correspondences[\"keypoints1\"].cpu().numpy()\nFm, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\ninliers = inliers &gt; 0\nFinally, let’s draw the matches with a function from kornia_moons. The correct matches are in green and imprecise matches - in blue\ndraw_LAF_matches(\n    KF.laf_from_center_scale_ori(\n        torch.from_numpy(mkpts0).view(1, -1, 2),\n        torch.ones(mkpts0.shape[0]).view(1, -1, 1, 1),\n        torch.ones(mkpts0.shape[0]).view(1, -1, 1),\n    ),\n    KF.laf_from_center_scale_ori(\n        torch.from_numpy(mkpts1).view(1, -1, 2),\n        torch.ones(mkpts1.shape[0]).view(1, -1, 1, 1),\n        torch.ones(mkpts1.shape[0]).view(1, -1, 1),\n    ),\n    torch.arange(mkpts0.shape[0]).view(-1, 1).repeat(1, 2),\n    K.tensor_to_image(img1),\n    K.tensor_to_image(img2),\n    inliers,\n    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": None, \"feature_color\": (0.2, 0.5, 1), \"vertical\": False},\n)"
  },
  {
    "objectID": "nbs/image_matching.html#loftr-indoor",
    "href": "nbs/image_matching.html#loftr-indoor",
    "title": "Image matching example with LoFTR",
    "section": "LoFTR Indoor",
    "text": "LoFTR Indoor\nWe recommend to use KF.LoFTR(pretrained='indoor_new') and resize images to be not bigger than 640x480 pixels for the indoor model.\n\n%%capture\n\ndownload_image(\"https://github.com/zju3dv/LoFTR/raw/master/assets/scannet_sample_images/scene0711_00_frame-001680.jpg\")\ndownload_image(\"https://github.com/zju3dv/LoFTR/raw/master/assets/scannet_sample_images/scene0711_00_frame-001995.jpg\")\n\nmatcher = KF.LoFTR(pretrained=\"indoor_new\")\n\n\nfname1 = \"scene0711_00_frame-001680.jpg\"\nfname2 = \"scene0711_00_frame-001995.jpg\"\n\nimg1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32)[None, ...]\nimg2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32)[None, ...]\n\nimg1 = K.geometry.resize(img1, (480, 640), antialias=True)\nimg2 = K.geometry.resize(img2, (480, 640), antialias=True)\n\nmatcher = KF.LoFTR(pretrained=\"indoor_new\")\n\ninput_dict = {\n    \"image0\": K.color.rgb_to_grayscale(img1),  # LofTR works on grayscale images only\n    \"image1\": K.color.rgb_to_grayscale(img2),\n}\n\nwith torch.inference_mode():\n    correspondences = matcher(input_dict)\n\nmkpts0 = correspondences[\"keypoints0\"].cpu().numpy()\nmkpts1 = correspondences[\"keypoints1\"].cpu().numpy()\nFm, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 1.0, 0.999, 100000)\ninliers = inliers &gt; 0\n\ndraw_LAF_matches(\n    KF.laf_from_center_scale_ori(\n        torch.from_numpy(mkpts0).view(1, -1, 2),\n        torch.ones(mkpts0.shape[0]).view(1, -1, 1, 1),\n        torch.ones(mkpts0.shape[0]).view(1, -1, 1),\n    ),\n    KF.laf_from_center_scale_ori(\n        torch.from_numpy(mkpts1).view(1, -1, 2),\n        torch.ones(mkpts1.shape[0]).view(1, -1, 1, 1),\n        torch.ones(mkpts1.shape[0]).view(1, -1, 1),\n    ),\n    torch.arange(mkpts0.shape[0]).view(-1, 1).repeat(1, 2),\n    K.tensor_to_image(img1),\n    K.tensor_to_image(img2),\n    inliers,\n    draw_dict={\n        \"inlier_color\": (0.2, 1, 0.2),\n        \"tentative_color\": (1.0, 0.5, 1),\n        \"feature_color\": (0.2, 0.5, 1),\n        \"vertical\": False,\n    },\n)"
  },
  {
    "objectID": "nbs/fit_plane.html",
    "href": "nbs/fit_plane.html",
    "title": "Fit plane tutorial",
    "section": "",
    "text": "import plotly.express as px\nimport plotly.io as pio\nimport torch\nfrom kornia.core import stack\nfrom kornia.geometry.liegroup import So3\nfrom kornia.geometry.plane import Hyperplane, fit_plane\nfrom kornia.geometry.vector import Vector3\nfrom kornia.utils import create_meshgrid\n\n\n# define the plane\nplane_h = 25\nplane_w = 50\n\n# create a base mesh in the ground z == 0\nmesh = create_meshgrid(plane_h, plane_w, normalized_coordinates=True)\nX, Y = mesh[..., 0], mesh[..., 1]\nZ = 0 * X\n\nmesh_pts = Vector3.from_coords(X, Y, Z)\n\n\n# add noise to the mesh\nrand_pts = Vector3.random((plane_h, plane_w))\nrand_pts.z.clamp_(min=-0.1, max=0.1)\n\nmesh_view: Vector3 = mesh_pts + rand_pts\n\n\nx_view = mesh_view.x.ravel().detach().cpu().numpy().tolist()\ny_view = mesh_view.y.ravel().detach().cpu().numpy().tolist()\nz_view = mesh_view.z.ravel().detach().cpu().numpy().tolist()\nfig = px.scatter_3d(dict(x=x_view, y=y_view, z=z_view, category=[\"view\"] * len(x_view)), \"x\", \"y\", \"z\", color=\"category\")\nfig.show()\n\n                                                \n\n\n\n# create rotation\nangle_rad = torch.tensor(3.141616 / 4)\nrot_x = So3.rot_x(angle_rad)\nrot_z = So3.rot_z(angle_rad)\nrot = rot_x * rot_z\nprint(rot)\n\nParameter containing:\ntensor([ 0.8536,  0.3536, -0.1464,  0.3536], requires_grad=True)\n\n\n\n# apply the rotation to the mesh points\n# TODO: this should work as `rot * mesh_view`\npoints_rot = stack([rot * x for x in mesh_view.view(-1, 3)]).detach()\npoints_rot = Vector3(points_rot)\n\n\nx_rot = points_rot.x.ravel().detach().cpu().numpy().tolist()\ny_rot = points_rot.y.ravel().detach().cpu().numpy().tolist()\nz_rot = points_rot.z.ravel().detach().cpu().numpy().tolist()\n\nfig = px.scatter_3d(\n    dict(\n        x=x_view + x_rot,\n        y=y_view + y_rot,\n        z=z_view + z_rot,\n        category=[\"view\"] * len(x_view) + [\"rotated\"] * len(x_rot),\n    ),\n    \"x\",\n    \"y\",\n    \"z\",\n    color=\"category\",\n)\nfig.show()\n\n                                                \n\n\n\n# estimate the plane from the rotated points\nplane_in_ground_fit: Hyperplane = fit_plane(points_rot)\nprint(plane_in_ground_fit)\n\nNormal: x: -0.0002799616486299783\ny: 0.7073221206665039\nz: -0.7068911790847778\nOffset: 0.094654381275177\n\n\n\n# project the original points to the estimated plane\npoints_proj: Vector3 = plane_in_ground_fit.projection(mesh_view.view(-1, 3))\n\n\nx_proj = points_proj.x.ravel().detach().cpu().numpy().tolist()\ny_proj = points_proj.y.ravel().detach().cpu().numpy().tolist()\nz_proj = points_proj.z.ravel().detach().cpu().numpy().tolist()\ncategories = [\"view\"] * len(x_view) + [\"rotated\"] * len(x_rot) + [\"projection\"] * len(x_proj)\nfig = px.scatter_3d(\n    dict(\n        x=x_view + x_rot + x_proj,\n        y=y_view + y_rot + y_proj,\n        z=z_view + z_rot + z_proj,\n        category=categories,\n    ),\n    \"x\",\n    \"y\",\n    \"z\",\n    color=\"category\",\n)\nfig.show()"
  },
  {
    "objectID": "nbs/data_augmentation_sequential.html#install-and-get-data",
    "href": "nbs/data_augmentation_sequential.html#install-and-get-data",
    "title": "Augmentation Sequential",
    "section": "Install and get data",
    "text": "Install and get data\nWe install Kornia and some dependencies, and download a simple data sample\n\n!pip install kornia\n!pip install kornia-rs\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://raw.githubusercontent.com/kornia/data/main/panda.jpg\"\ndownload_image(url)\n\n\nimport cv2\nimport kornia as K\nimport numpy as np\nimport torch\nfrom kornia.augmentation import AugmentationSequential\nfrom kornia.geometry import bbox_to_mask\nfrom matplotlib import pyplot as plt\n\n\ndef plot_resulting_image(img, bbox, keypoints, mask):\n    img = img * mask\n    img_array = K.tensor_to_image(img.mul(255).byte()).copy()\n    img_draw = cv2.polylines(img_array, bbox.numpy(), isClosed=True, color=(255, 0, 0))\n    for k in keypoints[0]:\n        img_draw = cv2.circle(img_draw, tuple(k.numpy()[:2]), radius=6, color=(255, 0, 0), thickness=-1)\n    return img_draw\n\n\nimg_tensor = K.io.load_image(\"panda.jpg\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\nh, w = img_tensor.shape[-2:]\n\nplt.axis(\"off\")\nplt.imshow(K.tensor_to_image(img_tensor))\nplt.show()"
  },
  {
    "objectID": "nbs/data_augmentation_sequential.html#define-augmentation-sequential-and-different-labels",
    "href": "nbs/data_augmentation_sequential.html#define-augmentation-sequential-and-different-labels",
    "title": "Augmentation Sequential",
    "section": "Define Augmentation Sequential and Different Labels",
    "text": "Define Augmentation Sequential and Different Labels\n\naug_list = AugmentationSequential(\n    K.augmentation.ColorJitter(0.1, 0.1, 0.1, 0.1, p=1.0),\n    K.augmentation.RandomAffine(360, [0.1, 0.1], [0.7, 1.2], [30.0, 50.0], p=1.0),\n    K.augmentation.RandomPerspective(0.5, p=1.0),\n    data_keys=[\"input\", \"bbox\", \"keypoints\", \"mask\"],\n    same_on_batch=False,\n)\n\nbbox = torch.tensor([[[[355, 10], [660, 10], [660, 250], [355, 250]]]])\nkeypoints = torch.tensor([[[465, 115], [545, 116]]])\nmask = bbox_to_mask(torch.tensor([[[155, 0], [900, 0], [900, 400], [155, 400]]]), w, h)[None].float()\n\nimg_out = plot_resulting_image(img_tensor, bbox[0], keypoints, mask[0])\n\nplt.axis(\"off\")\nplt.imshow(img_out)\nplt.show()"
  },
  {
    "objectID": "nbs/data_augmentation_sequential.html#forward-computations",
    "href": "nbs/data_augmentation_sequential.html#forward-computations",
    "title": "Augmentation Sequential",
    "section": "Forward Computations",
    "text": "Forward Computations\n\nout_tensor = aug_list(img_tensor, bbox.float(), keypoints.float(), mask)\nimg_out = plot_resulting_image(\n    out_tensor[0][0],\n    out_tensor[1].int(),\n    out_tensor[2].int(),\n    out_tensor[3][0],\n)\n\nplt.axis(\"off\")\nplt.imshow(img_out)\nplt.show()"
  },
  {
    "objectID": "nbs/data_augmentation_sequential.html#inverse-transformations",
    "href": "nbs/data_augmentation_sequential.html#inverse-transformations",
    "title": "Augmentation Sequential",
    "section": "Inverse Transformations",
    "text": "Inverse Transformations\n\nout_tensor_inv = aug_list.inverse(*out_tensor)\nimg_out = plot_resulting_image(\n    out_tensor_inv[0][0],\n    out_tensor_inv[1].int(),\n    out_tensor_inv[2].int(),\n    out_tensor_inv[3][0],\n)\n\nplt.axis(\"off\")\nplt.imshow(img_out)\nplt.show()"
  },
  {
    "objectID": "nbs/morphology_101.html",
    "href": "nbs/morphology_101.html",
    "title": "Introduction to Morphological Operators",
    "section": "",
    "text": "By the end, you will be able to use morphological operations as easy as:\nnew_image = morph.operation(original_image, structuring_element)\nBut first things first, let’s prepare the environment."
  },
  {
    "objectID": "nbs/morphology_101.html#download-kornia",
    "href": "nbs/morphology_101.html#download-kornia",
    "title": "Introduction to Morphological Operators",
    "section": "Download Kornia",
    "text": "Download Kornia\nIf you don’t have Kornia installed, you can download it using pip.\n\n%%capture\n!pip install kornia\n!pip install kornia-rs"
  },
  {
    "objectID": "nbs/morphology_101.html#prepare-the-image",
    "href": "nbs/morphology_101.html#prepare-the-image",
    "title": "Introduction to Morphological Operators",
    "section": "Prepare the image",
    "text": "Prepare the image\nWith kornia.morphology, you can apply morphological operators in 3 channel color images. Besides, all operators are differentiable. Let’s download the image\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1].split(\"?\")[0] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_image(\n    \"https://image.shutterstock.com/image-photo/portrait-surprised-cat-scottish-straight-260nw-499196506.jpg\", \"img.jpg\"\n)\n\n'img.jpg'\n\n\n\nImports and read the image\n\nimport kornia as K\nimport torch\n\ndevice = \"cpu\"  # 'cuda:0' for GPU\n\n\nimg_t = K.io.load_image(\"img.jpg\", K.io.ImageLoadType.RGB32, device=device)[None, ...]\n\n\n\nStructuring element\nWe have the original image ready to go, now we need the second part in the operation, the structuring element (aka Kernel).\nThe kernel must be a 2-dim tensor with odd sides, i.e. 3x3.\n\nkernel = torch.tensor([[0, 1, 0], [1, 1, 1], [0, 1, 0]]).to(device)\n\n\n\nMaking plots!\nIn this tutorial we are gonna compare the images before and after transforming them.\nIt make sense to create a function to plot and see the changes!\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\n\ndef plot_morph_image(tensor):\n    # kornia.tensor_to_image\n    image = K.tensor_to_image(tensor.squeeze(0))  # Tensor to image\n\n    # Plot before-after\n    rcParams[\"figure.figsize\"] = 20, 20\n    fig, ax = plt.subplots(1, 2)\n    ax[0].axis(\"off\")\n    ax[0].imshow(K.tensor_to_image(img_t))\n    ax[1].axis(\"off\")\n    ax[1].imshow(image)"
  },
  {
    "objectID": "nbs/morphology_101.html#morphology",
    "href": "nbs/morphology_101.html#morphology",
    "title": "Introduction to Morphological Operators",
    "section": "Morphology",
    "text": "Morphology\nThe main goal of kornia.morphology is that you could easily implement several morphological operator as follows:\nnew_image = morph.operation(original_image, structuring_element)\nLet’s check them all!\n\nDilation\n\nfrom kornia import morphology as morph\n\ndilated_image = morph.dilation(img_t, kernel)  # Dilation\nplot_morph_image(dilated_image)  # Plot\n\n\n\n\n\n\n\n\n\n\nErosion\n\neroded_image = morph.erosion(img_t, kernel)  # Erosion\nplot_morph_image(eroded_image)  # Plot\n\n\n\n\n\n\n\n\n\n\nOpen\n\nopened_image = morph.opening(img_t, kernel)  # Open\nplot_morph_image(opened_image)\n\n\n\n\n\n\n\n\n\n\nClose\n\nclosed_image = morph.closing(img_t, kernel)  # Close\nplot_morph_image(closed_image)  # Plot\n\n\n\n\n\n\n\n\n\n\nMorphological Gradient\n\ngraded_image = morph.gradient(img_t, kernel)  # Morphological gradient\nplot_morph_image(1.0 - graded_image)\n\n\n\n\n\n\n\n\n\n\nBottom Hat\n\nbottom_image = morph.bottom_hat(img_t, kernel)  # Black Hat\nplot_morph_image(1.0 - bottom_image)\n\n\n\n\n\n\n\n\n\n\nTop Hat\n\ntoph_image = morph.top_hat(img_t, kernel)  # Top Hat\nplot_morph_image(1.0 - toph_image)"
  },
  {
    "objectID": "nbs/morphology_101.html#conclusion",
    "href": "nbs/morphology_101.html#conclusion",
    "title": "Introduction to Morphological Operators",
    "section": "Conclusion",
    "text": "Conclusion\nAnd that’s it!\nNow you know how to use Kornia to apply differentiable morphological operations in your PyTorch pipeline.\nMany thanks for using Kornia, and have fun!"
  },
  {
    "objectID": "nbs/data_augmentation_segmentation.html#install-and-get-data",
    "href": "nbs/data_augmentation_segmentation.html#install-and-get-data",
    "title": "Data Augmentation Semantic Segmentation",
    "section": "Install and get data",
    "text": "Install and get data\nWe install Kornia and some dependencies, and download a simple data sample\n\n%%capture\n%matplotlib inline\n!pip install kornia\n!pip install kornia-rs\n!pip install opencv-python\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/causevic16semseg3.png\"\ndownload_image(url)\n\n'causevic16semseg3.png'\n\n\n\n# import the libraries\nimport kornia as K\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn"
  },
  {
    "objectID": "nbs/data_augmentation_segmentation.html#define-augmentation-pipeline",
    "href": "nbs/data_augmentation_segmentation.html#define-augmentation-pipeline",
    "title": "Data Augmentation Semantic Segmentation",
    "section": "Define Augmentation pipeline",
    "text": "Define Augmentation pipeline\nWe define a class to define our augmentation API using an nn.Module\n\nclass MyAugmentation(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # we define and cache our operators as class members\n        self.k1 = K.augmentation.ColorJitter(0.15, 0.25, 0.25, 0.25)\n        self.k2 = K.augmentation.RandomAffine([-45.0, 45.0], [0.0, 0.15], [0.5, 1.5], [0.0, 0.15])\n\n    def forward(self, img: torch.Tensor, mask: torch.Tensor) -&gt; torch.Tensor:\n        # 1. apply color only in image\n        # 2. apply geometric tranform\n        img_out = self.k2(self.k1(img))\n\n        # 3. infer geometry params to mask\n        # TODO: this will change in future so that no need to infer params\n        mask_out = self.k2(mask, self.k2._params)\n\n        return img_out, mask_out\n\nLoad the data and apply the transforms\n\ndef load_data(data_path: str) -&gt; torch.Tensor:\n    data_t: torch.Tensor = K.io.load_image(data_path, K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n    img, labels = data_t[..., :571], data_t[..., 572:]\n    return img, labels\n\n\n# load data (B, C, H, W)\nimg, labels = load_data(\"causevic16semseg3.png\")\n\n# create augmentation instance\naug = MyAugmentation()\n\n# apply the augmenation pipelone to our batch of data\nimg_aug, labels_aug = aug(img, labels)\n\n# visualize\nimg_out = torch.cat([img, labels], dim=-1)\nplt.imshow(K.tensor_to_image(img_out))\nplt.axis(\"off\")\n\n# generate several samples\nnum_samples: int = 10\n\nfor img_id in range(num_samples):\n    # generate data\n    img_aug, labels_aug = aug(img, labels)\n    img_out = torch.cat([img_aug, labels_aug], dim=-1)\n\n    # save data\n    plt.figure()\n    plt.imshow(K.tensor_to_image(img_out))\n    plt.axis(\"off\")\n    # plt.savefig(f\"img_{img_id}.png\", bbox_inches=\"tight\")\n    plt.show()"
  },
  {
    "objectID": "nbs/aliased_and_not_aliased_patch_extraction.html",
    "href": "nbs/aliased_and_not_aliased_patch_extraction.html",
    "title": "Image anti-alias with local features",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/drslump.jpg\"\ndownload_image(url)\n\n'drslump.jpg'\n\n\nFirst, lets load some image.\n\n%matplotlib inline\nimport kornia as K\nimport kornia.feature as KF\nimport matplotlib.pyplot as plt\nimport torch\n\n\ndevice = torch.device(\"cpu\")\n\nimg_original = K.io.load_image(\"drslump.jpg\", K.io.ImageLoadType.RGB32, device=device)[None, ...]\n\nplt.figure()\nplt.imshow(K.tensor_to_image(img_original))\n\n\n\n\n\n\n\n\n\nB, CH, H, W = img_original.shape\n\nDOWNSAMPLE = 4\nimg_small = K.geometry.resize(img_original, (H // DOWNSAMPLE, W // DOWNSAMPLE), interpolation=\"area\")\nplt.figure()\nplt.imshow(K.tensor_to_image(img_small))\n\n\n\n\n\n\n\n\nNow, lets define a keypoint with a large support region.\n\ndef show_lafs(img, lafs, idx=0, color=\"r\", figsize=(10, 7)):\n    x, y = KF.laf.get_laf_pts_to_draw(lafs, idx)\n    plt.figure(figsize=figsize)\n    if isinstance(img, torch.Tensor):\n        img_show = K.tensor_to_image(img)\n    else:\n        img_show = img\n    plt.imshow(img_show)\n    plt.plot(x, y, color)\n    return\n\n\nlaf_orig = torch.tensor([[150.0, 0, 180], [0, 150, 280]]).float().view(1, 1, 2, 3)\nlaf_small = laf_orig / float(DOWNSAMPLE)\n\nshow_lafs(img_original, laf_orig, figsize=(6, 4))\nshow_lafs(img_small, laf_small, figsize=(6, 4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow lets compare how extracted patch would look like when extracted in a naive way and from scale pyramid.\n\nPS = 32\nwith torch.no_grad():\n    patches_pyr_orig = KF.extract_patches_from_pyramid(img_original, laf_orig.to(device), PS)\n    patches_simple_orig = KF.extract_patches_simple(img_original, laf_orig.to(device), PS)\n\n    patches_pyr_small = KF.extract_patches_from_pyramid(img_small, laf_small.to(device), PS)\n    patches_simple_small = KF.extract_patches_simple(img_small, laf_small.to(device), PS)\n\n# Now we will glue all the patches together:\n\n\ndef vert_cat_with_margin(p1, p2, margin=3):\n    b, n, ch, h, w = p1.size()\n    return torch.cat([p1, torch.ones(b, n, ch, h, margin).to(device), p2], dim=4)\n\n\ndef horiz_cat_with_margin(p1, p2, margin=3):\n    b, n, ch, h, w = p1.size()\n    return torch.cat([p1, torch.ones(b, n, ch, margin, w).to(device), p2], dim=3)\n\n\npatches_pyr = vert_cat_with_margin(patches_pyr_orig, patches_pyr_small)\npatches_naive = vert_cat_with_margin(patches_simple_orig, patches_simple_small)\n\npatches_all = horiz_cat_with_margin(patches_naive, patches_pyr)\n\nNow lets show the result. Top row is what you get if you are extracting patches without any antialiasing - note how the patches extracted from the images of different sizes differ.\nBottom row is patches, which are extracted from images of different sizes using a scale pyramid. They are not yet exactly the same, but the difference is much smaller.\n\nplt.figure(figsize=(10, 10))\nplt.imshow(K.tensor_to_image(patches_all[0, 0]))\n\n\n\n\n\n\n\n\nLets check how much it influences local descriptor performance such as HardNet\n\nhardnet = KF.HardNet(True).eval()\nall_patches = (\n    torch.cat([patches_pyr_orig, patches_pyr_small, patches_simple_orig, patches_simple_small], dim=0)\n    .squeeze(1)\n    .mean(dim=1, keepdim=True)\n)\nwith torch.no_grad():\n    descs = hardnet(all_patches)\n    distances = torch.cdist(descs, descs)\n    print(distances.cpu().detach().numpy())\n\n[[0.         0.16867691 0.8070452  0.52112377]\n [0.16867691 0.         0.7973113  0.48472866]\n [0.8070452  0.7973113  0.         0.59267515]\n [0.52112377 0.48472866 0.59267515 0.        ]]\n\n\nSo the descriptor difference between antialiased patches is 0.09 and between naively extracted – 0.44"
  },
  {
    "objectID": "nbs/color_yuv420_to_rgb.html",
    "href": "nbs/color_yuv420_to_rgb.html",
    "title": "Convert RGB to YUV420",
    "section": "",
    "text": "Get data and libraries to work with\n\n%%capture\n!pip install kornia\n!pip install py7zr\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/foreman_qcif.7z\"\ndownload_image(url)\n\n'foreman_qcif.7z'\n\n\n\n\nImport needed libs\n\nimport kornia\nimport numpy as np\n\n# prepare the data, decompress so we have a foreman_qcif.yuv ready\nimport py7zr\nimport torch\n\nwith py7zr.SevenZipFile(\"foreman_qcif.7z\", mode=\"r\") as z:\n    z.extractall()\n\n\n\nDefine functions for reading the yuv file to torch tensor for use in Kornia\n\nimport matplotlib.pyplot as plt\n\n\ndef read_frame(fname, framenum):\n    # A typical 420 yuv file is 3 planes Y, u then v with u/v a quartyer the size of Y\n    # Build rgb png images from foreman that is 3 plane yuv420\n    yuvnp = np.fromfile(fname, dtype=np.uint8, count=int(176 * 144 * 1.5), offset=int(176 * 144 * 1.5) * framenum)\n    y = torch.from_numpy(yuvnp[0 : 176 * 144].reshape((1, 1, 144, 176)).astype(np.float32) / 255.0)\n\n    uv_tmp = yuvnp[176 * 144 : int(144 * 176 * 3 / 2)].reshape((1, 2, int(144 / 2), int(176 / 2)))\n    # uv (chroma) is typically defined from -0.5 to 0.5 (or -128 to 128 for 8-bit)\n    uv = torch.from_numpy(uv_tmp.astype(np.float32) / 255.0) - 0.5\n    return (y, uv)\n\n\n\nSample what the images look like Y, u, v channels separaatly and then converted to rgn through kornia (and back to numpy in this case)\n\n(y, uv) = read_frame(\"foreman_qcif.yuv\", 0)  # using compression classic foreman\nplt.imshow((y.numpy()[0, 0, :, :] * 255.0).astype(np.uint8), cmap=\"gray\")\nplt.figure()\nplt.imshow(((uv.numpy()[0, 0, :, :] + 0.5) * 255.0).astype(np.uint8), cmap=\"gray\")\nplt.figure()\nplt.imshow(((uv.numpy()[0, 1, :, :] + 0.5) * 255.0).astype(np.uint8), cmap=\"gray\")\n\nrgb = np.moveaxis(kornia.color.yuv420_to_rgb(y, uv).numpy(), 1, 3).reshape((144, 176, 3))\n\nprint(\"as converted through kornia\")\nplt.figure()\nplt.imshow((rgb * 255).astype(np.uint8))\n\nas converted through kornia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use these in some internal Kornia algorithm implementations. Lets pretend we want to do LoFTR on the red channel\n\nimport cv2\n\nloftr = kornia.feature.LoFTR(\"outdoor\")\n(y0, uv0) = read_frame(\"foreman_qcif.yuv\", 175)\n(y1, uv1) = read_frame(\"foreman_qcif.yuv\", 185)\nrgb0 = kornia.color.yuv420_to_rgb(y0, uv0)\nrgb1 = kornia.color.yuv420_to_rgb(y1, uv1)\n\nwith torch.no_grad():\n    matches = loftr({\"image0\": rgb0[:, 0:1, :, :], \"image1\": rgb1[:, 0:1, :, :]})\n\nmatched_image = cv2.drawMatches(\n    np.moveaxis(rgb0.numpy()[0, :, :, :] * 255.0, 0, 2).astype(np.uint8),\n    [cv2.KeyPoint(x[0], x[1], 0) for x in matches[\"keypoints0\"].numpy()],\n    np.moveaxis(rgb1.numpy()[0, :, :, :] * 255.0, 0, 2).astype(np.uint8),\n    [cv2.KeyPoint(x[0], x[1], 0) for x in matches[\"keypoints1\"].numpy()],\n    [cv2.DMatch(x, x, 0) for x in range(len(matches[\"keypoints1\"].numpy()))],\n    None,\n)\n\nplt.figure(figsize=(30, 30))\nplt.imshow(matched_image)"
  },
  {
    "objectID": "nbs/data_patch_sequential.html#install-and-get-data",
    "href": "nbs/data_patch_sequential.html#install-and-get-data",
    "title": "Patch Sequential",
    "section": "Install and get data",
    "text": "Install and get data\nWe install Kornia and some dependencies, and download a simple data sample\n\n!pip install kornia\n!pip install kornia-rs\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://raw.githubusercontent.com/kornia/data/main/panda.jpg\"\ndownload_image(url)\n\n'panda.jpg'\n\n\n\nimport kornia as K\nimport torch\nfrom kornia.augmentation import ImageSequential, PatchSequential\nfrom matplotlib import pyplot as plt\n\nimg_tensor = K.io.load_image(\"panda.jpg\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\nh, w = img_tensor.shape[2:]\n\nplt.imshow(K.tensor_to_image(img_tensor))\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "nbs/data_patch_sequential.html#patch-augmentation-sequential-with-patchwise_applytrue",
    "href": "nbs/data_patch_sequential.html#patch-augmentation-sequential-with-patchwise_applytrue",
    "title": "Patch Sequential",
    "section": "Patch Augmentation Sequential with patchwise_apply=True",
    "text": "Patch Augmentation Sequential with patchwise_apply=True\npatchwise_apply is a feature that used to define unique processing pipeline for each patch location. If patchwise_apply=True, the number of pipelines defined must be as same as the number of patches in an image.\n\npseq = PatchSequential(\n    ImageSequential(\n        K.augmentation.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n        K.augmentation.RandomPerspective(0.2, p=0.5),\n        K.augmentation.RandomSolarize(0.1, 0.1, p=0.5),\n    ),\n    K.augmentation.RandomAffine(15, [0.1, 0.1], [0.7, 1.2], [0.0, 20.0], p=0.5),\n    K.augmentation.RandomPerspective(0.2, p=0.5),\n    ImageSequential(\n        K.augmentation.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n        K.augmentation.RandomPerspective(0.2, p=0.5),\n        K.augmentation.RandomSolarize(0.1, 0.1, p=0.5),\n    ),\n    K.augmentation.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n    K.augmentation.RandomAffine(15, [0.1, 0.1], [0.7, 1.2], [0.0, 20.0], p=0.5),\n    K.augmentation.RandomPerspective(0.2, p=0.5),\n    K.augmentation.RandomSolarize(0.1, 0.1, p=0.5),\n    K.augmentation.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n    K.augmentation.RandomAffine(15, [0.1, 0.1], [0.7, 1.2], [0.0, 20.0], p=0.5),\n    ImageSequential(\n        K.augmentation.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n        K.augmentation.RandomPerspective(0.2, p=0.5),\n        K.augmentation.RandomSolarize(0.1, 0.1, p=0.5),\n    ),\n    K.augmentation.RandomSolarize(0.1, 0.1, p=0.5),\n    K.augmentation.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n    K.augmentation.RandomAffine(15, [0.1, 0.1], [0.7, 1.2], [0.0, 20.0], p=0.5),\n    K.augmentation.RandomPerspective(0.2, p=0.5),\n    K.augmentation.RandomSolarize(0.1, 0.1, p=0.5),\n    patchwise_apply=True,\n    same_on_batch=True,\n)\nout_tensor = pseq(img_tensor.repeat(2, 1, 1, 1))\n\nplt.figure(figsize=(21, 9))\nplt.imshow(K.tensor_to_image(torch.cat([out_tensor[0], out_tensor[1]], dim=2)))\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "nbs/data_patch_sequential.html#patch-augmentation-sequential-with-patchwise_applyfalse",
    "href": "nbs/data_patch_sequential.html#patch-augmentation-sequential-with-patchwise_applyfalse",
    "title": "Patch Sequential",
    "section": "Patch Augmentation Sequential with patchwise_apply=False",
    "text": "Patch Augmentation Sequential with patchwise_apply=False\nIf patchwise_apply=False, all the args will be combined and applied as one pipeline for each patch.\n\npseq = PatchSequential(\n    K.augmentation.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.75),\n    K.augmentation.RandomAffine(15, [0.1, 0.1], [0.7, 1.2], [0.0, 20.0], p=0.5),\n    patchwise_apply=False,\n    same_on_batch=False,\n)\nout_tensor = pseq(img_tensor.repeat(2, 1, 1, 1))\n\nplt.figure(figsize=(21, 9))\nplt.imshow(K.tensor_to_image(torch.cat([out_tensor[0], out_tensor[1]], dim=2)))\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "nbs/connected_components.html",
    "href": "nbs/connected_components.html",
    "title": "Connected Components Algorithm",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/cells_binary.png\"\ndownload_image(url)\n\n'cells_binary.png'\n\n\n\nfrom __future__ import annotations\n\nimport kornia as K\nimport matplotlib.pyplot as plt\nimport torch\n\nWe define utility functions to visualize the segmentation properly\n\ndef create_random_labels_map(classes: int) -&gt; dict[int, tuple[int, int, int]]:\n    labels_map: Dict[int, Tuple[int, int, int]] = {}\n    for i in classes:\n        labels_map[i] = torch.randint(0, 255, (3,))\n    labels_map[0] = torch.zeros(3)\n    return labels_map\n\n\ndef labels_to_image(img_labels: torch.Tensor, labels_map: Dict[int, Tuple[int, int, int]]) -&gt; torch.Tensor:\n    \"\"\"Function that given an image with labels ids and their pixels intrensity mapping, creates a RGB\n    representation for visualisation purposes.\"\"\"\n    assert len(img_labels.shape) == 2, img_labels.shape\n    H, W = img_labels.shape\n    out = torch.empty(3, H, W, dtype=torch.uint8)\n    for label_id, label_val in labels_map.items():\n        mask = img_labels == label_id\n        for i in range(3):\n            out[i].masked_fill_(mask, label_val[i])\n    return out\n\n\ndef show_components(img, labels):\n    color_ids = torch.unique(labels)\n    labels_map = create_random_labels_map(color_ids)\n    labels_img = labels_to_image(labels, labels_map)\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 12))\n\n    # Showing Original Image\n    ax1.imshow(img)\n    ax1.axis(\"off\")\n    ax1.set_title(\"Orginal Image\")\n\n    # Showing Image after Component Labeling\n    ax2.imshow(labels_img.permute(1, 2, 0).squeeze().numpy())\n    ax2.axis(\"off\")\n    ax2.set_title(\"Component Labeling\")\n\n    plt.show()\n\nWe load the image using Kornia\n\nimg_t = K.io.load_image(\"cells_binary.png\", K.io.ImageLoadType.GRAY32)[None, ...]\n\nprint(img_t.shape)\n\ntorch.Size([1, 1, 602, 602])\n\n\nApply the Connected-component labelling algorithm using the kornia.contrib.connected_components functionality. The num_iterations parameter will control the total number of iterations of the algorithm to finish until it converges to a solution.\n\nlabels_out = K.contrib.connected_components(img_t, num_iterations=150)\nprint(labels_out.shape)\n\ntorch.Size([1, 1, 602, 602])\n\n\n\nshow_components(img_t.numpy().squeeze(), labels_out.squeeze())\n\n\n\n\n\n\n\n\nWe can also explore the labels\n\nprint(torch.unique(labels_out))\n\ntensor([     0.,  13235.,  24739.,  31039.,  32177.,  44349.,  59745.,  61289.,\n         66209.,  69449.,  78869.,  94867., 101849., 102217., 102319., 115227.,\n        115407., 137951., 138405., 150047., 158715., 162179., 170433., 170965.,\n        174279., 177785., 182867., 210145., 212647., 215451., 216119., 221291.,\n        222367., 226183., 226955., 248757., 252823., 255153., 263337., 265505.,\n        270299., 270649., 277725., 282775., 296897., 298545., 299793., 300517.,\n        313961., 316217., 321259., 322235., 335599., 337037., 340289., 347363.,\n        352235., 352721., 360801., 360903., 360965., 361073., 361165., 361197.])"
  },
  {
    "objectID": "nbs/image_enhancement.html",
    "href": "nbs/image_enhancement.html",
    "title": "Image Enhancement",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_image(\"https://github.com/kornia/data/raw/main/ninja_turtles.jpg\")\n\n'ninja_turtles.jpg'\nimport kornia as K\nimport numpy as np\nimport torch\nimport torchvision\nfrom matplotlib import pyplot as plt\ndef imshow(input: torch.Tensor):\n    out: torch.Tensor = torchvision.utils.make_grid(input, nrow=2, padding=5)\n    out_np: np.ndarray = K.utils.tensor_to_image(out)\n    plt.imshow(out_np)\n    plt.axis(\"off\")\n    plt.show()\nWe use Kornia to load an image to memory represented as a tensor\nx_rgb = K.io.load_image(\"ninja_turtles.jpg\", K.io.ImageLoadType.RGB32)[None, ...]\nCreate batch\nx_rgb = x_rgb.expand(4, -1, -1, -1)  # 4xCxHxW\nimshow(x_rgb)"
  },
  {
    "objectID": "nbs/image_enhancement.html#adjust-brightness",
    "href": "nbs/image_enhancement.html#adjust-brightness",
    "title": "Image Enhancement",
    "section": "Adjust brightness",
    "text": "Adjust brightness\n\nx_out: torch.Tensor = K.enhance.adjust_brightness(x_rgb, torch.linspace(0.2, 0.8, 4))\nimshow(x_out)"
  },
  {
    "objectID": "nbs/image_enhancement.html#adjust-contrast",
    "href": "nbs/image_enhancement.html#adjust-contrast",
    "title": "Image Enhancement",
    "section": "Adjust Contrast",
    "text": "Adjust Contrast\n\nx_out: torch.Tensor = K.enhance.adjust_contrast(x_rgb, torch.linspace(0.5, 1.0, 4))\nimshow(x_out)"
  },
  {
    "objectID": "nbs/image_enhancement.html#adjust-saturation",
    "href": "nbs/image_enhancement.html#adjust-saturation",
    "title": "Image Enhancement",
    "section": "Adjust Saturation",
    "text": "Adjust Saturation\n\nx_out: torch.Tensor = K.enhance.adjust_saturation(x_rgb, torch.linspace(0.0, 1.0, 4))\nimshow(x_out)"
  },
  {
    "objectID": "nbs/image_enhancement.html#adjust-gamma",
    "href": "nbs/image_enhancement.html#adjust-gamma",
    "title": "Image Enhancement",
    "section": "Adjust Gamma",
    "text": "Adjust Gamma\n\nx_out: torch.Tensor = K.enhance.adjust_gamma(x_rgb, torch.tensor([0.2, 0.4, 0.5, 0.6]))\nimshow(x_out)"
  },
  {
    "objectID": "nbs/image_enhancement.html#adjust-hue",
    "href": "nbs/image_enhancement.html#adjust-hue",
    "title": "Image Enhancement",
    "section": "Adjust Hue",
    "text": "Adjust Hue\n\nx_out: torch.Tensor = K.enhance.adjust_hue(x_rgb, torch.linspace(0.0, 3.14159, 4))\nimshow(x_out)"
  },
  {
    "objectID": "nbs/homography.html",
    "href": "nbs/homography.html",
    "title": "Image Alignment by Homography Optimization",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_image(\"https://github.com/kornia/data/raw/main/homography/H1to2p\")\ndownload_image(\"https://github.com/kornia/data/raw/main/homography/img1.ppm\")\ndownload_image(\"https://github.com/kornia/data/raw/main/homography/img2.ppm\")\n\n'img2.ppm'\nImport needed libraries\nimport os\nfrom typing import List\n\nimport kornia as K\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom kornia.geometry import resize\n\n# computer vision libs :D\nDefine the hyper parameters to perform the online optimisation\nlearning_rate: float = 1e-3  # the gradient optimisation update step\nnum_iterations: int = 100  # the number of iterations until convergence\nnum_levels: int = 6  # the total number of image pyramid levels\nerror_tol: float = 1e-8  # the optimisation error tolerance\n\nlog_interval: int = 100  # print log every N iterations\ndevice = K.utils.get_cuda_or_mps_device_if_available()\nprint(\"Using \", device)\n\nUsing  cpu\nDefine a container to hold the homography as a nn.Parameter so that cen be used by the autograd within the torch.optim framework.\nWe initialize the homography with the identity transformation.\nclass MyHomography(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.homography = nn.Parameter(torch.Tensor(3, 3))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.eye_(self.homography)\n\n    def forward(self) -&gt; torch.Tensor:\n        return torch.unsqueeze(self.homography, dim=0)  # 1x3x3\nRead the images and the ground truth homograpy to convert to tensor. In addition, we normalize the homography in order to smooth the gradiens during the optimisation process.\nimg_src: torch.Tensor = K.io.load_image(\"img1.ppm\", K.io.ImageLoadType.RGB32, device=device)[None, ...]\nimg_dst: torch.Tensor = K.io.load_image(\"img2.ppm\", K.io.ImageLoadType.RGB32, device=device)[None, ...]\nprint(img_src.shape)\nprint(img_dst.shape)\n\ndst_homo_src_gt = np.loadtxt(\"H1to2p\")\ndst_homo_src_gt = torch.from_numpy(dst_homo_src_gt)[None].float().to(device)\nprint(dst_homo_src_gt.shape)\nprint(dst_homo_src_gt)\n\nheight, width = img_src.shape[-2:]\n\n# warp image in normalized coordinates\nnormal_transform_pixel: torch.Tensor = K.geometry.normal_transform_pixel(height, width, device=device)\n\ndst_homo_src_gt_norm: torch.Tensor = normal_transform_pixel @ dst_homo_src_gt @ torch.inverse(normal_transform_pixel)\n\nimg_src_to_dst_gt: torch.Tensor = K.geometry.homography_warp(img_src, torch.inverse(dst_homo_src_gt_norm), (height, width))\n\nimg_src_vis: np.ndarray = K.utils.tensor_to_image(K.color.bgr_to_rgb(img_src))\nimg_dst_vis: np.ndarray = K.utils.tensor_to_image(K.color.bgr_to_rgb(img_dst))\nimg_src_to_dst_gt_vis: np.ndarray = K.utils.tensor_to_image(K.color.bgr_to_rgb(img_src_to_dst_gt))\n\ntorch.Size([1, 3, 640, 800])\ntorch.Size([1, 3, 640, 800])\ntorch.Size([1, 3, 3])\ntensor([[[ 8.7977e-01,  3.1245e-01, -3.9431e+01],\n         [-1.8389e-01,  9.3847e-01,  1.5316e+02],\n         [ 1.9641e-04, -1.6015e-05,  1.0000e+00]]])\nShow the source image, the target and the source image warped to the target using the ground truth homography transformation.\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True)\nfig.set_figheight(15)\nfig.set_figwidth(15)\n\nax1.imshow(img_src_vis)\nax1.set_title(\"Source image\")\n\nax2.imshow(img_dst_vis)\nax2.set_title(\"Destination image\")\n\nax3.imshow(img_src_to_dst_gt_vis)\nax3.set_title(\"Source to Destination image\")\nplt.show()\nInitialize the homography warper and pass the parameters to the torch.optim.Adam optimizer to perform an online gradient descent optimisation to approximate the mapping transformation between the two images.\n# create homography parameters\ndst_homo_src = MyHomography().to(device)\n\n# create optimizer\noptimizer = optim.Adam(dst_homo_src.parameters(), lr=learning_rate)\n\n# send data to device\nimg_src, img_dst = img_src.to(device), img_dst.to(device)\nIn order to perform the online optimisation, we will apply a know fine-to-coarse strategy. For this reason, we precompute a gaussian pyramid from each image with a certain number of levels.\n### compute Gaussian Pyramid\n\n\ndef get_gaussian_pyramid(img: torch.Tensor, num_levels: int) -&gt; List[torch.Tensor]:\n    r\"\"\"Utility function to compute a gaussian pyramid.\"\"\"\n    pyramid = []\n    pyramid.append(img)\n    for _ in range(num_levels - 1):\n        img_curr = pyramid[-1]\n        img_down = K.geometry.pyrdown(img_curr)\n        pyramid.append(img_down)\n    return pyramid\n\n\n# compute the gaussian pyramids\nimg_src_pyr: List[torch.Tensor] = get_gaussian_pyramid(img_src, num_levels)\nimg_dst_pyr: List[torch.Tensor] = get_gaussian_pyramid(img_dst, num_levels)"
  },
  {
    "objectID": "nbs/homography.html#main-optimization-loop",
    "href": "nbs/homography.html#main-optimization-loop",
    "title": "Image Alignment by Homography Optimization",
    "section": "Main optimization loop",
    "text": "Main optimization loop\nDefine the loss function to minimize the photometric error at each pyramid level:\n$ L = |I_{ref} - (I_{dst}, H_{ref}^{dst}))|$\n\ndef compute_scale_loss(\n    img_src: torch.Tensor,\n    img_dst: torch.Tensor,\n    dst_homo_src: nn.Module,\n    optimizer: torch.optim,\n    num_iterations: int,\n    error_tol: float,\n) -&gt; torch.Tensor:\n    assert len(img_src.shape) == len(img_dst.shape), (img_src.shape, img_dst.shape)\n\n    # init loop parameters\n    loss_tol = torch.tensor(error_tol)\n    loss_prev = torch.finfo(img_src.dtype).max\n\n    for i in range(num_iterations):\n        # create homography warper\n        src_homo_dst: torch.Tensor = torch.inverse(dst_homo_src)\n\n        _height, _width = img_src.shape[-2:]\n        warper = K.geometry.HomographyWarper(_height, _width)\n        img_src_to_dst = warper(img_src, src_homo_dst)\n\n        # compute and mask loss\n        loss = F.l1_loss(img_src_to_dst, img_dst, reduction=\"none\")  # 1x3xHxW\n\n        ones = warper(torch.ones_like(img_src), src_homo_dst)\n        loss = loss.masked_select(ones &gt; 0.9).mean()\n\n        # compute gradient and update optimizer parameters\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\nRun the main body loop to warp the images from each pyramid level and evaluate the loss to perform gradient update.\n\n# pyramid loop\n\n\nfor iter_idx in range(num_levels):\n    # get current pyramid data\n    scale: int = (num_levels - 1) - iter_idx\n    img_src = img_src_pyr[scale]\n    img_dst = img_dst_pyr[scale]\n\n    # compute scale loss\n    compute_scale_loss(img_src, img_dst, dst_homo_src(), optimizer, num_iterations, error_tol)\n\n    print(f\"Optimization iteration: {iter_idx}/{num_levels}\")\n\n    # merge warped and target image for visualization\n    h, w = img_src.shape[-2:]\n    warper = K.geometry.HomographyWarper(h, w)\n    img_src_to_dst = warper(img_src, torch.inverse(dst_homo_src()))\n    img_src_to_dst_gt = warper(img_src, torch.inverse(dst_homo_src_gt_norm))\n\n    # compute the reprojection error\n    error = F.l1_loss(img_src_to_dst, img_src_to_dst_gt, reduction=\"none\")\n    print(f\"Reprojection error: {error.mean()}\")\n\n    # show data\n    img_src_vis = K.utils.tensor_to_image(K.color.bgr_to_rgb(img_src))\n    img_dst_vis = K.utils.tensor_to_image(K.color.bgr_to_rgb(img_dst))\n    img_src_to_dst_merge = 0.65 * img_src_to_dst + 0.35 * img_dst\n    img_src_to_dst_vis = K.utils.tensor_to_image(K.color.bgr_to_rgb(img_src_to_dst_merge))\n    img_src_to_dst_gt_vis = K.utils.tensor_to_image(K.color.bgr_to_rgb(img_src_to_dst_gt))\n\n    error_sum = error.mean(dim=1, keepdim=True)\n    error_vis = K.utils.tensor_to_image(error_sum)\n\n    # show the original images at each scale level, the result of warping using\n    # the homography at moment, and the estimated error against the GT homography.\n\n    %matplotlib inline\n    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, sharey=False)\n    fig.set_figheight(15)\n    fig.set_figwidth(15)\n\n    ax1.imshow(img_src_vis)\n    ax1.set_title(\"Source image\")\n\n    ax2.imshow(img_dst_vis)\n    ax2.set_title(\"Destination image\")\n\n    ax3.imshow(img_src_to_dst_vis)\n    ax3.set_title(\"Source to Destination image\")\n\n    ax4.imshow(img_src_to_dst_gt_vis)\n    ax4.set_title(\"Source to Destination image GT\")\n\n    ax5.imshow(error_vis, cmap=\"gray\", vmin=0, vmax=1)\n    ax5.set_title(\"Error\")\n    plt.show()\n\nOptimization iteration: 0/6\nReprojection error: 0.17220830917358398\n\n\n\n\n\n\n\n\n\nOptimization iteration: 1/6\nReprojection error: 0.11565501242876053\n\n\n\n\n\n\n\n\n\nOptimization iteration: 2/6\nReprojection error: 0.018368173390626907\n\n\n\n\n\n\n\n\n\nOptimization iteration: 3/6\nReprojection error: 0.013175368309020996\n\n\n\n\n\n\n\n\n\nOptimization iteration: 4/6\nReprojection error: 0.008068887516856194\n\n\n\n\n\n\n\n\n\nOptimization iteration: 5/6\nReprojection error: 0.005315570626407862"
  },
  {
    "objectID": "nbs/image_matching_lightglue.html",
    "href": "nbs/image_matching_lightglue.html",
    "title": "Image matching example with LightGlue and DISK",
    "section": "",
    "text": "First, we will install everything needed:\n\nfresh version of kornia for DISK\nfresh version of OpenCV for MAGSAC++ geometry estimation\nkornia_moons for the conversions and visualization\n\nDocs: kornia.feature.DISK\n\n%%capture\n!pip install kornia\n!pip install kornia-rs\n!pip install kornia_moons --no-deps\n!pip install opencv-python --upgrade\n\nNow let’s download an image pair\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl_a = \"https://github.com/kornia/data/raw/main/matching/kn_church-2.jpg\"\nurl_b = \"https://github.com/kornia/data/raw/main/matching/kn_church-8.jpg\"\ndownload_image(url_a)\ndownload_image(url_b)\n\n'kn_church-8.jpg'\n\n\nFirst, imports.\n\nimport cv2\nimport kornia as K\nimport kornia.feature as KF\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom kornia.feature.adalam import AdalamFilter\nfrom kornia_moons.viz import *\n\ndevice = K.utils.get_cuda_or_mps_device_if_available()\nprint(device)\n\nmps\n\n\nHere we show how to use LightGlue with provided kornia LightGlueMatcher interface\n\n# %%capture\nfname1 = \"kn_church-2.jpg\"\nfname2 = \"kn_church-8.jpg\"\n\nlg_matcher = KF.LightGlueMatcher(\"disk\").eval().to(device)\n\n\nimg1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32, device=device)[None, ...]\nimg2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n\nnum_features = 2048\ndisk = KF.DISK.from_pretrained(\"depth\").to(device)\n\nhw1 = torch.tensor(img1.shape[2:], device=device)\nhw2 = torch.tensor(img2.shape[2:], device=device)\n\n\nwith torch.inference_mode():\n    inp = torch.cat([img1, img2], dim=0)\n    features1, features2 = disk(inp, num_features, pad_if_not_divisible=True)\n    kps1, descs1 = features1.keypoints, features1.descriptors\n    kps2, descs2 = features2.keypoints, features2.descriptors\n    lafs1 = KF.laf_from_center_scale_ori(kps1[None], torch.ones(1, len(kps1), 1, 1, device=device))\n    lafs2 = KF.laf_from_center_scale_ori(kps2[None], torch.ones(1, len(kps2), 1, 1, device=device))\n    dists, idxs = lg_matcher(descs1, descs2, lafs1, lafs2, hw1=hw1, hw2=hw2)\n\n\nprint(f\"{idxs.shape[0]} tentative matches with DISK LightGlue\")\n\nLoaded LightGlue model\n\n\nAnd here the same with original LightGlue object\n\nlg = KF.LightGlue(\"disk\").to(device).eval()\n\nimage0 = {\n    \"keypoints\": features1.keypoints[None],\n    \"descriptors\": features1.descriptors[None],\n    \"image_size\": torch.tensor(img1.shape[-2:][::-1]).view(1, 2).to(device),\n}\nimage1 = {\n    \"keypoints\": features2.keypoints[None],\n    \"descriptors\": features2.descriptors[None],\n    \"image_size\": torch.tensor(img2.shape[-2:][::-1]).view(1, 2).to(device),\n}\n\nwith torch.inference_mode():\n    out = lg({\"image0\": image0, \"image1\": image1})\n    idxs = out[\"matches\"][0]\n    print(f\"{idxs.shape[0]} tentative matches with DISK LightGlue\")\n\n724 tentative matches with DISK LightGlue\n\n\nRANSAC to get fundamental matrix\n\ndef get_matching_keypoints(kp1, kp2, idxs):\n    mkpts1 = kp1[idxs[:, 0]]\n    mkpts2 = kp2[idxs[:, 1]]\n    return mkpts1, mkpts2\n\n\nmkpts1, mkpts2 = get_matching_keypoints(kps1, kps2, idxs)\n\nFm, inliers = cv2.findFundamentalMat(\n    mkpts1.detach().cpu().numpy(), mkpts2.detach().cpu().numpy(), cv2.USAC_MAGSAC, 1.0, 0.999, 100000\n)\ninliers = inliers &gt; 0\nprint(f\"{inliers.sum()} inliers with DISK\")\n\nLet’s draw the inliers in green and tentative correspondences in yellow\n\ndraw_LAF_matches(\n    KF.laf_from_center_scale_ori(kps1[None].cpu()),\n    KF.laf_from_center_scale_ori(kps2[None].cpu()),\n    idxs.cpu(),\n    K.tensor_to_image(img1.cpu()),\n    K.tensor_to_image(img2.cpu()),\n    inliers,\n    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": (1, 1, 0.2, 0.3), \"feature_color\": None, \"vertical\": False},\n)"
  },
  {
    "objectID": "nbs/descriptors_matching.html",
    "href": "nbs/descriptors_matching.html",
    "title": "Image matching example with kornia local features: TFeat, MKD, OriNet, HyNet and OpenCV detector",
    "section": "",
    "text": "It is possible to use OpenCV local features, such as SIFT with kornia via kornia_moons library.\nFirst, we will install everything needed:\n%%capture\n!pip install kornia\n!pip install kornia-rs\n!pip install kornia_moons\n!pip install opencv-python --upgrade\nNow let’s download an image pair\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl_a = \"https://github.com/kornia/data/raw/main/matching/kn_church-2.jpg\"\nurl_b = \"https://github.com/kornia/data/raw/main/matching/kn_church-8.jpg\"\ndownload_image(url_a)\ndownload_image(url_b)\n\n'kn_church-8.jpg'\nFirst, we will define image matching pipeline with OpenCV SIFT features. We will also use kornia for the state-of-the-art match filtering – Lowe ratio + mutual nearest neighbor check."
  },
  {
    "objectID": "nbs/descriptors_matching.html#using-opencv-sift-as-is-and-converting-it-manually",
    "href": "nbs/descriptors_matching.html#using-opencv-sift-as-is-and-converting-it-manually",
    "title": "Image matching example with kornia local features: TFeat, MKD, OriNet, HyNet and OpenCV detector",
    "section": "Using OpenCV SIFT as is and converting it manually",
    "text": "Using OpenCV SIFT as is and converting it manually\n\ndef sift_matching(fname1, fname2):\n    img1 = cv2.cvtColor(cv2.imread(fname1), cv2.COLOR_BGR2RGB)\n    print(img1.shape)\n    img2 = cv2.cvtColor(cv2.imread(fname2), cv2.COLOR_BGR2RGB)\n\n    # OpenCV SIFT\n    sift = cv2.SIFT_create(8000)\n    kps1, descs1 = sift.detectAndCompute(img1, None)\n    kps2, descs2 = sift.detectAndCompute(img2, None)\n\n    # Converting to kornia for matching via AdaLAM\n    lafs1 = laf_from_opencv_SIFT_kpts(kps1)\n    lafs2 = laf_from_opencv_SIFT_kpts(kps2)\n    dists, idxs = KF.match_adalam(\n        torch.from_numpy(descs1), torch.from_numpy(descs2), lafs1, lafs2, hw1=img1.shape[:2], hw2=img2.shape[:2]\n    )\n\n    # Converting back to kornia via to use OpenCV MAGSAC++\n    tentatives = cv2_matches_from_kornia(dists, idxs)\n    src_pts = np.float32([kps1[m.queryIdx].pt for m in tentatives]).reshape(-1, 2)\n    dst_pts = np.float32([kps2[m.trainIdx].pt for m in tentatives]).reshape(-1, 2)\n\n    F, inliers_mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.25, 0.999, 100000)\n    # Drawing matches using kornia_moons\n    draw_LAF_matches(\n        lafs1,\n        lafs2,\n        idxs,\n        img1,\n        img2,\n        inliers_mask.astype(bool).reshape(-1),\n        draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": None, \"feature_color\": None, \"vertical\": False},\n    )\n    print(f\"{inliers_mask.sum()} inliers found\")\n    return\n\n\nfname1 = \"kn_church-2.jpg\"\nfname2 = \"kn_church-8.jpg\"\nsift_matching(fname1, fname2)\n\n11 inliers found"
  },
  {
    "objectID": "nbs/descriptors_matching.html#using-opencv-sift-with-kornia-matcher",
    "href": "nbs/descriptors_matching.html#using-opencv-sift-with-kornia-matcher",
    "title": "Image matching example with kornia local features: TFeat, MKD, OriNet, HyNet and OpenCV detector",
    "section": "Using OpenCV SIFT with kornia matcher",
    "text": "Using OpenCV SIFT with kornia matcher\nNow we need to define a function to feed the OpenCV keypoints into local descriptors from kornia. Luckily, that is easy with the help of kornia_moons.\n\ndef get_matching_kpts(lafs1, lafs2, idxs):\n    src_pts = KF.get_laf_center(lafs1).view(-1, 2)[idxs[:, 0]].detach().cpu().numpy()\n    dst_pts = KF.get_laf_center(lafs2).view(-1, 2)[idxs[:, 1]].detach().cpu().numpy()\n    return src_pts, dst_pts\n\n\ndef sift_korniadesc_matching(fname1, fname2, descriptor):\n    timg1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n    timg2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\n    sift = OpenCVDetectorKornia(cv2.SIFT_create(8000))\n    local_feature = KF.LocalFeature(sift, KF.LAFDescriptor(descriptor))\n\n    lafs1, resps1, descs1 = local_feature(K.color.rgb_to_grayscale(timg1))\n    lafs2, resps2, descs2 = local_feature(K.color.rgb_to_grayscale(timg2))\n\n    dists, idxs = KF.match_adalam(descs1[0], descs2[0], lafs1, lafs2, hw1=timg1.shape[2:], hw2=timg2.shape[2:])\n\n    src_pts, dst_pts = get_matching_kpts(lafs1, lafs2, idxs)\n    F, inliers_mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.25, 0.999, 100000)\n    draw_LAF_matches(\n        lafs1,\n        lafs2,\n        idxs,\n        K.tensor_to_image(timg1),\n        K.tensor_to_image(timg2),\n        inliers_mask.astype(bool),\n        draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": None, \"feature_color\": None, \"vertical\": False},\n    )\n    print(f\"{inliers_mask.sum()} inliers found\")\n\nNow let’s try kornia new descriptors – MKD and TFeat. MKD is one of the best handcrafted local feature descriptors, presented in IJCV 2018 paper “Understanding and Improving Kernel Local Descriptors”.\n\nmkd = KF.MKDDescriptor(32)\nwith torch.inference_mode():\n    sift_korniadesc_matching(fname1, fname2, mkd)\n\n12 inliers found\n\n\n\n\n\n\n\n\n\nResult seems 2 inliers better than with SIFTs. Let’s try TFeat - lightweight deep learning-based descriptor from BMVC 2016 paper “Learning local feature descriptors with triplets and shallow convolutional neural networks”\n\ntfeat = KF.TFeat(True)\nwith torch.inference_mode():\n    sift_korniadesc_matching(fname1, fname2, tfeat)\n\n22 inliers found"
  },
  {
    "objectID": "nbs/descriptors_matching.html#good-old-hardnet",
    "href": "nbs/descriptors_matching.html#good-old-hardnet",
    "title": "Image matching example with kornia local features: TFeat, MKD, OriNet, HyNet and OpenCV detector",
    "section": "Good old HardNet",
    "text": "Good old HardNet\nIn the worst-case we can always fall back to the HardNet – more robust, but also slower than TFeat and MKD, descriptor\n\ndevice = torch.device(\"cpu\")\nhardnet = KF.HardNet(True).eval()\nwith torch.inference_mode():\n    sift_korniadesc_matching(fname1, fname2, hardnet)\n\n26 inliers found"
  },
  {
    "objectID": "nbs/descriptors_matching.html#affnet",
    "href": "nbs/descriptors_matching.html#affnet",
    "title": "Image matching example with kornia local features: TFeat, MKD, OriNet, HyNet and OpenCV detector",
    "section": "AffNet",
    "text": "AffNet\nWe haven’t done yet! SIFT detector is a great tool, but we can improve it by using deep learned affine shape estimation – AffNet. You can do it, using a single function wrapper - OpenCVDetectorWithAffNetKornia.\n\ndef siftaffnet_korniadesc_matching(fname1, fname2, descriptor):\n    timg1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n    timg2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\n    # Magic is here\n    sift = OpenCVDetectorWithAffNetKornia(cv2.SIFT_create(8000))\n\n    local_feature = KF.LocalFeature(sift, KF.LAFDescriptor(descriptor))\n    with torch.inference_mode():\n        lafs1, resps1, descs1 = local_feature(K.color.rgb_to_grayscale(timg1))\n        lafs2, resps2, descs2 = local_feature(K.color.rgb_to_grayscale(timg2))\n        dists, idxs = KF.match_adalam(descs1[0], descs2[0], lafs1, lafs2, hw1=timg1.shape[2:], hw2=timg2.shape[2:])\n\n    src_pts, dst_pts = get_matching_kpts(lafs1, lafs2, idxs)\n\n    F, inliers_mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.25, 0.999, 100000)\n    draw_LAF_matches(\n        lafs1,\n        lafs2,\n        idxs,\n        K.tensor_to_image(timg1),\n        K.tensor_to_image(timg2),\n        inliers_mask.astype(bool),\n        draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": None, \"feature_color\": None, \"vertical\": False},\n    )\n    print(f\"{inliers_mask.sum()} inliers found\")\n\n\nsiftaffnet_korniadesc_matching(fname1, fname2, hardnet)\n\n39 inliers found"
  },
  {
    "objectID": "nbs/descriptors_matching.html#hynet",
    "href": "nbs/descriptors_matching.html#hynet",
    "title": "Image matching example with kornia local features: TFeat, MKD, OriNet, HyNet and OpenCV detector",
    "section": "HyNet",
    "text": "HyNet\n\nsiftaffnet_korniadesc_matching(fname1, fname2, KF.HyNet(True).eval())\n\n47 inliers found"
  },
  {
    "objectID": "nbs/descriptors_matching.html#sosnet",
    "href": "nbs/descriptors_matching.html#sosnet",
    "title": "Image matching example with kornia local features: TFeat, MKD, OriNet, HyNet and OpenCV detector",
    "section": "SOSNet",
    "text": "SOSNet\n\nsiftaffnet_korniadesc_matching(fname1, fname2, KF.SOSNet(True).eval())\n\n48 inliers found"
  },
  {
    "objectID": "nbs/descriptors_matching.html#hardnet8",
    "href": "nbs/descriptors_matching.html#hardnet8",
    "title": "Image matching example with kornia local features: TFeat, MKD, OriNet, HyNet and OpenCV detector",
    "section": "HardNet8",
    "text": "HardNet8\n\nsiftaffnet_korniadesc_matching(fname1, fname2, KF.HardNet8(True).eval())\n\n38 inliers found"
  },
  {
    "objectID": "nbs/fit_line.html",
    "href": "nbs/fit_line.html",
    "title": "Fit line tutorial",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport torch\nfrom kornia.core import concatenate, stack\nfrom kornia.geometry.line import ParametrizedLine, fit_line\n\n\nstd = 1.2  # standard deviation for the points\nnum_points = 50  # total number of points\n\n\n# create a baseline\np0 = torch.tensor([0.0, 0.0])\np1 = torch.tensor([1.0, 1.0])\n\nl1 = ParametrizedLine.through(p0, p1)\nprint(l1)\n\nOrigin: Parameter containing:\ntensor([0., 0.], requires_grad=True)\nDirection: Parameter containing:\ntensor([0.7071, 0.7071], requires_grad=True)\n\n\n\n# sample some points and weights\npts, w = [], []\nfor t in torch.linspace(-10, 10, num_points):\n    p2 = l1.point_at(t)\n    p2_noise = torch.rand_like(p2) * std\n    p2 += p2_noise\n    pts.append(p2)\n    w.append(1 - p2_noise.mean())\npts = stack(pts)\nw = stack(w)\n\n\n# fit the the line\nl2 = fit_line(pts[None, ...], w[None, ...])\nprint(l2)\n\n# project some points along the estimated line\np3 = l2.point_at(-10)\np4 = l2.point_at(10)\n\nOrigin: Parameter containing:\ntensor([[0.5933, 0.5888]], requires_grad=True)\nDirection: Parameter containing:\ntensor([[-0.7146, -0.6995]], requires_grad=True)\n\n\n\nX = concatenate((p3, p4), dim=0).detach().numpy()\nX_pts = pts.detach().numpy()\n\nplt.plot(X_pts[..., :, 0], X_pts[:, 1], \"ro\")\nplt.plot(X[:, 0], X[:, 1])\nplt.show()"
  },
  {
    "objectID": "nbs/image_matching_adalam.html",
    "href": "nbs/image_matching_adalam.html",
    "title": "Image matching example with KeyNet-AdaLAM",
    "section": "",
    "text": "First, we will install everything needed:\n\nfresh version of kornia for AdaLAM\nfresh version of OpenCV for MAGSAC++ geometry estimation\nkornia_moons for the conversions and visualization\n\nDocs: match_adalam\n\n%%capture\n!pip install kornia\n!pip install kornia-rs\n!pip install kornia_moons\n!pip install opencv-python --upgrade\n\nNow let’s download an image pair\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl_a = \"https://github.com/kornia/data/raw/main/matching/kn_church-2.jpg\"\nurl_b = \"https://github.com/kornia/data/raw/main/matching/kn_church-8.jpg\"\ndownload_image(url_a)\ndownload_image(url_b)\n\n'kn_church-8.jpg'\n\n\nFirst, imports.\n\nimport cv2\nimport kornia as K\nimport kornia.feature as KF\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom kornia_moons.viz import *\n\n# device = K.utils.get_cuda_or_mps_device_if_available()\ndevice = torch.device(\"cpu\")\n\n\n%%capture\nfname1 = \"kn_church-2.jpg\"\nfname2 = \"kn_church-8.jpg\"\n\nimg1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32, device=device)[None, ...]\nimg2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n\nfeature = KF.KeyNetAffNetHardNet(5000, True).eval().to(device)\n\ninput_dict = {\n    \"image0\": K.color.rgb_to_grayscale(img1),  # LofTR works on grayscale images only\n    \"image1\": K.color.rgb_to_grayscale(img2),\n}\n\nhw1 = torch.tensor(img1.shape[2:])\nhw2 = torch.tensor(img1.shape[2:])\n\nadalam_config = {\"device\": device}\n\nwith torch.inference_mode():\n    lafs1, resps1, descs1 = feature(K.color.rgb_to_grayscale(img1))\n    lafs2, resps2, descs2 = feature(K.color.rgb_to_grayscale(img2))\n    dists, idxs = KF.match_adalam(\n        descs1.squeeze(0),\n        descs2.squeeze(0),\n        lafs1,\n        lafs2,  # Adalam takes into account also geometric information\n        config=adalam_config,\n        hw1=hw1,\n        hw2=hw2,  # Adalam also benefits from knowing image size\n    )\n\n\nprint(f\"{idxs.shape[0]} tentative matches with AdaLAM\")\n\n405 tentative matches with AdaLAM\n\n\n\ndef get_matching_keypoints(lafs1, lafs2, idxs):\n    mkpts1 = KF.get_laf_center(lafs1).squeeze()[idxs[:, 0]].detach().cpu().numpy()\n    mkpts2 = KF.get_laf_center(lafs2).squeeze()[idxs[:, 1]].detach().cpu().numpy()\n    return mkpts1, mkpts2\n\n\nmkpts1, mkpts2 = get_matching_keypoints(lafs1, lafs2, idxs)\n\nFm, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.75, 0.999, 100000)\ninliers = inliers &gt; 0\nprint(f\"{inliers.sum()} inliers with AdaLAM\")\n\n195 inliers with AdaLAM\n\n\nLet’s draw the inliers in green and tentative correspondences in yellow\n\ndraw_LAF_matches(\n    lafs1,\n    lafs2,\n    idxs,\n    K.tensor_to_image(img1),\n    K.tensor_to_image(img2),\n    inliers,\n    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": (1, 1, 0.2, 0.3), \"feature_color\": None, \"vertical\": False},\n)"
  },
  {
    "objectID": "nbs/zca_whitening.html#install-necessary-packages",
    "href": "nbs/zca_whitening.html#install-necessary-packages",
    "title": "ZCA Whitening",
    "section": "Install necessary packages",
    "text": "Install necessary packages\n\n%%capture\n!pip install kornia numpy matplotlib\n\n\n# Import required libraries\nimport kornia as K\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import make_grid\n\n\n# Select runtime device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using {device}\")\n\nUsing cuda:0"
  },
  {
    "objectID": "nbs/zca_whitening.html#zca-on-mnist",
    "href": "nbs/zca_whitening.html#zca-on-mnist",
    "title": "ZCA Whitening",
    "section": "ZCA on MNIST",
    "text": "ZCA on MNIST\nDownload and load the MNIST dataset.\n\n%%capture\ndataset = datasets.MNIST(\"./data/mnist\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n\nStack whole dataset in order to fit ZCA on whole dataset.\n\nimages = []\nfor i in range(len(dataset)):\n    im, _ = dataset[i]\n    images.append(im)\nimages = torch.stack(images, dim=0).to(device)\n\nCreate an ZCA object and fit the transformation in the forward pass. Setting include_fit is necessary if you need to include the ZCA fitting processing the backwards pass.\n\nzca = K.enhance.ZCAWhitening(eps=0.1)\nimages_zca = zca(images, include_fit=True)\n\nThe result shown should enhance the edges of the MNIST digits because the regularization parameter \\(\\epsilon\\) increases the importance of the higher frequencies which typically correspond to the lowest eigenvalues in ZCA. The result looks similar to the demo from the Stanford ZCA tutorial\n\ngrid_im = make_grid(images[0:30], nrow=5, normalize=True).cpu().numpy()\ngrid_zca = make_grid(images_zca[0:30], nrow=5, normalize=True).cpu().numpy()\n\n\nplt.figure(figsize=(15, 15))\nplt.subplot(1, 2, 1)\nplt.imshow(np.transpose(grid_im, [1, 2, 0]))\nplt.title(\"Input Images\")\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1, 2, 2)\nplt.imshow(np.transpose(grid_zca, [1, 2, 0]))\nplt.title(r\"ZCA Images $\\epsilon = 0.1$\")\nplt.xticks([])\nplt.yticks([])\nplt.show()"
  },
  {
    "objectID": "nbs/zca_whitening.html#zca-on-cifar-10",
    "href": "nbs/zca_whitening.html#zca-on-cifar-10",
    "title": "ZCA Whitening",
    "section": "ZCA on CIFAR-10",
    "text": "ZCA on CIFAR-10\nIn the next example, we explore using ZCA on the CIFAR 10 dataset, which is a dataset of color images (e.g 4-D tensor \\([B, C, H, W]\\)). In the cell below, we prepare the dataset.\n\n%%capture\ndataset = datasets.CIFAR10(\"./data/cifar\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\nimages = []\nfor i in range(len(dataset)):\n    im, _ = dataset[i]\n    images.append(im)\nimages = torch.stack(images, dim=0).to(device)\n\nWe show another way to fit the ZCA transform using the fit method useful when ZCA is included in data augumentation pipelines. Also if compute_inv = True, this enables the computation of inverse ZCA transform in case a reconstruction is required.\n\nzca = K.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\nzca.fit(images)\nzca_images = zca(images)\nimage_re = zca.inverse_transform(zca_images)\n\nNote how the higher frequency details are more present in the ZCA normalized images for CIFAR-10 dataset.\n\ngrid_im = make_grid(images[0:30], nrow=5, normalize=True).cpu().numpy()\ngrid_zca = make_grid(zca_images[0:30], nrow=5, normalize=True).cpu().numpy()\ngrid_re = make_grid(image_re[0:30], nrow=5, normalize=True).cpu().numpy()\n\n\nerr_grid = grid_re - grid_im  # Compute error image\n\nplt.figure(figsize=(15, 15))\nplt.subplot(2, 2, 1)\nplt.imshow(np.transpose(grid_im, [1, 2, 0]))\nplt.title(\"Input Images\")\nplt.xticks([])\nplt.yticks([])\nplt.subplot(2, 2, 2)\nplt.imshow(np.transpose(grid_zca, [1, 2, 0]))\nplt.title(r\"ZCA Images $\\epsilon = 0.1$\")\nplt.xticks([])\nplt.yticks([])\nplt.subplot(2, 2, 3)\nplt.imshow(np.transpose(grid_re, [1, 2, 0]))\nplt.title(r\"Reconstructed Images\")\nplt.xticks([])\nplt.yticks([])\nplt.subplot(2, 2, 4)\nplt.imshow(np.sum(abs(np.transpose(err_grid, [1, 2, 0])), axis=-1))\nplt.colorbar()\nplt.title(\"Error Image\")\nplt.xticks([])\nplt.yticks([])\nplt.show()"
  },
  {
    "objectID": "nbs/zca_whitening.html#differentiability-of-zca",
    "href": "nbs/zca_whitening.html#differentiability-of-zca",
    "title": "ZCA Whitening",
    "section": "Differentiability of ZCA",
    "text": "Differentiability of ZCA\nWe will as simple Gaussian dataset with a mean (3,3) and a diagonal covariance of 1 and 9 to explore the differentiability of ZCA.\n\nnum_data = 100  # Number of points in the dataset\ntorch.manual_seed(1234)\nx = torch.cat([torch.randn((num_data, 1), requires_grad=True), 3 * torch.randn((num_data, 1), requires_grad=True)], dim=1) + 3\n\nplt.scatter(x.detach().numpy()[:, 0], x.detach().numpy()[:, 1])\nplt.xlim([-10, 10])\nplt.ylim([-10, 10])\nplt.show()\n\n\n\n\n\n\n\n\nHere we explore the affect of the detach_transform option when computing the backwards pass.\n\nzca_detach = K.enhance.ZCAWhitening(eps=1e-6, detach_transforms=True)\nzca_grad = K.enhance.ZCAWhitening(eps=1e-6, detach_transforms=False)\n\nAs a sanity check, the Jacobian between the input and output of the ZCA transform should be same for all data points in the detached case since the transform acts as linear transform (e.g \\(T(X-\\mu)\\)). In the non-detached case, the Jacobian should vary across datapoints since the input affects the computation of the ZCA transformation matrix (e.g. \\(T(X)(X-\\mu(X))\\)). As the number of samples increases, the Jacobians in the detached and non-detached cases should be roughly the same since the influence of a single datapoint decreases. You can test this by changing num_data . Also note that include_fit=True is included in the forward pass since creation of the transform matrix needs to be included in the forward pass in order to correctly compute the backwards pass.\n\nimport torch.autograd as autograd\n\nJ = autograd.functional.jacobian(lambda x: zca_detach(x, include_fit=True), x)\n\nnum_disp = 5\nprint(f\"Jacobian matrices detached for the first {num_disp} points\")\nfor i in range(num_disp):\n    print(J[i, :, i, :])\n\nprint(\"\\n\")\n\nJ = autograd.functional.jacobian(lambda x: zca_grad(x, include_fit=True), x)\nprint(f\"Jacobian matrices attached for the first {num_disp} points\")\nfor i in range(num_disp):\n    print(J[i, :, i, :])\n\nJacobian matrices detached for the first 5 points\ntensor([[ 1.0177, -0.0018],\n        [-0.0018,  0.3618]])\ntensor([[ 1.0177, -0.0018],\n        [-0.0018,  0.3618]])\ntensor([[ 1.0177, -0.0018],\n        [-0.0018,  0.3618]])\ntensor([[ 1.0177, -0.0018],\n        [-0.0018,  0.3618]])\ntensor([[ 1.0177, -0.0018],\n        [-0.0018,  0.3618]])\n\n\nJacobian matrices attached for the first 5 points\ntensor([[ 1.0003, -0.0018],\n        [-0.0018,  0.3547]])\ntensor([[ 1.0006, -0.0027],\n        [-0.0027,  0.3555]])\ntensor([[ 0.9911, -0.0028],\n        [-0.0028,  0.3506]])\ntensor([[9.9281e-01, 4.4671e-04],\n        [4.4671e-04, 3.5368e-01]])\ntensor([[ 1.0072, -0.0019],\n        [-0.0019,  0.3581]])\n\n\nLastly, we plot the ZCA whitened data. Note that setting the include_fit to True stores the resulting transformations for future use.\n\nx_zca = zca_detach(x).detach().numpy()\nplt.scatter(x_zca[:, 0], x_zca[:, 1])\nplt.ylim([-4, 4])\nplt.xlim([-4, 4])\nplt.show()"
  },
  {
    "objectID": "nbs/unsharp_mask.html",
    "href": "nbs/unsharp_mask.html",
    "title": "Sharpen image using unsharp mask",
    "section": "",
    "text": "We first install kornia\n\n%%capture\n%matplotlib inline\n!pip install kornia\n!pip install kornia-rs\n\n\nimport kornia\nimport matplotlib.pyplot as plt\n\nkornia.__version__\n\n'0.6.12'\n\n\nDownloading the example image.\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/squirrel.jpg\"\ndownload_image(url)\n\n'squirrel.jpg'\n\n\n\n# Read the image with Kornia\ndata = kornia.io.load_image(\"squirrel.jpg\", kornia.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\nWe create Unsharp Mask filter object and apply it to data. The unsharp mask filter is initialized with the format kornia.filters.UnsharpMask(kernel_size, sigma). You can tune these parametres and experiment!\n\nsharpen = kornia.filters.UnsharpMask((9, 9), (2.5, 2.5))\nsharpened_tensor = sharpen(data)\ndifference = (sharpened_tensor - data).abs()\n\n\n# Converting the sharpened tensor to image\nsharpened_image = kornia.utils.tensor_to_image(sharpened_tensor)\ndifference_image = kornia.utils.tensor_to_image(difference)\n\nSo, let us understand how we arrived till here.\n\nIn the unsharp mask technique, first a gaussian blur is applied to the data.\nThen the blur is subtracted from the orignal data.\nThe resultant is added to the origanl data.\nSo, what do we get? Sharpened data!\n\n\n# To display the input image, sharpened image and the difference image\nfig, axs = plt.subplots(1, 3, figsize=(16, 10))\naxs = axs.ravel()\n\naxs[0].axis(\"off\")\naxs[0].set_title(\"image source\")\naxs[0].imshow(kornia.tensor_to_image(data))\n\naxs[1].axis(\"off\")\naxs[1].set_title(\"sharpened\")\naxs[1].imshow(sharpened_image)\n\naxs[2].axis(\"off\")\naxs[2].set_title(\"difference\")\naxs[2].imshow(difference_image)\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "nbs/image_matching_disk.html",
    "href": "nbs/image_matching_disk.html",
    "title": "Image matching example with DISK local features",
    "section": "",
    "text": "First, we will install everything needed:\n\nfresh version of kornia for DISK\nfresh version of OpenCV for MAGSAC++ geometry estimation\nkornia_moons for the conversions and visualization\n\nDocs: kornia.feature.DISK\n\n%%capture\n!pip install kornia\n!pip install kornia-rs\n!pip install kornia_moons --no-deps\n!pip install opencv-python --upgrade\n\nNow let’s download an image pair\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl_a = \"https://github.com/kornia/data/raw/main/matching/kn_church-2.jpg\"\nurl_b = \"https://github.com/kornia/data/raw/main/matching/kn_church-8.jpg\"\ndownload_image(url_a)\ndownload_image(url_b)\n\n'kn_church-8.jpg'\n\n\nFirst, imports.\n\nimport cv2\nimport kornia as K\nimport kornia.feature as KF\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom kornia.feature.adalam import AdalamFilter\nfrom kornia_moons.viz import *\n\ndevice = K.utils.get_cuda_or_mps_device_if_available()\nprint(device)\n\ncuda:0\n\n\n\n# %%capture\nfname1 = \"kn_church-2.jpg\"\nfname2 = \"kn_church-8.jpg\"\n\nadalam_config = KF.adalam.get_adalam_default_config()\n# adalam_config['orientation_difference_threshold'] = None\n# adalam_config['scale_rate_threshold'] = None\nadalam_config[\"force_seed_mnn\"] = False\nadalam_config[\"search_expansion\"] = 16\nadalam_config[\"ransac_iters\"] = 256\n\n\nimg1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32, device=device)[None, ...]\nimg2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n\nnum_features = 2048\ndisk = KF.DISK.from_pretrained(\"depth\").to(device)\n\nhw1 = torch.tensor(img1.shape[2:], device=device)\nhw2 = torch.tensor(img2.shape[2:], device=device)\n\nmatch_with_adalam = True\n\nwith torch.inference_mode():\n    inp = torch.cat([img1, img2], dim=0)\n    features1, features2 = disk(inp, num_features, pad_if_not_divisible=True)\n    kps1, descs1 = features1.keypoints, features1.descriptors\n    kps2, descs2 = features2.keypoints, features2.descriptors\n    if match_with_adalam:\n        lafs1 = KF.laf_from_center_scale_ori(kps1[None], 96 * torch.ones(1, len(kps1), 1, 1, device=device))\n        lafs2 = KF.laf_from_center_scale_ori(kps2[None], 96 * torch.ones(1, len(kps2), 1, 1, device=device))\n\n        dists, idxs = KF.match_adalam(descs1, descs2, lafs1, lafs2, hw1=hw1, hw2=hw2, config=adalam_config)\n    else:\n        dists, idxs = KF.match_smnn(descs1, descs2, 0.98)\n\n\nprint(f\"{idxs.shape[0]} tentative matches with DISK AdaLAM\")\n\n222 tentative matches with DISK AdaLAM\n\n\n\ndef get_matching_keypoints(kp1, kp2, idxs):\n    mkpts1 = kp1[idxs[:, 0]]\n    mkpts2 = kp2[idxs[:, 1]]\n    return mkpts1, mkpts2\n\n\nmkpts1, mkpts2 = get_matching_keypoints(kps1, kps2, idxs)\n\nFm, inliers = cv2.findFundamentalMat(\n    mkpts1.detach().cpu().numpy(), mkpts2.detach().cpu().numpy(), cv2.USAC_MAGSAC, 1.0, 0.999, 100000\n)\ninliers = inliers &gt; 0\nprint(f\"{inliers.sum()} inliers with DISK\")\n\n103 inliers with DISK\n\n\nLet’s draw the inliers in green and tentative correspondences in yellow\n\ndraw_LAF_matches(\n    KF.laf_from_center_scale_ori(kps1[None].cpu()),\n    KF.laf_from_center_scale_ori(kps2[None].cpu()),\n    idxs.cpu(),\n    K.tensor_to_image(img1.cpu()),\n    K.tensor_to_image(img2.cpu()),\n    inliers,\n    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": (1, 1, 0.2, 0.3), \"feature_color\": None, \"vertical\": False},\n)"
  },
  {
    "objectID": "nbs/image_histogram.html#install-kornia",
    "href": "nbs/image_histogram.html#install-kornia",
    "title": "Image histogram and equalizations techniques",
    "section": "Install Kornia",
    "text": "Install Kornia\n\n%%capture\n!pip install kornia\n!pip install kornia-rs"
  },
  {
    "objectID": "nbs/image_histogram.html#prepare-the-data",
    "href": "nbs/image_histogram.html#prepare-the-data",
    "title": "Image histogram and equalizations techniques",
    "section": "Prepare the data",
    "text": "Prepare the data\nThe low contrast color image used in this tutorial can be downloaded here (By Biem (Own work) [Public domain], via Wikimedia Commons)\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_image(\"https://github.com/cvg/SOLD2/raw/main/assets/images/terrace0.JPG\")\ndownload_image(\"https://github.com/kornia/data/raw/main/soccer.jpg\")\ndownload_image(\"https://github.com/kornia/data/raw/main/mountains.jpg\")\n\n'mountains.jpg'\n\n\n\nfrom typing import List, Tuple\n\nimport cv2\nimport kornia as K\nimport numpy as np\nimport torch\nimport torchvision\nfrom matplotlib import pyplot as plt\n\nImage show functionality\n\ndef imshow(input: torch.Tensor, height: int, width: int):\n    out: torch.Tensor = torchvision.utils.make_grid(input, nrow=2)\n    out_np: np.array = K.utils.tensor_to_image(out)\n    plt.figure(figsize=(height, width))\n    plt.imshow(out_np)\n    plt.axis(\"off\");\n\nImage read functionality\n\ndef imread(data_path: str) -&gt; torch.Tensor:\n    \"\"\"Utility function that load an image an convert to torch.\"\"\"\n    img_t = K.io.load_image(data_path, K.io.ImageLoadType.RGB32)\n    img_t = K.geometry.resize(img_t, 1200, side=\"long\", align_corners=True)[..., :600, :]\n\n    return img_t[None, ...]\n\nImage and histogram plot functionality\n\ndef histogram_img(img_t: torch.Tensor, size: Tuple[int, int] = (16, 4)):\n    CH, H, W = img_t.shape\n    img = K.utils.tensor_to_image(img_t.mul(255.0).byte())\n\n    plt.figure(figsize=size)\n    ax1 = plt.subplot(1, 2, 1)\n    ax2 = plt.subplot(1, 2, 2)\n\n    colors = (\"b\", \"g\", \"r\")\n    kwargs = dict(histtype=\"stepfilled\", alpha=0.3, density=True, ec=\"k\")\n\n    for i in range(CH):\n        img_vec = img[..., i].flatten()\n        ax2.hist(img_vec, range=(0, 255), bins=256, color=colors[i], **kwargs)\n\n    ax1.imshow(img, cmap=(None if CH &gt; 1 else \"gray\"))\n    ax1.grid(False)\n    ax1.axis(\"off\")\n\n    plt.show()\n\nLoad the images in batch using OpenCV and show them as a grid.\n\nimg_rgb_list: List[torch.Tensor] = []\nimg_rgb_list.append(imread(\"terrace0.JPG\"))\nimg_rgb_list.append(imread(\"mountains.jpg\"))\nimg_rgb_list.append(imread(\"soccer.jpg\"))\n\n\n# cast to torch.Tensor\nimg_rgb: torch.Tensor = torch.cat(img_rgb_list, dim=0)\nprint(f\"Image tensor shape: {img_rgb.shape}\")\n\n# Disable the line below to make everything happen in the GPU !\n# img_rbg = img_rbg.cuda()\n\nimshow(img_rgb, 10, 10)  # plot grid !\n\nImage tensor shape: torch.Size([3, 3, 600, 1200])"
  },
  {
    "objectID": "nbs/image_histogram.html#image-histogram",
    "href": "nbs/image_histogram.html#image-histogram",
    "title": "Image histogram and equalizations techniques",
    "section": "Image Histogram",
    "text": "Image Histogram\nDefinition - An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image.[1] It plots the number of pixels for each tonal value. By looking at the histogram for a specific image a viewer will be able to judge the entire tonal distribution at a glance Read more - Wikipedia.\nIn short - An image histogram is: - It is a graphical representation of the intensity distribution of an image. - It quantifies the number of pixels for each intensity value considered.\nSee also OpenCV tutorial: https://docs.opencv.org/master/d4/d1b/tutorial_histogram_equalization.html\n\nLightness with Kornia\nWe first will compute the histogram of the lightness of the image. To do so, we compute first the color space Lab and take the first component known as luminance that reflects the lightness of the scene.\nNotice that kornia Lab representation is in the range of [0, 100] and for convenience to plot the histogram we will normalize the image between [0, 1].\nNote: kornia computes in batch, for convenience we show only one image result. That’s it - modify below the plot_indices variable to explore the results of the batch.\n\nplot_indices: int = 0  # try: [0, 1, 2, 3]\n\nTip: replace K.color.rgb_to_lab by K.color.rgb_to_grayscale to see the pixel distribution in the grayscale color space. Explore also kornia.color for more exotic color spaces.\n\nimg_lab: torch.Tensor = K.color.rgb_to_lab(img_rgb)\nlightness: torch.Tensor = img_lab[..., :1, :, :] / 100.0  # L in lab is in range [0, 100]\n\nhistogram_img(lightness[plot_indices])\n\n\n\n\n\n\n\n\n\n\nRGB histogram with Kornia\nSimilar to above - you can just visualize the three (red, green, blue) channels pixel distribution.\n\nTip - Use as follows to visualize a single channel (green)\n\nhistogram_img(img_rgb[plot_indices, 1:2])\n\n\n\nhistogram_img(img_rgb[plot_indices])"
  },
  {
    "objectID": "nbs/image_histogram.html#histogram-stretching",
    "href": "nbs/image_histogram.html#histogram-stretching",
    "title": "Image histogram and equalizations techniques",
    "section": "Histogram stretching",
    "text": "Histogram stretching\nSometimes our images have a pixel distribution that is not suitable for our application, being biased to a certain range depending on the illumination of the scene.\nIn the next sections, we are going to show a couple of techniques to solve those issues. We will start with a basic technique to normalize the image by its minimum and maximum values with the objective to strecth the image histrogram.\n\non the lightness with Kornia\nWe use kornina.enhance.normalize_min_max to normalize the image Luminance. Note: compare the histrograms with the original image.\nTip - play with the other functions from kornia.enhance to modify the intensity values of the image and thus its histograms.\n\nlightness_stretched = K.enhance.normalize_min_max(lightness)\nhistogram_img(lightness_stretched[plot_indices])\n\n\n\n\n\n\n\n\nIn order to properly visualize the effect of the normalization in the color histogram, we take the normalized Luminance and use it to cast back to RGB.\n\nimg_rgb_new = K.color.lab_to_rgb(torch.cat([lightness_stretched * 100.0, img_lab[:, 1:]], dim=1))\nhistogram_img(img_rgb_new[plot_indices])\n\n\n\n\n\n\n\n\n\n\non the RGB with Kornia\nIn this case we normalize each channel independently where we can see that resulting image is not as clear as the one only stretching the Luminance.\n\nrgb_stretched = K.enhance.normalize_min_max(img_rgb)\nhistogram_img(rgb_stretched[plot_indices])"
  },
  {
    "objectID": "nbs/image_histogram.html#histogram-equalization",
    "href": "nbs/image_histogram.html#histogram-equalization",
    "title": "Image histogram and equalizations techniques",
    "section": "Histogram Equalization",
    "text": "Histogram Equalization\nA more advanced technique to improve the pixel distribution is the so called Histogram equalization - a method in image processing of contrast adjustment using the image’s histogram [Read more - Wikipedia].\nIn kornia we have implemented in terms of torch tensor to equalize the images in batch and the gpu very easily.\n\non the lightness with Kornia\n\nlightness_equalized = K.enhance.equalize(lightness)\nhistogram_img(lightness_equalized[plot_indices])\n\n\n\n\n\n\n\n\nWe convert back from Lab to RGB using the equalized Luminance and visualize the histogram of the RGB.\n\nimg_rgb_new = K.color.lab_to_rgb(torch.cat([lightness_equalized * 100.0, img_lab[:, 1:]], dim=1))\nhistogram_img(img_rgb_new[plot_indices])\n\n\n\n\n\n\n\n\n\n\non the RGB with Kornia\n\nrgb_equalized = K.enhance.equalize(img_rgb)\nhistogram_img(rgb_equalized[plot_indices])\n\n\n\n\n\n\n\n\n\n\non the RGB with OpenCV\nJust to compare against OpenCV - close results :)\n\nrgb_equalized_cv = []\nfor img in img_rgb:\n    equ00 = torch.tensor(cv2.equalizeHist(K.utils.tensor_to_image(img[0].mul(255).clamp(0, 255).byte())))\n    equ01 = torch.tensor(cv2.equalizeHist(K.utils.tensor_to_image(img[1].mul(255).clamp(0, 255).byte())))\n    equ02 = torch.tensor(cv2.equalizeHist(K.utils.tensor_to_image(img[2].mul(255).clamp(0, 255).byte())))\n    rgb_equalized_cv.append(torch.stack([equ00, equ01, equ02]))\nrgb_equalized_cv = torch.stack(rgb_equalized_cv)\n\nhistogram_img(rgb_equalized_cv[plot_indices] / 255.0)"
  },
  {
    "objectID": "nbs/image_histogram.html#adaptive-histogram-equalization",
    "href": "nbs/image_histogram.html#adaptive-histogram-equalization",
    "title": "Image histogram and equalizations techniques",
    "section": "Adaptive Histogram Equalization",
    "text": "Adaptive Histogram Equalization\nAdaptive histogram equalization (AHE) is a computer image processing technique used to improve contrast in images. It differs from ordinary histogram equalization in the respect that the adaptive method computes several histograms, each corresponding to a distinct section of the image, and uses them to redistribute the lightness values of the image. It is therefore suitable for improving the local contrast and enhancing the definitions of edges in each region of an image [Read more - Wikipedia].\n\non the lightness with Kornia\nWe will use kornia.enhance.equalize_clahe and by playing with the clip_limit and grid_size variables to produce different effects to the image.\n\nlightness_equalized = K.enhance.equalize_clahe(lightness, clip_limit=0.0)\nhistogram_img(lightness_equalized[plot_indices])\n\n\n\n\n\n\n\n\nWe convert back from Lab to RGB using the equalized Luminance and visualize the histogram of the RGB.\n\nimg_rgb_new = K.color.lab_to_rgb(torch.cat([lightness_equalized * 100.0, img_lab[:, 1:]], dim=1))\nhistogram_img(img_rgb_new[plot_indices])\n\n\n\n\n\n\n\n\n\n\non the RGB with Kornia\n\nrgb_equalized = K.enhance.equalize_clahe(img_rgb, clip_limit=0.0)\nhistogram_img(rgb_equalized[plot_indices])"
  },
  {
    "objectID": "nbs/image_histogram.html#contrast-limited-adaptive-histogram-equalization-clahe",
    "href": "nbs/image_histogram.html#contrast-limited-adaptive-histogram-equalization-clahe",
    "title": "Image histogram and equalizations techniques",
    "section": "Contrast Limited Adaptive Histogram Equalization (CLAHE)",
    "text": "Contrast Limited Adaptive Histogram Equalization (CLAHE)\nAn improvement of the algorithm is CLAHE that divides the image into small blocks and controlled by the variable grid_size. This means, that the equalization is performed locally in each of the NxM sublocks to obtain a better distribution of the pixel values.\n\non the lightness with Kornia\n\nlightness_equalized = K.enhance.equalize_clahe(lightness, clip_limit=20.0, grid_size=(8, 8))\nhistogram_img(lightness_equalized[plot_indices])\n\n\n\n\n\n\n\n\nWe convert back from Lab to RGB using the equalized Luminance and visualize the histogram of the RGB.\n\nimg_rgb_new = K.color.lab_to_rgb(torch.cat([lightness_equalized * 100.0, img_lab[:, 1:]], dim=1))\nhistogram_img(img_rgb_new[plot_indices])\n\n\n\n\n\n\n\n\n\n\non the RGB with Kornia\nWe directly equalize all the RGB channels at once\n\nrgb_equalized = K.enhance.equalize_clahe(img_rgb, clip_limit=20.0, grid_size=(8, 8))\nhistogram_img(rgb_equalized[plot_indices])\n\n\n\n\n\n\n\n\n\n\non the RGB with OpenCV\n\nimgs = []\nclahe = cv2.createCLAHE(clipLimit=20.0, tileGridSize=(8, 8))\nfor im in img_rgb:\n    # equalize channels independently as gray scale images\n    equ00 = torch.tensor(clahe.apply(K.utils.tensor_to_image(im[0].mul(255).clamp(0, 255).byte())))\n    equ01 = torch.tensor(clahe.apply(K.utils.tensor_to_image(im[1].mul(255).clamp(0, 255).byte())))\n    equ02 = torch.tensor(clahe.apply(K.utils.tensor_to_image(im[2].mul(255).clamp(0, 255).byte())))\n    imgs.append(torch.stack([equ00, equ01, equ02]))\nimgs = torch.stack(imgs)\n\nhistogram_img(imgs[plot_indices] / 255.0)"
  },
  {
    "objectID": "nbs/line_detection_and_matching_sold2.html#setup",
    "href": "nbs/line_detection_and_matching_sold2.html#setup",
    "title": "Line detection and matching example with SOLD2: Self-supervised Occlusion-aware Line Description and Detection",
    "section": "Setup",
    "text": "Setup\nInstall the libraries:\n\n%%capture\n!pip install kornia\n!pip install kornia-rs\n!pip install opencv-python --upgrade # Just for windows\n!pip install matplotlib\n\nNow let’s download an image\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1].split(\"?\")[0] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_image(\"https://github.com/cvg/SOLD2/raw/main/assets/images/terrace0.JPG\")\ndownload_image(\"https://github.com/cvg/SOLD2/raw/main/assets/images/terrace1.JPG\")\n\n'terrace1.JPG'\n\n\nThen, we will load the libraries\n\nimport kornia as K\nimport kornia.feature as KF\nimport torch\n\nLoad the images and convert into torch tensors.\n\nfname1 = \"terrace0.JPG\"\nfname2 = \"terrace1.JPG\"\n\ntorch_img1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32)[None, ...]\ntorch_img2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32)[None, ...]\n\ntorch_img1.shape, torch_img2.shape\n\n(torch.Size([1, 3, 496, 744]), torch.Size([1, 3, 496, 744]))\n\n\nPrepare the data for the model, which is expected a batch of images in gray scale (shape: (Batch size, 1, Height, Width)).\nThe SOLD2 model was tuned for images in the range 400~800px when using config=None.\n\n# First, convert the images to gray scale\ntorch_img1_gray = K.color.rgb_to_grayscale(torch_img1)\ntorch_img2_gray = K.color.rgb_to_grayscale(torch_img2)\n\n\ntorch_img1_gray.shape, torch_img2_gray.shape\n\n(torch.Size([1, 1, 496, 744]), torch.Size([1, 1, 496, 744]))\n\n\n\n# then, stack the images to create/simulate a batch\nimgs = torch.cat([torch_img1_gray, torch_img2_gray], dim=0)\n\nimgs.shape\n\ntorch.Size([2, 1, 496, 744])"
  },
  {
    "objectID": "nbs/line_detection_and_matching_sold2.html#performs-line-detection-and-matching",
    "href": "nbs/line_detection_and_matching_sold2.html#performs-line-detection-and-matching",
    "title": "Line detection and matching example with SOLD2: Self-supervised Occlusion-aware Line Description and Detection",
    "section": "Performs line detection and matching",
    "text": "Performs line detection and matching\nLoad the sold2 model with pre-trained=True, which will download and set pre-trained weights to the model.\n\n%%capture\nsold2 = KF.SOLD2(pretrained=True, config=None)\n\n\nPerform the model prediction\n\n%%capture\nwith torch.inference_mode():\n    outputs = sold2(imgs)\n\nOrganize the outputs for demo.\nAttention: The detected line segments is in ij coordinates convention.\n\noutputs.keys()\n\ndict_keys(['junction_heatmap', 'line_heatmap', 'dense_desc', 'line_segments'])\n\n\n\nline_seg1 = outputs[\"line_segments\"][0]\nline_seg2 = outputs[\"line_segments\"][1]\ndesc1 = outputs[\"dense_desc\"][0]\ndesc2 = outputs[\"dense_desc\"][1]\n\n\n\nPerform line matching\n\nwith torch.inference_mode():\n    matches = sold2.match(line_seg1, line_seg2, desc1[None], desc2[None])\n\n\nvalid_matches = matches != -1\nmatch_indices = matches[valid_matches]\n\nmatched_lines1 = line_seg1[valid_matches]\nmatched_lines2 = line_seg2[match_indices]"
  },
  {
    "objectID": "nbs/line_detection_and_matching_sold2.html#plot-lines-detected-and-also-the-match",
    "href": "nbs/line_detection_and_matching_sold2.html#plot-lines-detected-and-also-the-match",
    "title": "Line detection and matching example with SOLD2: Self-supervised Occlusion-aware Line Description and Detection",
    "section": "Plot lines detected and also the match",
    "text": "Plot lines detected and also the match\nPlot functions adapted from original code.\n\nimport copy\n\nimport matplotlib\nimport matplotlib.colors as mcolors\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_images(imgs, titles=None, cmaps=\"gray\", dpi=100, size=6, pad=0.5):\n    \"\"\"Plot a set of images horizontally.\n    Args:\n        imgs: a list of NumPy or PyTorch images, RGB (H, W, 3) or mono (H, W).\n        titles: a list of strings, as titles for each image.\n        cmaps: colormaps for monochrome images.\n    \"\"\"\n    n = len(imgs)\n    if not isinstance(cmaps, (list, tuple)):\n        cmaps = [cmaps] * n\n    figsize = (size * n, size * 3 / 4) if size is not None else None\n    fig, ax = plt.subplots(1, n, figsize=figsize, dpi=dpi)\n    if n == 1:\n        ax = [ax]\n    for i in range(n):\n        ax[i].imshow(imgs[i], cmap=plt.get_cmap(cmaps[i]))\n        ax[i].get_yaxis().set_ticks([])\n        ax[i].get_xaxis().set_ticks([])\n        ax[i].set_axis_off()\n        for spine in ax[i].spines.values():  # remove frame\n            spine.set_visible(False)\n        if titles:\n            ax[i].set_title(titles[i])\n    fig.tight_layout(pad=pad)\n\n\ndef plot_lines(lines, line_colors=\"orange\", point_colors=\"cyan\", ps=4, lw=2, indices=(0, 1)):\n    \"\"\"Plot lines and endpoints for existing images.\n    Args:\n        lines: list of ndarrays of size (N, 2, 2).\n        colors: string, or list of list of tuples (one for each keypoints).\n        ps: size of the keypoints as float pixels.\n        lw: line width as float pixels.\n        indices: indices of the images to draw the matches on.\n    \"\"\"\n    if not isinstance(line_colors, list):\n        line_colors = [line_colors] * len(lines)\n    if not isinstance(point_colors, list):\n        point_colors = [point_colors] * len(lines)\n\n    fig = plt.gcf()\n    ax = fig.axes\n    assert len(ax) &gt; max(indices)\n    axes = [ax[i] for i in indices]\n    fig.canvas.draw()\n\n    # Plot the lines and junctions\n    for a, l, lc, pc in zip(axes, lines, line_colors, point_colors):\n        for i in range(len(l)):\n            line = matplotlib.lines.Line2D(\n                (l[i, 1, 1], l[i, 0, 1]),\n                (l[i, 1, 0], l[i, 0, 0]),\n                zorder=1,\n                c=lc,\n                linewidth=lw,\n            )\n            a.add_line(line)\n        pts = l.reshape(-1, 2)\n        a.scatter(pts[:, 1], pts[:, 0], c=pc, s=ps, linewidths=0, zorder=2)\n\n\ndef plot_color_line_matches(lines, lw=2, indices=(0, 1)):\n    \"\"\"Plot line matches for existing images with multiple colors.\n    Args:\n        lines: list of ndarrays of size (N, 2, 2).\n        lw: line width as float pixels.\n        indices: indices of the images to draw the matches on.\n    \"\"\"\n    n_lines = len(lines[0])\n\n    cmap = plt.get_cmap(\"nipy_spectral\", lut=n_lines)\n    colors = np.array([mcolors.rgb2hex(cmap(i)) for i in range(cmap.N)])\n\n    np.random.shuffle(colors)\n\n    fig = plt.gcf()\n    ax = fig.axes\n    assert len(ax) &gt; max(indices)\n    axes = [ax[i] for i in indices]\n    fig.canvas.draw()\n\n    # Plot the lines\n    for a, l in zip(axes, lines):\n        for i in range(len(l)):\n            line = matplotlib.lines.Line2D(\n                (l[i, 1, 1], l[i, 0, 1]),\n                (l[i, 1, 0], l[i, 0, 0]),\n                zorder=1,\n                c=colors[i],\n                linewidth=lw,\n            )\n            a.add_line(line)\n\n\nimgs_to_plot = [K.tensor_to_image(torch_img1), K.tensor_to_image(torch_img2)]\nlines_to_plot = [line_seg1.numpy(), line_seg2.numpy()]\n\nplot_images(imgs_to_plot, [\"Image 1 - detected lines\", \"Image 2 - detected lines\"])\nplot_lines(lines_to_plot, ps=3, lw=2, indices={0, 1})\n\n\n\n\n\n\n\n\n\nplot_images(imgs_to_plot, [\"Image 1 - matched lines\", \"Image 2 - matched lines\"])\nplot_color_line_matches([matched_lines1, matched_lines2], lw=2)"
  },
  {
    "objectID": "nbs/line_detection_and_matching_sold2.html#example-of-homography-from-line-segment-correspondences-from-sold2",
    "href": "nbs/line_detection_and_matching_sold2.html#example-of-homography-from-line-segment-correspondences-from-sold2",
    "title": "Line detection and matching example with SOLD2: Self-supervised Occlusion-aware Line Description and Detection",
    "section": "Example of homography from line segment correspondences from SOLD2",
    "text": "Example of homography from line segment correspondences from SOLD2\nRobust geometry estimation with Random sample consensus (RANSAC)\nLoad the model:\n\nransac = K.geometry.RANSAC(model_type=\"homography_from_linesegments\", inl_th=3.0)\n\n\nPerform the model correspondencies\n\nH_ransac, correspondence_mask = ransac(matched_lines1.flip(dims=(2,)), matched_lines2.flip(dims=(2,)))\n\nWrap the image 1 to image 2\n\nimg1_warp_to2 = K.geometry.warp_perspective(torch_img1, H_ransac[None], (torch_img1.shape[-2:]))\n\n\n\nPlot the matched lines and wrapped image\n\nplot_images(\n    imgs_to_plot,\n    [\"Image 1 - lines with correspondence\", \"Image 2 - lines with correspondence\"],\n)\nplot_color_line_matches([matched_lines1[correspondence_mask], matched_lines2[correspondence_mask]], lw=2)\n\n\n\n\n\n\n\n\n\nplot_images(\n    [K.tensor_to_image(torch_img2), K.tensor_to_image(img1_warp_to2)],\n    [\"Image 2\", \"Image 1 wrapped to 2\"],\n)"
  },
  {
    "objectID": "nbs/geometry_generate_patch.html",
    "href": "nbs/geometry_generate_patch.html",
    "title": "Image patch generation",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_image(\"https://github.com/kornia/data/raw/main/homography/img1.ppm\")\n\n'img1.ppm'\n\n\nFirst load libraries and images\n\n%matplotlib inline\nimport kornia as K\nimport matplotlib.pyplot as plt\nimport torch\n\n\ndef imshow(image: torch.tensor, height: int = 10, width: int = 10):\n    \"\"\"Utility function to plot images.\"\"\"\n    plt.figure(figsize=(height, width))\n    plt.imshow(K.tensor_to_image(image))\n    plt.axis(\"off\")\n    plt.show()\n\nLoad and show the original image\n\ntorch.manual_seed(0)\n\ntimg: torch.Tensor = K.io.load_image(\"img1.ppm\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\nimshow(timg, 10, 10)\n\n\n\n\n\n\n\n\nIn the following section we are going to take the original image and generate random crops of a given size.\n\nrandom_crop = K.augmentation.RandomCrop((64, 64))\n\npatch = torch.cat([random_crop(timg) for _ in range(15)], dim=-1)\n\nimshow(patch[0], 22, 22)\n\n\n\n\n\n\n\n\nNext, we will show how to crop patches and apply forth and back random geometric transformations.\n\n# transform a patch\n\nrandom_crop = K.augmentation.RandomCrop((64, 64))\nrandom_affine = K.augmentation.RandomAffine([-15, 15], [0.0, 0.25])\n\n# crop\npatch = random_crop(timg)\n\n# transform and retrieve transformation\npatch_affine = random_affine(patch)\ntransformation = random_affine.get_transformation_matrix(patch)\n\n# invert patch\n_, _, H, W = patch.shape\npatch_inv = K.geometry.warp_perspective(patch_affine, torch.inverse(transformation), (H, W))\n\n# visualise - (original, transformed, reconstructed)\npatches_vis = torch.cat([patch, patch_affine, patch_inv], dim=-1)\nimshow(patches_vis, 15, 15)"
  },
  {
    "objectID": "nbs/image_stitching.html",
    "href": "nbs/image_stitching.html",
    "title": "Image stitching example with LoFTR",
    "section": "",
    "text": "First, we will install everything needed:\n%%capture\n!pip install kornia\n!pip install kornia-rs\nNow let’s download an image pair\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1].split(\"?\")[0] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_image(\"https://github.com/kornia/data/raw/main/prtn00.jpg\")\ndownload_image(\"https://github.com/kornia/data/raw/main/prtn01.jpg\")\n\n'prtn01.jpg'\n%%capture\nimport kornia as K\nimport kornia.feature as KF\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n\ndef load_images(fnames):\n    return [K.io.load_image(fn, K.io.ImageLoadType.RGB32)[None, ...] for fn in fnames]\n\n\nimgs = load_images([\"prtn00.jpg\", \"prtn01.jpg\"])"
  },
  {
    "objectID": "nbs/image_stitching.html#stitch-them-together",
    "href": "nbs/image_stitching.html#stitch-them-together",
    "title": "Image stitching example with LoFTR",
    "section": "Stitch them together",
    "text": "Stitch them together\n\nfrom kornia.contrib import ImageStitcher\n\nIS = ImageStitcher(KF.LoFTR(pretrained=\"outdoor\"), estimator=\"ransac\")\n\nwith torch.no_grad():\n    out = IS(*imgs)\n\nplt.imshow(K.tensor_to_image(out))\nplt.show()"
  },
  {
    "objectID": "nbs/image_stitching.html#another-example",
    "href": "nbs/image_stitching.html#another-example",
    "title": "Image stitching example with LoFTR",
    "section": "Another example",
    "text": "Another example\n\ndownload_image(\"https://github.com/daeyun/Image-Stitching/blob/master/img/hill/1.JPG?raw=true\")\ndownload_image(\"https://github.com/daeyun/Image-Stitching/blob/master/img/hill/2.JPG?raw=true\")\ndownload_image(\"https://github.com/daeyun/Image-Stitching/blob/master/img/hill/3.JPG?raw=true\")\n\n'3.JPG'\n\n\n\nimgs = load_images([\"1.JPG\", \"2.JPG\", \"3.JPG\"])\n\n\nf, axarr = plt.subplots(1, 3, figsize=(16, 6))\n\naxarr[0].imshow(K.tensor_to_image(imgs[0]))\naxarr[0].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\naxarr[1].imshow(K.tensor_to_image(imgs[1]))\naxarr[1].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\naxarr[2].imshow(K.tensor_to_image(imgs[2]))\naxarr[2].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n\n\n\n\n\n\n\n\n\nmatcher = KF.LocalFeatureMatcher(KF.GFTTAffNetHardNet(100), KF.DescriptorMatcher(\"snn\", 0.8))\nIS = ImageStitcher(matcher, estimator=\"ransac\")\n\nwith torch.no_grad():\n    out = IS(*imgs)\n\n\nplt.figure(figsize=(16, 16))\nplt.imshow(K.tensor_to_image(out))"
  },
  {
    "objectID": "nbs/color_conversions.html",
    "href": "nbs/color_conversions.html",
    "title": "Color space conversion",
    "section": "",
    "text": "%%capture\n!pip install kornia\n!pip install kornia-rs"
  },
  {
    "objectID": "nbs/color_conversions.html#explanation",
    "href": "nbs/color_conversions.html#explanation",
    "title": "Color space conversion",
    "section": "Explanation",
    "text": "Explanation\nImages are asumed to be loaded either in RGB or Grayscale space.\n\nWe will use OpenCV to load images.\nConvert from BGR to RGB (note that OpenCV loads images in BGR format).\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/simba.png\"\ndownload_image(url)\n\n'simba.png'\n\n\n\nimport kornia as K\nimport numpy as np\nimport torch\nimport torchvision\nfrom matplotlib import pyplot as plt\n\n\n# read the image with Kornia\nimg_tensor = K.io.load_image(\"simba.png\", K.io.ImageLoadType.RGB32)  # CxHxW\nimg_array = K.tensor_to_image(img_tensor)\n\nplt.axis(\"off\")\nplt.imshow(img_array)\nplt.show()\n\n\n\n\n\n\n\n\nAlternatively we can use use kornia.color to perform the color transformation.\n\nConvert the tensor to RGB\nConvert back the tensor to numpy for visualisation.\n\n\nx_rgb: torch.Tensor = img_tensor\n\n# to BGR\nx_bgr: torch.Tensor = K.color.rgb_to_bgr(x_rgb)\n\n# convert back to numpy and visualize\nimg_np: np.array = K.tensor_to_image(x_bgr)\nplt.imshow(img_np)\nplt.axis(\"off\");\n\n\n\n\n\n\n\n\nUsing kornia we easily perform color transformation in batch mode.\n\ndef imshow(input: torch.Tensor):\n    out: torch.Tensor = torchvision.utils.make_grid(input, nrow=2, padding=5)\n    out_np: np.array = K.tensor_to_image(out)\n    plt.imshow(out_np)\n    plt.axis(\"off\")\n    plt.show()\n\n\n# create a batch of images\nxb_bgr = torch.stack([x_bgr, K.geometry.hflip(x_bgr), K.geometry.vflip(x_bgr), K.geometry.rot180(x_bgr)])\nimshow(xb_bgr)\n\n\n\n\n\n\n\n\n\n# convert to back to RGB\nxb_rgb = K.color.bgr_to_rgb(xb_bgr)\nimshow(xb_rgb)\n\n\n\n\n\n\n\n\n\n# convert to grayscale\n# NOTE: image comes in torch.uint8, and kornia assumes floating point type\nxb_gray = K.color.rgb_to_grayscale(xb_rgb)\nimshow(xb_gray)\n\n\n\n\n\n\n\n\n\n# convert to HSV\nxb_hsv = K.color.rgb_to_hsv(xb_rgb)\nimshow(xb_hsv)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "nbs/gaussian_blur.html#preparation",
    "href": "nbs/gaussian_blur.html#preparation",
    "title": "Blur image using GaussianBlur operator",
    "section": "Preparation",
    "text": "Preparation\nWe first install Kornia.\n\n%%capture\n%matplotlib inline\n!pip install kornia\n!pip install kornia-rs\n\n\nimport kornia\n\nkornia.__version__\n\n'0.6.12'\n\n\nNow we download the example image.\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\nurl = \"https://github.com/kornia/data/raw/main/bennett_aden.png\"\ndownload_image(url)\n\n'bennett_aden.png'"
  },
  {
    "objectID": "nbs/gaussian_blur.html#example",
    "href": "nbs/gaussian_blur.html#example",
    "title": "Blur image using GaussianBlur operator",
    "section": "Example",
    "text": "Example\nWe first import the required libraries and load the data.\n\nimport matplotlib.pyplot as plt\nimport torch\n\n# read the image with kornia\ndata = kornia.io.load_image(\"./bennett_aden.png\", kornia.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\nTo apply a filter, we create the Gaussian Blur filter object and apply it to the data:\n\n# create the operator\ngauss = kornia.filters.GaussianBlur2d((11, 11), (10.5, 10.5))\n\n# blur the image\nx_blur: torch.tensor = gauss(data)\n\nThat’s it! We can compare the pre-transform image and the post-transform image:\n\n# convert back to numpy\nimg_blur = kornia.tensor_to_image(x_blur)\n\n# Create the plot\nfig, axs = plt.subplots(1, 2, figsize=(16, 10))\naxs = axs.ravel()\n\naxs[0].axis(\"off\")\naxs[0].set_title(\"image source\")\naxs[0].imshow(kornia.tensor_to_image(data))\n\naxs[1].axis(\"off\")\naxs[1].set_title(\"image blurred\")\naxs[1].imshow(img_blur)\n\npass"
  },
  {
    "objectID": "nbs/geometric_transforms.html",
    "href": "nbs/geometric_transforms.html",
    "title": "Geometric image and points transformations",
    "section": "",
    "text": "\\\nIn brief, in this tutorial we will learn how to:"
  },
  {
    "objectID": "nbs/geometric_transforms.html#installation",
    "href": "nbs/geometric_transforms.html#installation",
    "title": "Geometric image and points transformations",
    "section": "Installation",
    "text": "Installation\nWe first install Kornia v0.2.0 and Matplotlib for visualisation.\nTo play with data we will use some samples from HPatches dataset [1].\n\n[1] HPatches: A benchmark and evaluation of handcrafted and learned local descriptors, Vassileios Balntas, Karel Lenc, Andrea Vedaldi and Krystian Mikolajczyk, CVPR 2017.\n\n\n%%capture\n!pip install kornia\n!pip install kornia-rs\n\n\nimport io\n\nimport requests\n\n\ndef download_image(url: str, filename: str = \"\") -&gt; str:\n    filename = url.split(\"/\")[-1] if len(filename) == 0 else filename\n    # Download\n    bytesio = io.BytesIO(requests.get(url).content)\n    # Save file\n    with open(filename, \"wb\") as outfile:\n        outfile.write(bytesio.getbuffer())\n\n    return filename\n\n\ndownload_image(\"https://github.com/kornia/data/raw/main/homography/img1.ppm\")\ndownload_image(\"https://github.com/kornia/data/raw/main/v_dogman.ppm\")\ndownload_image(\"https://github.com/kornia/data/raw/main/v_maskedman.ppm\")\ndownload_image(\"https://github.com/kornia/data/raw/main/delorean_side.png\")\n\n'delorean.png'"
  },
  {
    "objectID": "nbs/geometric_transforms.html#setup",
    "href": "nbs/geometric_transforms.html#setup",
    "title": "Geometric image and points transformations",
    "section": "Setup",
    "text": "Setup\nWe will import the needed libraries and create a small functionalities to make use of OpenCV I/O.\n\n%matplotlib inline\nimport cv2\nimport kornia as K\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nDefine a function for visualisation using Matplotlib.\n\ndef imshow(image: np.ndarray, height: int, width: int):\n    \"\"\"Utility function to plot images.\"\"\"\n    plt.figure(figsize=(height, width))\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.show()\n\nSince Kornia don’t provide render functionalities, let’s use OpenCV cv2.circle to draw points.\n\ndef draw_points(img_t: torch.Tensor, points: torch.Tensor) -&gt; np.ndarray:\n    \"\"\"Utility function to draw a set of points in an image.\"\"\"\n\n    # cast image to numpy (HxWxC)\n    img: np.ndarray = K.utils.tensor_to_image(img_t)\n\n    # using cv2.circle() method\n    # draw a circle with blue line borders of thickness of 2 px\n    img_out: np.ndarray = img.copy()\n\n    for pt in points:\n        x, y = int(pt[0]), int(pt[1])\n        img_out = cv2.circle(img_out, (x, y), radius=10, color=(0, 0, 255), thickness=5)\n    return np.clip(img_out, 0, 1)"
  },
  {
    "objectID": "nbs/geometric_transforms.html#transform-single-image",
    "href": "nbs/geometric_transforms.html#transform-single-image",
    "title": "Geometric image and points transformations",
    "section": "Transform single image",
    "text": "Transform single image\nIn this section we show how to open a single image, generate 2d random points and plot them using OpenCV and Matplotlib.\nNext, we will use kornia.augmentation.RandomAffine to gerenate a random synthetic view of the given image and show how to retrieve the generated transformation to later be used to transform the points between images.\n\n# load original image\nimg1: torch.Tensor = K.io.load_image(\"img1.ppm\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\n# generate N random points within the image\nN: int = 10  # the number of points\nB, CH, H, W = img1.shape\n\npoints1: torch.Tensor = torch.rand(1, N, 2)\npoints1[..., 0] *= W\npoints1[..., 1] *= H\n\n# draw points and show\nimg1_vis: np.ndarray = draw_points(img1[0], points1[0])\n\nimshow(img1_vis, 10, 10)\n\n\n\n\n\n\n\n\nNow lets move to a bit more complex example and start to use the kornia.augmentation API to transform an image and retrieve the applied transformation. We’ll show how to reuse this transformation to project the 2d points between images.\n\n# declare an instance of our random affine generation eith `return_transform`\n# set to True, so that we recieve a tuple with the transformed image and the\n# transformation applied to the original image.\ntransform: nn.Module = K.augmentation.RandomAffine(degrees=[-45.0, 45.0], p=1.0)\n\n# tranform image and retrieve transformation\nimg2 = transform(img1, transform=transform)\ntrans = transform.get_transformation_matrix(img1)\n\n# transform the original points\npoints2: torch.Tensor = K.geometry.transform_points(trans, points1)\n\nimg2_vis: np.ndarray = draw_points(img2, points2[0])\n\n\nimshow(img2_vis, 15, 15)"
  },
  {
    "objectID": "nbs/geometric_transforms.html#transform-batch-of-images",
    "href": "nbs/geometric_transforms.html#transform-batch-of-images",
    "title": "Geometric image and points transformations",
    "section": "Transform batch of images",
    "text": "Transform batch of images\nIn the introduction we explained about the capability of kornia.augmentation to be integrated with other torch components such as nn.Module and nn.Sequential.\nWe will create a small component to perform data augmentation on batched images reusing the same ideas showed before to transform images and points.\nFirst, lets define a class that will generate samples of synthetic views with a small color augmentation using the kornia.augmentation.ColorJitter and kornia.augmentation.RandomAffine components.\nNOTE: we set the forward pass to have no gradients with the decorator @torch.no_grad() to make it more memory efficient.\n\nfrom typing import Dict\n\n\nclass DataAugmentator(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        # declare kornia components as class members\n        self.k1 = K.augmentation.RandomAffine([-60, 60], p=1.0)\n        self.k2 = K.augmentation.ColorJitter(0.5, 0.5, p=1.0)\n\n    @torch.no_grad()\n    def forward(self, img1: torch.Tensor, pts1: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        assert len(img1.shape) == 4, img1.shape\n\n        # apply geometric transform the transform matrix\n        img2 = self.k1(img1)\n        trans = self.k1.get_transformation_matrix(img1)\n\n        # apply color transform\n        img1, img2 = self.k2(img1), self.k2(img2)\n\n        # finally, lets use the transform to project the points\n        pts2: torch.Tensor = K.geometry.transform_points(trans, pts1)\n\n        return dict(img1=img1, img2=img2, pts1=pts1, pts2=pts2)\n\nLets use the defined component and generate some syntethic data !\n\n# load data and make a batch\nimg1: torch.Tensor = K.io.load_image(\"v_dogman.ppm\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\nimg2: torch.Tensor = K.io.load_image(\"v_maskedman.ppm\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\n# crop data to make it homogeneous\ncrop = K.augmentation.CenterCrop((512, 786))\n\nimg1, img2 = crop(img1), crop(img2)\n\n# visualize\nimg_vis = torch.cat([img1, img2], dim=-1)\nimshow(K.tensor_to_image(img_vis), 15, 15)\n\n\n\n\n\n\n\n\n\n# create an instance of the augmentation pipeline\n# NOTE: remember that this is a nn.Module and could be\n# placed inside any network, pytorch-lighting module, etc.\naug: nn.Module = DataAugmentator()\n\nfor _ in range(5):  # create some samples\n    # generate batch\n    img_batch = torch.cat([img1, img2], dim=0)\n\n    # generate random points (or from a network)\n    N: int = 25\n    B, CH, H, W = img_batch.shape\n\n    points: torch.Tensor = torch.rand(B, N, 2)\n    points[..., 0] *= W\n    points[..., 1] *= H\n\n    # sample data\n    batch_data = aug(img_batch, points)\n\n    # plot and show\n    # visualize both images\n\n    img_vis_list = []\n\n    for i in range(2):\n        img1_vis: np.ndarray = draw_points(batch_data[\"img1\"][i], batch_data[\"pts1\"][i])\n        img_vis_list.append(img1_vis)\n\n        img2_vis: np.ndarray = draw_points(batch_data[\"img2\"][i], batch_data[\"pts2\"][i])\n        img_vis_list.append(img2_vis)\n\n    img_vis = np.concatenate(img_vis_list, axis=1)\n\n    imshow(img_vis, 20, 20)"
  },
  {
    "objectID": "nbs/geometric_transforms.html#bonus-backprop-to-the-future",
    "href": "nbs/geometric_transforms.html#bonus-backprop-to-the-future",
    "title": "Geometric image and points transformations",
    "section": "BONUS: Backprop to the future",
    "text": "BONUS: Backprop to the future\nOne of the main motivations during the desing for the kornia.augmentation API was to give to the user the flexibility to retrieve the applied transformation in order to achieve one of the main purposes of Kornia - the reverse engineering.\nIn this case we will show how easy one can combine Kornia and PyTorch components to undo the transformations and go back to the original data.\n“Wait a minute, Doc. Are you telling me you built a time machine…out of a PyTorch?” - Marty McFLy\n\n# lets start the Delorean engine\ndelorean: torch.Tensor = K.io.load_image(\"delorean_side.png\", K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n\nimshow(K.utils.tensor_to_image(delorean), 10, 10)\n\n\n\n\n\n\n\n\n“If my calculations are correct, when this baby hits 88 miles per hour, you’re gonna see some serious shit.” - Doc. Brown\n\n# turn on the time machine panel (TMP)\n\nTMP = K.augmentation.RandomHorizontalFlip(p=1.0)\n\ndelorean_past = TMP(delorean)  # go !\ntime_coords_past = TMP.get_transformation_matrix(delorean)\n\nimshow(K.utils.tensor_to_image(delorean_past), 10, 10)\n\n\n\n\n\n\n\n\n\nLet’s go back to the future !\n“Marty! You’ve gotta come back with me!” - Doc. Brown\n\n# lets go back to the past\n\ntime_coords_future: torch.Tensor = torch.inverse(time_coords_past)\n\nH, W = delorean_past.shape[-2:]\ndelorean_future = K.geometry.warp_perspective(delorean_past, time_coords_future, (H, W))\n\nimshow(K.utils.tensor_to_image(delorean_future), 10, 10)"
  }
]